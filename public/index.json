[
    {"body":"Role We’re looking for self-motivated team members who crave a challenge and feel energized to roll up their sleeves and help realize InfinyOn’s enormous potential. Chart your own path and take healthy risks as we solve big problems together.\nInfinyOn Cloud, built on Fluvio OSS, delivers a complete end-to-end event streaming experience via a Software as a Service (SaaS) model. Moving data multiple times between databases, microservices, and analytics tools is expensive and complicated. InfinyOn intelligent data pipelines bundle data streaming and in-line SmartModule processing in one product allowing vendors to deploy and operate microservices directly on the pipeline. The InfinyOn approach eliminates infrastructure complexity and operational overhead for out-of-band microservices and reduces the need for excessive data movement.\nThe Head of Product is directly responsible for working with Cloud customers and the open source community to understand requirements and craft a roadmap that brings InfinyOn to the forefront of the intelligent data streaming market. The ideal candidate can synthesize many use cases and prioritize features based on vision, effort, and market opportunity. In addition, they will work closely with sales, marketing, and engineering teams to facilitate transparency, fast decision-making, and smooth collaboration.\nSkills and Experience 5+ years of product management experience focused on SaaS and IaaS products Strong technical background Experience at a startup creating product roadmaps from 0-1 Distributed systems experience Demonstrated fluency with cloud computing Open-source experience desirable Responsibilities Directly responsible for working with customers and communities to craft a product roadmap that meets the needs of the Fluvio community. Work at all levels of the technology stack to derive requirements and bring core features to market, whether they are proprietary or donated to open-source. Drive Fluvio OSS and InfinyOn Cloud Roadmap with a focus on user experience, platform stability, and scalability. Develop product and technical specifications for Fluvio and InfinyOn Cloud features. Align product roadmap with company priorities. Understand engineering capacity and balance backlog priorities based on team velocity and market need. Improve the release process from use case definition to market introduction. Educate InfinyOn\u0026rsquo;s Sales and Marketing teams and the broader community about new features. ","description":"The InfinyOn Head of Product is directly responsible for working with Cloud customers and the open source community to understand requirements and craft a roadmap that brings InfinyOn to the forefront of the intelligent data streaming market.","keywords":null,"summary":"Role We’re looking for self-motivated team members who crave a challenge and feel energized to roll up their sleeves and help realize InfinyOn’s enormous potential. Chart your own path and take healthy risks as we solve big problems together.\nInfinyOn Cloud, built on Fluvio OSS, delivers a complete end-to-end event streaming experience via a Software as a Service (SaaS) model. Moving data multiple times between databases, microservices, and analytics tools is expensive and complicated.","title":"Head of Product Management (Global)","url":"http://localhost:1315/careers/head-of-product-management/"},{"body":"InfinyOn Cloud is managed through the Fluvio CLI via the fluvio-cloud plugin distributed by default by the installation script.\nLet\u0026rsquo;s create an InfinyOn Cloud account and login using the Fluvio CLI:\n1. Create an InfinyOn Cloud account SignUp for a free account here: InfinyOn Cloud - Signup\nYou need an InfinyOn Cloud account to:\nstart a managed Fluvio cluster download SmartModules and Connectors from InfinyOn Hub 2. Install Fluvio CLI Use the following curl commad to download fluvio CLI:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash The CLI and related tools and config files and placed in the $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\n3. Provision a Cluster Pick a region to create a cluster from the InfinyOn Cloud UI.\nYou can also use fluvio cli:\n%copy first-line%\n$ fluvio cloud cluster create 4. Login to InfinyOn Cloud Run this command to log into InfinyOn Cloud.\nYou log in with Oauth2 or email/password:\nOAuth2 %copy first-line%\n$ fluvio cloud login --use-oauth2 A web browser has been opened at https://\u0026lt;OAUTH2_SERVER_DOMAIN\u0026gt;/activate?user_code=\u0026lt;CODE\u0026gt;. Please proceed with authentication. Email and password %copy first-line%\n$ fluvio cloud login InfinyOn Cloud email: example@infinyon.com Password: \u0026lt;hidden\u0026gt; \u0026#x1f389; Congratulations! You are now connected to InfinyOn Cloud.\n","description":"Step-by-step instructions on how to create an account and login to InfinyOn Cloud using the CLI.","keywords":null,"summary":"InfinyOn Cloud is managed through the Fluvio CLI via the fluvio-cloud plugin distributed by default by the installation script.\nLet\u0026rsquo;s create an InfinyOn Cloud account and login using the Fluvio CLI:\n1. Create an InfinyOn Cloud account SignUp for a free account here: InfinyOn Cloud - Signup\nYou need an InfinyOn Cloud account to:\nstart a managed Fluvio cluster download SmartModules and Connectors from InfinyOn Hub 2. Install Fluvio CLI Use the following curl commad to download fluvio CLI:","title":"Use InfinyOn Cloud Cluster","url":"http://localhost:1315/docs/tutorials/infinyon-cloud/"},{"body":"Role We are looking for a passionate senior Rust frontend engineer to build our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of real-time intelligent data services.\nIn this role, you will be crafting a beautiful and intuitive user interface for our web applications to enable users to move data in real-time. You will be solving many engineering challenges, such as optimizing data streams for browser stack, building powerful but simple user interfaces to control data movement, and converting data streams into real-time visualization.\nResponsibilities Create and maintain web-based user interfaces using a Rust Web Framework. Integrate Web Assembly and other web components. Work closely with designers to implement new features. Ensure high performance by profiling performance intensive code. Write reusable, testable, and efficient code. Design UI for demo purposes, improvising when necessary. Collaborate with team members working across the platform and contribute to product roadmap milestones. Requirements 2+ years of building large scale front end application for Web. 1+ year of experience in Rust. Experience with modern JavaScript UI frameworks such as React, Angular, Vue.js. Fundamental grasp of HTML/CSS/JavaScript. Expert in Websockets and other real-time technologies. Some knowlege of WebAssembly. Experience with a Cloud platform such as AWS, GCP or Azure. Strong problem-solving skills. Exposure to maintaining applications in production Fluent in English (spoken and written) Nice to have Experience with other Rust web frameworks such as Yew, Rust Dominator, etc. Understanding of browser security and best practices Familiarity with containers and Kubernetes Knowledge of distributed systems such as Kafka ","description":"The InfinyOn Cloud team is looking for an extraordinary frontend engineer passionate about building beautiful user interfaces for our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of intelligent data services","keywords":null,"summary":"Role We are looking for a passionate senior Rust frontend engineer to build our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of real-time intelligent data services.\nIn this role, you will be crafting a beautiful and intuitive user interface for our web applications to enable users to move data in real-time. You will be solving many engineering challenges, such as optimizing data streams for browser stack, building powerful but simple user interfaces to control data movement, and converting data streams into real-time visualization.","title":"Senior Rust Engineer - Cloud Web UI","url":"http://localhost:1315/careers/cloud-ui-engineer-senior-level/"},{"body":"Role We are looking for a passionate senior UI/UX designer for our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of real-time intelligent data services.\nIn this role, you will be designing beautiful and intuitive user interfaces for our web applications to enable users to move data in real-time. You will be solving many design challenges, such as onboarding users of varying levels of expertise in data streaming architecture. Our high-level goal is to make it simple for users to build and visualize complex data flow architectures.\nResponsibilities Lead the creation of high-quality user experiences by working closely with product managers and engineers. Consider the product roadmap and balance long-term goals with short-term technical constraints. Document decisions and their justifications to produce data-driven designs. Take ownership of design decisions from concept to production and iterate when necessary. Meet with customers to understand their needs. Creating high-level wireframes and high-fidelity mockups for UI components detailing size, color, and layout. About You You are excited to work on complex systems. You want to work in a startup environment and focus on getting things done. You are self-motivated \u0026amp; self-organized. You embrace a well-structured, iterative design process. You possess excellent communication skills. You are a team player. Requirements 4+ years of experience in software UX/Design. Examples of interactive design work in Web UI. Knowledge of CSS or Tailwind. Experience building products targeting technical users. Proficiency in Figma or other design tools. Strong problem-solving skills. Fluent in English (spoken and written). Nice to have Experience with designing for mobile devices. Familarity with Data oriented products. Understand enterprise feature such as security and role based access. ","description":"The InfinyOn Cloud team is looking for an extraordinary UI/UX designer passionate about designing beautiful user interfaces for our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of intelligent data services","keywords":null,"summary":"Role We are looking for a passionate senior UI/UX designer for our cloud offering. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of real-time intelligent data services.\nIn this role, you will be designing beautiful and intuitive user interfaces for our web applications to enable users to move data in real-time. You will be solving many design challenges, such as onboarding users of varying levels of expertise in data streaming architecture.","title":"Senior UI/UX Designer","url":"http://localhost:1315/careers/ui-ux-senior-level/"},{"body":"Hackernews Reader helps you build an XML reader that ingests hackernews articles, converts them to json, divides them into records, and publishes each record to a topic. This guide uses the following connector:\nhttp-source: to read periodically from a hackernews, parse the XML result into json records, and publish the result to a topic. Prerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Download smartmodules Start Connector Check Results Create http-source configuration file Create an HTTP source connector configuration file called hackernews.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: hackernews type: http-source topic: hackernews http: method: GET endpoint: \u0026#39;https://hnrss.org/newest\u0026#39; interval: 600s transforms: - uses: infinyon-labs/rss-json@0.1.0 - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: items: \u0026#34;\u0026#34; - uses: infinyon-labs/array-map-json@0.1.0 Download startmodules Download the smartmodules used by the connectors to your cluster:\n%copy%\n$ fluvio hub sm download infinyon/jolt@0.3.0 $ fluvio hub sm download infinyon-labs/rss-json@0.1.0 $ fluvio hub sm download infinyon-labs/array-map-json@0.1.0 Start Connector %copy%\n$ fluvio cloud connector create -c hackernews.yaml Check Results Connector logs:\n%copy%\n$ fluvio cloud connector log hackernews Records produced:\n%copy%\n$ fluvio consume hackernews -T 10 References How to Stream and Transform Data from Hacker News RSS Feed (YouTube Video) labs-rss-json-sm labs-array-map-json-sm ","description":"Data pipeline that periodically reads articles from Hackernews and publishes them on a topic.","keywords":null,"summary":"Hackernews Reader helps you build an XML reader that ingests hackernews articles, converts them to json, divides them into records, and publishes each record to a topic. This guide uses the following connector:\nhttp-source: to read periodically from a hackernews, parse the XML result into json records, and publish the result to a topic. Prerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Download smartmodules Start Connector Check Results Create http-source configuration file Create an HTTP source connector configuration file called hackernews.","title":"Hackernews Reader","url":"http://localhost:1315/docs/guides/hackernews-reader/"},{"body":"Role We are looking for an extraordinary Rust engineer passionate about building reliable and scalable infrastructure for real-time data streaming. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of intelligent data services without maintaining the infrastructure.\nIn this role, you will be working in every part of the infrastructure stack all the way from writing storage and network components, extending the control plane to building a UI dashboard.\nResponsibilities Build and maintain Cloud native microservices Design and implement solutions to provision and scale infrastructure Improve performance by designing and implementing low-latency and high-availability network services Implement user facing features across Web UI and HTTP API Write reusable, testable, and efficient code Collaborate with team members working across the platform and contribute to product roadmap milestones Develop and maintain Fluvio and related open source projects Requirements Proficient in Rust programming language 2+ years of experience in software engineering Proficiency in Networking and Linux. Strong problem-solving skills Exposure to deploying and maintaining applications in production Fluent in English (spoken and written) Experience with 1 or more Cloud Infrastructure such as AWS, Azure or GCP Nice to have Working knowledge of Rust async framework Familiarity with containers and Kubernetes Experience with Terraform Knowledge of distributed systems such as Kafka Knowledge of web technologies such as Svelte ","description":"The InfinyOn Cloud team is looking for a Rust engineer passionate about building reliable and scalable infrastructure for real-time data streaming. Your contributions will enable organizations to accelerate the adoption of intelligent data streaming services without maintaining the infrastructure.","keywords":null,"summary":"Role We are looking for an extraordinary Rust engineer passionate about building reliable and scalable infrastructure for real-time data streaming. Your contributions will help us fulfill our mission to enable organizations to accelerate the adoption of intelligent data services without maintaining the infrastructure.\nIn this role, you will be working in every part of the infrastructure stack all the way from writing storage and network components, extending the control plane to building a UI dashboard.","title":"Software Engineer (Cloud Services)","url":"http://localhost:1315/careers/cloud-engineer-mid-level/"},{"body":"Role We seek a Senior Rust engineer with a startup mentality to construct the next generation of real-time cloud data infrastructure. You will collaborate with a team of talented engineers across global locations to design and operate the highest reliable and scalable real-time data stack. You are skilled in turning cutting-edge technologies such as Rust, Kubernetes, and WebAssembly into simple, user-friendly products. As a Cloud software engineer, you will be working in every part of the stack, from writing low-level networking code to building high-performance distributed services. To construct a cloud-native infrastructure, you will leverage and extend our open-source project Fluvio.\nResponsibilities Build and operate Cloud Infrastructure products utilizing Rust, Kubernetes, and WebAssembly. Closely collaborate with teams to create highly scalable, operable, and maintainable products. Own and drive the whole product lifecycle - from inception and design through deployment, operation, and refinement. Invent tools and processes that enable fast, accurate, reliable, easy-to-use development and deployment systems. Provide fast and comprehensive validation test suites to ensure the highest software quality. Develop and maintain continuous integration and deployment system with the shortest and most reliable deployment cycle. Identify areas for improvement and create innovative solutions that enable high developer velocity. Working with the Product team to define and prioritize product roadmap to meet business goals. Monitor and optimize the different parts of cloud infrastructure to ensure high performance and reliability. Requirements 2+ years in Rust programming language with Async programming experience. 3+ years of experience in cloud infrastructure with at least 1 year of AWS experience. At least 1 year of Kubernetes experience. 2+ years of experience in writing REST services. Expert in Networking and Linux. Strong problem-solving skills. Knowledge of distributed systems and data structures. Significant experience in deploying and maintaining cloud applications in production. Fluent in English (spoken and written) Ability to work in a fast and dynamic environment. Preferred Skills and Experience Experience with EKS and expertiese in many AWS services as S3, IAM, EC2, VPC, etc. Experience with Terraform Writing Kubernetes Operators Knowledge of event streaming systems such as Kafka, Flink, etc. ","description":"The InfinyOn Engineering team is looking for a Senior Rust engineer passionate about building reliable and scalable cloud infrastructure for real-time data streaming. Your contributions will enable organizations to accelerate the adoption of intelligent data streaming services without maintaining the infrastructure.","keywords":null,"summary":"Role We seek a Senior Rust engineer with a startup mentality to construct the next generation of real-time cloud data infrastructure. You will collaborate with a team of talented engineers across global locations to design and operate the highest reliable and scalable real-time data stack. You are skilled in turning cutting-edge technologies such as Rust, Kubernetes, and WebAssembly into simple, user-friendly products. As a Cloud software engineer, you will be working in every part of the stack, from writing low-level networking code to building high-performance distributed services.","title":"Sr. Rust Cloud Software Engineer","url":"http://localhost:1315/careers/cloud-engineer-senior-level/"},{"body":"Installing Fluvio on Docker requires two containers, one for the Streaming Controller (SC) and the other for the Streaming Processing Unit (SPU).\nPrerequisites Docker \u0026amp; Docker Compose We used Colima, but any Docker environment will do. Fluvio CLI Install fvm and follow the instructions:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash You are all set; let\u0026rsquo;s get started.\nStart Fluvio Cluster To start a fluvio cluster, we\u0026rsquo;ll need to perform the following steps:\nStart SC and SPU containers Add the cluster profile to Fluvio Authorize the SPU with the SC Start SC and SPU Containers Open a terminal, and add the following sub-directories:\n%copy%\n$ mkdir metadata; mkdir data The SC uses metadata to store the metadata configurations and the SPU data to persist the data records.\nCreate a file called docker-compose.yaml and paste the following configuration:\n%copy%\nversion: \u0026#39;3.8\u0026#39; services: sc: image: infinyon/fluvio:latest hostname: sc container_name: sc volumes: - ./metadata:/fluvio/metadata environment: - RUST_LOG=info ports: - \u0026#34;9103:9003\u0026#34; command: - ./fluvio-run - sc - --local - /fluvio/metadata spu-5001: image: infinyon/fluvio:latest hostname: spu-5001 container_name: spu-5001 volumes: - ./data:/fluvio/data environment: - RUST_LOG=info ports: - \u0026#34;9110:9010\u0026#34; command: - ./fluvio-run - spu - -i - \u0026#34;5001\u0026#34; - -p - spu-5001:9010 - -v - spu-5001:9011 - --sc-addr - sc:9004 - --log-base-dir - /fluvio/data Note\nThe SC \u0026amp; SPU default ports are mapped to avoid colisions if you are also running an local cluster: SC public port 9103 SPU public 9110 The compose file uses the metadata and data directories we\u0026rsquo;ve created above. Run the compose file to start containers (add -d to run in the background):\n%copy%\n$ docker compose up With the containers running, we\u0026rsquo;ll need to link them in a cluster.\nAdd the cluster profile to Fluvio Fluvio uses profiles to switch between clusters. For example, a Local cluster, an InfinyOn Cloud cluster, and a Docker cluster. Let\u0026rsquo;s create a Docker cluster:\n%copy%\n$ fluvio profile add docker 127.0.0.1:9103 docker Fluvio automatically switches to the new profile.\nAuthorize the SPU with the SC The SC and SPU are both running. However they are not yet talking to each other, as the SC does not allow the SPU to attach to the cluster without prior authorization.\nRegister the SPU with the SC:\n%copy%\n$ fluvio cluster spu register \\ --id 5001 \\ --public-server 0.0.0.0:9110 \\ --private-server 0.0.0.0:9011 Finally, check the cluster status:\n%copy%\n$ fluvio cluster status \u0026#x1f389; Congratulations! Your cluster is now ready for use.\nAdvanced If you want to start the SC and the SPU independently, you may use docker run.\nRun the SC:\n%copy%\n$ docker run --name sc \\ -v ./metadata:/fluvio/metadata \\ -e RUST_LOG=info \\ --network host \\ infinyon/fluvio \\ /fluvio-run sc \\ --local /fluvio/metadata Run the SPU:\n%copy%\n$ docker run --name spu-5001 \\ -v ./data:/fluvio/data \\ -e RUST_LOG=info \\ --network host \\ infinyon/fluvio \\ /fluvio-run spu \\ -i 5001 \\ -p 0.0.0.0:9010 \\ -v 0.0.0.0:9011 \\ --log-base-dir /fluvio/data ","description":"Install Fluvio Cluster with Docker Compose","keywords":null,"summary":"Installing Fluvio on Docker requires two containers, one for the Streaming Controller (SC) and the other for the Streaming Processing Unit (SPU).\nPrerequisites Docker \u0026amp; Docker Compose We used Colima, but any Docker environment will do. Fluvio CLI Install fvm and follow the instructions:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash You are all set; let\u0026rsquo;s get started.\nStart Fluvio Cluster To start a fluvio cluster, we\u0026rsquo;ll need to perform the following steps:","title":"Install Fluvio Cluster in Docker","url":"http://localhost:1315/docs/tutorials/docker-installation/"},{"body":"Welcome to \u0026ldquo;Analytics 101.\u0026rdquo; The purpose of this guide is to set the context of the information economy in which we exist and describe the important concepts and critical constraints to build data products that delivers value and delight customers.\nAnalytics in the digital age There is no doubt that we live in a digital age transacting in the information economy. The three legs of the digital stool are computers, software, and data.\nNaturally, data is a valuable asset that businesses can leverage to gain a competitive edge and thrive in the market. Analytics, the systematic analysis of data, is a an important mechanism to extract insights, inform decisions, and drive improvements in digitized industries.\nThe Significance of Analytics in Decision Making In a world flooded with data, making sense of it all can be overwhelming. Analytics is the method to the madness. Analytics involves the process of collecting, processing, analyzing, and interpreting data to generate insights. These insights empower individuals and organizations to make data-informed decisions and be more data-driven.\nDecisions made solely based on intuition or historical experiences are like trying to create wealth by gambling at a casino. The risks are extremely high and the odds of success are pretty low. Analytics provides a structured approach to decision-making by allowing us to tap into patterns, trends, and relationships found in the data.\nHigh level use cases Whether it\u0026rsquo;s a retailer predicting consumer preferences, a healthcare provider optimizing patient care, or a city planner enhancing urban sustainability, analytics equips decision-makers with the knowledge they need to make choices that are not just reactive, but proactive and informed.\nAnalytics is a valuable practice across numerous sectors. In marketing, it enables businesses to understand customer behavior, personalize offerings, and optimize marketing campaigns for maximum impact. In finance, analytics aids in risk assessment, fraud detection, and investment strategies. Manufacturing industries utilize analytics to enhance production efficiency and minimize downtime. Analytics plays a crucial role in healthcare, where it aids in disease prediction, patient treatment, and drug discovery.\nThe reach of analytics extends even further. Public services can use analytics to allocate resources effectively, predict traffic congestion, and improve emergency response times. Environmental efforts benefit from analytics by analyzing ecological trends and predicting natural disasters. Educational institutions leverage analytics to tailor learning experiences and enhance student outcomes. In essence, analytics is the driving force that empowers industries to innovate, adapt, and thrive in the digital age.\nAs you embark on this journey through \u0026ldquo;Analytics 101,\u0026rdquo; prepare to explore the fundamental concepts that underpin analytics, learn about various methodologies, and discover how analytics can transform raw data into actionable insights. By the end of this guide, you\u0026rsquo;ll be equipped with the knowledge to harness the power of analytics in your decision-making processes, contributing to a more informed and efficient future.\nStay curious, as we dive into the heart of analytics and unravel its potential to reshape the way we perceive and navigate the world around us.\nFundamentals of Analytics Let\u0026rsquo;s start with the foundations of Analytics as we begin building up this knowledge base.\nWe\u0026rsquo;ll explore the definition of analytics and break down its key components, helping you grasp the essential building blocks that make this field so powerful. By the end of this chapter, you\u0026rsquo;ll understand the various types of analytics, the distinction between structured and unstructured data, and the step-by-step workflow that guides the entire analytics process.\nDefinition of Analytics and Its Key Components Analytics is the systematic examination of data to uncover meaningful insights, patterns, and trends that can inform decision-making. It involves the application of statistical, mathematical, and computational techniques to transform raw data into actionable information. Within the realm of analytics, several key components play a vital role:\nData: The raw information collected from various sources. This data can be anything from sales figures and customer demographics to social media interactions and sensor readings.\nMethods: The analytical techniques and methodologies used to process and analyze data. These methods can include statistical tests, machine learning algorithms, and optimization techniques.\nTechnology: The tools and platforms that facilitate data manipulation, analysis, and visualization. These tools can range from spreadsheet software and programming languages to specialized analytics software and platforms.\nDomain Knowledge: The understanding of the industry, business, or context within which the analytics is being performed. Domain knowledge helps ensure that the insights extracted are relevant and actionable.\nForms of data analytics Analytics can be categorized into three main types - descriptive, predictive, and prescriptive. Each type serves a unique purpose in the data analysis process:\nDescriptive Analytics: This type focuses on summarizing historical data to provide insights into what has happened. It answers questions like \u0026ldquo;What happened?\u0026rdquo; and involves techniques such as data aggregation, summary statistics, and data visualization.\nPredictive Analytics: Predictive analytics uses historical data to forecast future trends and outcomes. It answers questions like \u0026ldquo;What is likely to happen?\u0026rdquo; Machine learning algorithms and statistical models are commonly used in predictive analytics.\nPrescriptive Analytics: Building on predictive analytics, prescriptive analytics goes a step further by suggesting actions to take based on predicted outcomes. It answers questions like \u0026ldquo;What should we do?\u0026rdquo; and involves optimization techniques and decision models.\nIntroduction to Data Types: Structured vs. Unstructured Data Data comes in various forms, with two primary categories being structured and unstructured data:\nStructured Data: This type of data is highly organized and conforms to a specific format. It is often stored in databases and can be easily analyzed using traditional methods. Examples include numerical values, dates, and categorical data.\nUnstructured Data: Unstructured data lacks a specific format and organization. It includes textual data, images, audio recordings, and social media posts. Analyzing unstructured data requires more advanced techniques like natural language processing and image recognition.\nExplanation of the Analytics Workflow The analytics workflow outlines the sequence of steps required to transform raw data into meaningful insights:\nData Collection: Gather data from various sources, ensuring data quality and reliability.\nData Processing: Clean, transform, and organize the data to make it suitable for analysis.\nData Analysis: Apply appropriate techniques to extract insights, patterns, and trends from the processed data.\nData Interpretation: Interpret the results of the analysis in the context of the problem you\u0026rsquo;re trying to solve.\nThroughout this guide, you\u0026rsquo;ll gain a deeper understanding of each step in the analytics workflow, equipping you with the knowledge to navigate the intricacies of data analysis successfully. With the fundamentals in place, let\u0026rsquo;s move forward and explore the world of data collection and preparation.\nData Collection and Preparation Let\u0026rsquo;s discuss data preperation in this section. data collection and preparation—two foundational steps in the analytics process. We\u0026rsquo;ll explore the significance of data quality and reliability, understand the different sources of data, including internal and external, primary and secondary, and delve into various data collection methods such as surveys, sensors, and web scraping. Additionally, we\u0026rsquo;ll discuss the essential process of data processing, where data is cleaned, transformed, and organized to ensure it\u0026rsquo;s primed for effective analysis.\nImportance of Data Quality and Reliability The old adage \u0026ldquo;garbage in, garbage out\u0026rdquo; couldn\u0026rsquo;t be more relevant in the world of analytics. Data quality and reliability play a pivotal role in the accuracy and validity of your insights. Poor-quality data can lead to skewed results and misguided decisions. Ensuring data accuracy, completeness, consistency, and timeliness is essential to generate reliable and actionable insights.\nData Sources Data can originate from various sources, each with its unique characteristics:\nInternal Data: Data generated within an organization\u0026rsquo;s systems and operations. Examples include sales records, customer databases, and employee records.\nExternal Data: Data obtained from sources outside the organization. This could be market research reports, social media data, weather data, or economic indicators.\nPrimary Data: Data collected directly by the analyst for a specific purpose. This involves conducting surveys, interviews, or experiments to gather firsthand information.\nSecondary Data: Data collected by someone else for their own purposes but is repurposed for your analysis. This could be publicly available data, such as government statistics or academic research findings.\nData Collection Methods The methods used to collect data depend on the nature of the data and the research objectives:\nSurveys: Gathering data through questionnaires, interviews, or online forms to capture opinions, preferences, and experiences directly from respondents.\nSensors and IoT Devices: Collecting data from sensors, such as temperature sensors, GPS trackers, and wearable devices, to monitor physical processes and environmental conditions.\nWeb Scraping: Extracting data from websites to collect information that is publicly available but not easily downloadable.\nObservations: Collecting data by observing and recording behaviors, events, or occurrences in a natural or controlled environment.\nData Processing: Cleaning, Transforming, and Formatting Before data can be subjected to analysis, it often requires preprocessing to ensure its quality and usability:\nData Cleaning: Identifying and correcting errors, inconsistencies, and inaccuracies in the data. This could involve handling missing values, correcting typos, and resolving duplicate entries.\nData Transformation: Converting data into a format that is suitable for analysis. This might involve scaling, normalizing, or aggregating data.\nData Formatting: Structuring the data in a way that facilitates analysis. This includes arranging data into tables or datasets that are easy to navigate.\nBy emphasizing the importance of data quality, exploring diverse data sources and collection methods, and understanding the significance of data preprocessing, you\u0026rsquo;ll be well-equipped to gather, prepare, and handle data effectively in your analytics journey. The next step takes us into the realm of Exploratory Data Analysis, where we\u0026rsquo;ll learn how to unveil insights hidden within our data.\nExploratory Data Analysis (EDA) or Data Profiling Assuming that you have an inventory of data and a list of business problems, the first step is to understand what insights are available in the data. Data profiling and exploratory data analysis is the flow of identifying patterns, trends, and insights that are embedded in the data. EDA utilizes basic statistical techniques and data visualization methods to identify outliers, missing values, potential data anomalies, and the overall analytical value of the data to solve specific business problems.\nRole of EDA in Uncovering Patterns and Insights Exploratory Data Analysis is the preliminary phase of data analysis, aimed at gaining an initial understanding of the data\u0026rsquo;s characteristics. It serves as a crucial foundation for subsequent analysis and decision-making. EDA involves summarizing the main features of the data and visualizing its structure to reveal potential patterns, anomalies, and relationships that might otherwise go unnoticed.\nBasic Statistical Techniques EDA often begins with applying basic statistical measures to the data. These measures provide insights into the central tendency, dispersion, and distribution of the data:\nMean: The average of all values in the dataset, providing a measure of central tendency.\nMedian: The middle value when the data is sorted, useful for assessing the data\u0026rsquo;s central position.\nMode: The most frequently occurring value, helping identify the most common value in the dataset.\nStandard Deviation: A measure of the spread or dispersion of data points around the mean.\nThese statistical techniques offer a snapshot of the data\u0026rsquo;s characteristics and variability, setting the stage for more in-depth analysis.\nData Visualization and Reporting Data visualization is a powerful tool for understanding data at a glance. EDA relies heavily on creating visual representations to grasp the data\u0026rsquo;s distribution and relationships:\nHistograms: Visualize the frequency distribution of numerical data by dividing it into bins and displaying the number of data points in each bin.\nScatter Plots: Plot points on a graph to show how one variable relates to another, revealing patterns or correlations.\nBox Plots: Display the distribution of data through quartiles, helping identify outliers and the spread of the data.\nThese visualization techniques offer intuitive insights into the data\u0026rsquo;s structure and relationships, allowing you to make informed decisions about subsequent analysis steps.\nOutliers, Missing Values, and Common Data Issues During EDA, it\u0026rsquo;s crucial to identify and address anomalies that might distort your analysis:\nOutliers: Data points that deviate significantly from the rest of the data. Outliers could signal errors or interesting phenomena.\nMissing Values: Identifying areas with missing data and deciding how to handle them is important to avoid biased analysis.\nData Issues: Through visualization and statistical techniques, you can detect potential data entry errors, inconsistencies, or outliers that could affect your conclusions.\nBy mastering EDA techniques, you\u0026rsquo;ll develop a keen eye for patterns, become adept at spotting potential data issues, and acquire the skills needed to lay a solid foundation for more advanced analyses.\nAs we journey further into the world of data analysis, remember that EDA is not just a one-time process; it\u0026rsquo;s an iterative practice that helps refine your understanding of data as you progress. The insights gained through EDA pave the way for meaningful and impactful analytics.\n","description":"A guide to set the context and describe concepts for building and delivering data products in the new age of information economy.","keywords":null,"summary":"Welcome to \u0026ldquo;Analytics 101.\u0026rdquo; The purpose of this guide is to set the context of the information economy in which we exist and describe the important concepts and critical constraints to build data products that delivers value and delight customers.\nAnalytics in the digital age There is no doubt that we live in a digital age transacting in the information economy. The three legs of the digital stool are computers, software, and data.","title":"Analytics 101","url":"http://localhost:1315/docs/resources/analytics-101/"},{"body":"This guide will get you started with the InfinyOn Stateful Streaming Development Kit (SSDK), a utility that helps developers build and troubleshoot event-streaming data pipelines, and the data engineers run them in production.\nOverview for preview-5 enhanced state management with support for arrow data type and access from external services adaptors to external libraries such as http for call-outs and polars for table queries. Overview for preview-6 added support to multiple sources(merge) and multiple sinks(split) in each service. added support to pass environment variables to operators. Sample data pipeline The pipeline reads inputs from the sentence topic, applies a tumbling window service that computes the top three most frequently used words every 20 seconds.\nThe pipelines reads the sentences topic, partitions each sentence into words, increments their count, and stores the result in the count-per-word table. The pipeline also reads the probe-word topic, looks-up each word in the count-per-word table, and return the result to the word-counts topic.\nPrerequisites Building a Stateful Service data pipeline requires the following component :\nRust Rust 1.72 or beyond - Install Rust Installing Fluvio \u0026amp; Start a Cluster SSDK requires a Fluvio Cluster to consume, produce, and stream records between services.\nDownload and install the CLI.\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you must add the executables to your shell $PATH.\nStart a Local cluster:\n%copy first-line%\n$ fluvio cluster start If you prefer to run your cluster in InfinyON Cloud follow the instructions here.\nRun the following command to check the CLI and the Cluster platform versions:\n%copy first-line%\n$ fluvio version Your Fluvio cluster is ready for use.\nInstall and Setup SSDK SSDK is in preview and it requires the following image:\n%copy first-line%\n$ fvm install ssdk-preview6 Setup the SSDK environment (one-time command):\n%copy first-line%\n$ ssdk setup Your SSDK environment is ready to go.\nBuild your first Stateful Service data pipeline We are building a data pipeline that reads words from a topic, counts them, and publishes them to another topic. Use the following steps to create and test the project:\nCreate a data pipeline file Generate and run the project Test the data pipeline Let\u0026rsquo;s get started.\n1. Create a Data Pipeline file Open the terminal, and create a new directory:\n%copy%\n$ mkdir probe-words; cd probe-words Crate a file called data-pipeline.yaml and copy/paste the following content:\n%copy%\napiVersion: 0.3.0 meta: name: probe-words version: 0.1.0 namespace: examples config: converter: raw topics: sentences: schema: value: type: string probe-words: schema: value: type: string word-counts: schema: value: type: string converter: raw services: word-count-processing: sources: - type: topic id: sentences transforms: states: - name: count-per-word type: keyed-state properties: key: type: string value: type: arrow-row properties: count: type: i32 steps: - operator: flat-map run: | fn split_sequence(sentence: String) -\u0026gt; Result\u0026lt;Vec\u0026lt;String\u0026gt;, String\u0026gt; { Ok(sentence.split_whitespace().map(String::from).collect()) } - operator: assign-key run: | fn assign_key_word(word: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { Ok(word.to_lowercase().chars().filter(|c| c.is_alphanumeric()).collect()) } - operator: update-state run: | fn count_word(_word: String) -\u0026gt; Result\u0026lt;(), String\u0026gt; { let mut state = count_per_word(); state.count += 1; state.update(); Ok(()) } look-up-word: sources: - type: topic id: probe-words transforms: states: - name: count-per-word from: word-count-processing.count-per-word steps: - operator: map run: | fn query_word_count(word: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { use polars::prelude::{col,lit,IntoLazy}; let df = count_per_word(); let val = df .clone() .lazy() .filter(col(\u0026#34;id\u0026#34;).eq(lit(word.clone()))) .collect() .expect(\u0026#34;parse\u0026#34;); println!(\u0026#34;{:#?}\u0026#34;, val); if let Some(count) = val.column(\u0026#34;count\u0026#34;).unwrap().i32().unwrap().get(0) { Ok(format!(\u0026#34;word: {} count: {}\u0026#34;, word, count)) } else { Ok(format!(\u0026#34;word: {} not found\u0026#34;, word)) } } sinks: - type: topic id: word-counts This examples covers all new constructs introduced in preview-5:\narror-row table polars adapter ref-state to access states from different services. In addition, the data pipelines shows how to build services that are linked through the state object rather than topics.\n2. Generate and run the project Generate command parses the data-pipeline.yaml file and builds the project:\n%copy first-line%\n$ ssdk generate The code generated from the yaml file is maintained by ssdk, and it is not meant to be modified directly but rather through ssdk update. If you are interested in what\u0026rsquo;s under the hood, inspect the project directory:\n%copy first-line%\n$ tree .ssdk/project -\u0026gt; In upcoming releases, we\u0026rsquo;ll expose additional tooling for composition that will allow you to write, test functions and services separately, then compose them into a data pipeline.\nRun ssdk build to compile the WASM binaries:\n%copy first-line%\n$ ssdk build Note The the compilation will take several minutes due to the polars adapter. Just grab a drink and it should be done. If you end-up changing the code, just use ssdk update and it should be faster. We\u0026rsquo;ll shorted the compilation time in upcoming releases.\nLet\u0026rsquo;s run the project:\n%copy first-line%\n$ ssdk run --ui loading workflow at \u0026#34;data-pipeline.yaml\u0026#34; sucessfully read service file, executing Please visit http://127.0.0.1:8000 to view your workflow visualization ... \u0026gt;\u0026gt; Note:\nThe run command looks-up the topics in the cluster and automatically creates them if they don\u0026rsquo;t exist. The --ui flag generates a visual representation of the data pipeline at http://127.0.0.1:8000. When you close the run intractive editor, the data pipeline stops processing records. 3. Test the Data Pipeline To test the data pipeline, we\u0026rsquo;ll first populate with with sentences, then probe a few words.\nProduce to sentences Let\u0026rsquo;s write a series of famous quotes into the sentences topic:\n%copy first-line%\n$ echo \u0026#34;behind every great man is a woman rolling her eyes\u0026#34; | fluvio produce sentences %copy first-line%\n$ echo \u0026#34;the eyes reflect what is in the heart and soul\u0026#34; | fluvio produce sentences %copy first-line%\n$ echo \u0026#34;keep your eyes on the stars and your feet on the ground\u0026#34; | fluvio produce sentences Alernatively, you can open up the producer in intractive mode with fluvio produce command\nCheckout the internal state Let\u0026rsquo;s take a look at what the internal state of the count-per-word state looks like:\n%copy first-line%\n\u0026gt;\u0026gt; show state word-count-processing/count-per-word/state --table Key count a 1 and 2 behind 1 every 1 eyes 3 feet 1 great 1 ... There are 23 words in total.\nProbe for words and watch the result. In a separate terminal, produce words on probe-words topic:\n%copy first-line%\n$ echo \u0026#34;eyes\u0026#34; | fluvio produce probe-words Let\u0026rsquo;s open a third terminal, and consume from word-counts topic:\n%copy first-line%\n$ fluvio consume -B word-counts The result is as expected:\nword: eyes count: 3 \u0026#x1f389; Congratulations! Your first Stateful Service is up and running!\nFeel free to try other words.\nNext Step Stay tuned for preview 7, where we\u0026rsquo;ll add other features brought up in the feedback sessions.\n","description":"Guide on how to create Stateful Services data pipelines.","keywords":null,"summary":"This guide will get you started with the InfinyOn Stateful Streaming Development Kit (SSDK), a utility that helps developers build and troubleshoot event-streaming data pipelines, and the data engineers run them in production.\nOverview for preview-5 enhanced state management with support for arrow data type and access from external services adaptors to external libraries such as http for call-outs and polars for table queries. Overview for preview-6 added support to multiple sources(merge) and multiple sinks(split) in each service.","title":"Getting Started with Stateful Services","url":"http://localhost:1315/docs/stateful-services/getting-started/"},{"body":"Jolt provides JSON to JSON transformation\nThis is a map-type SmartModule that transforms JSON records leveraging Fluvio Jolt library, which has its own DSL (Domain Specific Language) to remove the need for coding simple transformations.\nThe transformations in Jolt are a set of operations that are sequentially performed over incoming records.\n-\u0026gt; Jolt only works on JSON text records.\nThere are three main types of operations:\nShift - move the field from one location to another Default - specify a default value for the field, if not present Remove - delete the field from the object There can be a mix and match of transformations applied at the same time. Let\u0026rsquo;s see below:\nSpecification example [ { \u0026#34;operation\u0026#34;: \u0026#34;remove\u0026#34;, // remove field $.id from incoming JSON object \u0026#34;spec\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;\u0026#34; } }, { \u0026#34;operation\u0026#34;: \u0026#34;shift\u0026#34;, // move everything inside $.data \u0026#34;spec\u0026#34;: { \u0026#34;*\u0026#34;: \u0026#34;data.\u0026amp;0\u0026#34;, } }, { \u0026#34;operation\u0026#34;: \u0026#34;default\u0026#34;, // if $.data.source does not exist, add it with value \u0026#34;http-connector\u0026#34; \u0026#34;spec\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;http-connector\u0026#34; } } } ] The Jolt SmartModule applies the operations in sequence: remove followed by shift followed by default.\nUsage example First, we need to download it to our cluster:\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.1.0 Second, we create a file transform.yaml with transformation specification defined above:\n%copy%\n# transform.yaml transforms: - uses: infinyon/jolt@0.1.0 with: spec: - operation: remove spec: id: \u0026#34;\u0026#34; - operation: shift spec: \u0026#34;*\u0026#34;: \u0026#34;data.\u0026amp;0\u0026#34; - operation: default spec: data: source: \u0026#34;http-connector\u0026#34; We\u0026rsquo;ll use 2 terminals. One each for producer and consumer for our topic jolt-topic.\nCreate the topic jolt-topic. Pick either terminal to run this in.\n%copy first-line%\n$ fluvio topic create jolt-topic topic \u0026#34;jolt-topic\u0026#34; created Next, in one terminal we start our producer. It will use transform.yaml to modify data before sending it to jolt-topic.\nIn the other terminal, and before we send data through the producer, start our consumer session.\n%copy first-line%\n$ fluvio produce --transforms-file ./transform.yaml jolt-topic \u0026gt; {} Ok! \u0026gt; {\u0026#34;id\u0026#34;:1, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;1111\u0026#34; } Ok! \u0026gt; {\u0026#34;id\u0026#34;:1, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;1111\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34; } Ok! \u0026gt; {\u0026#34;id\u0026#34;:1, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;mqtt-connector\u0026#34; } In the consumer terminal, we see the results of the transformation by Jolt SmartModule.\n%copy first-line%\n$ fluvio consume jolt-topic Consuming records from \u0026#39;jolt-topic\u0026#39; {\u0026#34;data\u0026#34;:{\u0026#34;source\u0026#34;:\u0026#34;http-connector\u0026#34;}} {\u0026#34;data\u0026#34;:{\u0026#34;account\u0026#34;:\u0026#34;1111\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;John Smith\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;http-connector\u0026#34;}} {\u0026#34;data\u0026#34;:{\u0026#34;account\u0026#34;:\u0026#34;1111\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;John Smith\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;http-connector\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;custom\u0026#34;}} {\u0026#34;data\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;John Smith\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;mqtt-connector\u0026#34;}} ⠒ ","description":"","keywords":null,"summary":"Jolt provides JSON to JSON transformation\nThis is a map-type SmartModule that transforms JSON records leveraging Fluvio Jolt library, which has its own DSL (Domain Specific Language) to remove the need for coding simple transformations.\nThe transformations in Jolt are a set of operations that are sequentially performed over incoming records.\n-\u0026gt; Jolt only works on JSON text records.\nThere are three main types of operations:\nShift - move the field from one location to another Default - specify a default value for the field, if not present Remove - delete the field from the object There can be a mix and match of transformations applied at the same time.","title":"Jolt","url":"http://localhost:1315/docs/smartmodules/jolt/"},{"body":"Role We are looking for a Rust expert passionate about building reliable and scalable infrastructure for real-time data streaming. If you are passionate about Rust and want to contribute to the next generation open-source distributed systems that will change the way we develop scalable apps, you are in the right place. You will be turning cutting edge ideas like event streaming, WebAssembly, and Serverless into a self-service platform. Checkout our open source project fluvio.io for details.\nResponsibilities Build and maintain Cloud native microservices Design and implement solutions to provision and scale infrastructure Improve performance by designing and implementing low-latency and high-availability network services Write reusable, testable, and efficient code Collaborate with team members working across the platform and contribute to product roadmap milestones Develop and maintain features in Fluvio and InfinyOn Cloud About You You are excited to code in Rust You have contributed to open-source projects in the past You are passionate about building a robust, distributed systems software You want work in startup and focus getting things done You possess excellent communication skills You are a team player Requirements 3+ years in Rust programming language 5+ years of building distributed systems in Scala/Java, Node/TypeScript, Python, Ruby, or Rust Experience in Test Driven Development Knowledge of cloud technologies Exposure to Kubernetes desired Strong problem-solving skills Proficient in English (spoken and written) ","description":"The Infrastructure team is looking for a Rust engineer to build reliable and scalable infrastructure for real-time data streaming. If you are an expert Rust engineer and have a passion for writing scalable distributed systems software, you are in the right place.","keywords":null,"summary":"Role We are looking for a Rust expert passionate about building reliable and scalable infrastructure for real-time data streaming. If you are passionate about Rust and want to contribute to the next generation open-source distributed systems that will change the way we develop scalable apps, you are in the right place. You will be turning cutting edge ideas like event streaming, WebAssembly, and Serverless into a self-service platform. Checkout our open source project fluvio.","title":"Sr. Rust Software Engineer (Infrastructure)","url":"http://localhost:1315/careers/infrastructure-engineer-senior-level/"},{"body":"This guide shows an end-to-end event pipeline that reads an event from a webhook, generates a formatted string, and publishes the result to Slack. While this is a simple example, it has many event notification use cases, such as:\nsubmission from website forms (via Cloudflare workers or your custom backend). activity from e-commerce platforms on purchases and shopping carts. notifications from github on your projects\u0026rsquo; activities. alerts from financial products on your transactions. notifications from any product that can invoke a webhook. This pipeline uses the following features:\nwebhook: that creates a public API to receive external events, transform them, and publish them to a topic. http-sink: that listens to the same topic and publishes them on Slack. Objective Show an example of how to build an event streaming pipeline that receives webhook events, transforms the input into a readable form, and generates an alert. We assume the events are generated by a user submitting a form, and we\u0026rsquo;ll format it accordingly.\nPrerequisites Fluvio CLI installed locally Account on InfinyOn Cloud Step-by-Step Create webhook configuration file Create http-sink configuration file Download SmartModules Start Webhook and Connector Test Data Pipeline Create webhook configuration file Create a webhook configuration file called form-webhook.yaml :\n%copy%\nmeta: name: form-webhook topic: form-events webhook: outputParts: body outputType: json transforms: - uses: infinyon-labs/json-formatter@0.1.0 with: spec: match: - key: \u0026#34;/type\u0026#34; value: \u0026#34;subscribe\u0026#34; format: with: \u0026#34;:loudspeaker: {} ({}) subscribed on {}\u0026#34; using: - \u0026#34;/name\u0026#34; - \u0026#34;/email\u0026#34; - \u0026#34;/source\u0026#34; output: \u0026#34;/formatted\u0026#34; - key: \u0026#34;/type\u0026#34; value: \u0026#34;use-case\u0026#34; format: with: \u0026#34;:confetti_ball: {} ({}) wants to solve the following \u0026#39;{}\u0026#39; use-case:\\n\u0026gt;{}\u0026#34; using: - \u0026#34;/name\u0026#34; - \u0026#34;/email\u0026#34; - \u0026#34;/source\u0026#34; - \u0026#34;/description\u0026#34; output: \u0026#34;/formatted\u0026#34; default: format: with: \u0026#34;{} ({}) submitted a request\u0026#34; using: - \u0026#34;/name\u0026#34; - \u0026#34;/email\u0026#34; output: \u0026#34;/formatted\u0026#34; The webhook reads the JSON body, applies the json-formatter smartmodule to generate readable text, and writes the new record to a topic called form-events. Checkout labs-json-formatter-sm in github for additional information.\nCreate http-sink configuration file Create an HTTP source connector configuration file called slack-form-alerts.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: slack-form-alerts type: http-sink topic: form-events secrets: - name: SLACK_USER_ALERTS http: endpoint: \u0026#34;https://hooks.slack.com/services/${{ secrets.SLACK_USER_ALERTS }}\u0026#34; headers: - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;formatted\u0026#34;: \u0026#34;text\u0026#34; The sink connector reads from the form-events topic and uses the jolt smartmodule to shift the formatted string into a field called text per the Slack instructions. Checkout fluvio-jolt in github for additional information.\nAdd Slack webhook token to InfinyOn Secrets The Slack webhook link is sensitive information, let\u0026rsquo;s add the access token part to secret in InfinyOn Cloud :\n%copy%\n$ fluvio cloud secret set SLACK_USER_ALERTS \u0026lt;webhook-token\u0026gt; Check out Slack Webhooks on how to create the webhook token.\nDownload SmartModules Download the smartmodules used by the webhook ad the connector:\n%copy%\n$ fluvio hub download infinyon/jolt@0.3.0 $ fluvio hub download infinyon-labs/json-formatter@0.1.0 Check fluvio smartmodule list to ensure they\u0026rsquo;ve been downloaded.\nStart Webhook and Connector Start webhook listener:\n%copy%\n$ fluvio cloud webhook create --config form-webhook.yaml Check fluvio cloud webhook list to ensure it has been successfully provisioned. In checkout the webhook link that we\u0026rsquo;ll use to test the pipeline: https://infinyon.cloud/webhooks/v1/[token]\nStart sink connector:\n%copy%\n$ fluvio cloud connector create -c slack-form-alerts.yaml Check fluvio cloud connector list to ensure it has been successfully provisioned.\nTest Data Pipeline Use curl to send a POST request with a fictious user request to our webhook link. In production environments, this iw what a website would send:\n%copy%\n$ curl -X POST https://infinyon.cloud/webhooks/v1/\u0026lt;token\u0026gt; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;alice@acme.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Alice Liddell\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;subscribe\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;front-page\u0026#34; }\u0026#39; The following alert is displayed in Slack:\n`:loudspeaker: Alice Liddell (\u0026#34;alice@acme.com) subscribed on front-page` will show-up in your slack channel. That\u0026rsquo;s all folks!\nReferences Webhook Basics Webhook Configuration File Cloudflare workers JSON formatter SmartModule ","description":"Event data pipeline that recevies events from an InfinyOn Cloud Webhook and sends an alert to Slack","keywords":null,"summary":"This guide shows an end-to-end event pipeline that reads an event from a webhook, generates a formatted string, and publishes the result to Slack. While this is a simple example, it has many event notification use cases, such as:\nsubmission from website forms (via Cloudflare workers or your custom backend). activity from e-commerce platforms on purchases and shopping carts. notifications from github on your projects\u0026rsquo; activities. alerts from financial products on your transactions.","title":"Webhook Events to Slack Notifications","url":"http://localhost:1315/docs/guides/webhook-to-slack/"},{"body":"","description":"A short tutorial on Data Streaming","keywords":null,"summary":"","title":"Data Streaming Basics","url":"http://localhost:1315/docs/tutorials/streaming-basics/"},{"body":" Do we really need another data streaming platform? Our response is a resounding YES!\nThe reason for the emergence of several new real-time data streaming platforms and databases is that current data processing platforms have limitations and fail to deliver the expected value and experience. The availability of modern programming paradigms in the Rust Programming Language and Web Assembly (WASM) presents an opportunity to reinvent the wheel.\nFor the past 5 years, we have been developing our core data processing platform using Rust. Meanwhile, WASM has become a powerful runtime for expressing operations in various web-friendly languages. There is no doubt about the capabilities of Rust compared to other programming languages, and we firmly believe that this is how data streaming will become accessible to the rest of the software world beyond the tech giants.\nHow is the InfinyOn Cloud Platform different compared to all the other available streaming platforms? InfinyOn Cloud data streaming platform is a unified, composable, stateful solution purpose-built for stream processing. InfinyOn Cloud is a managed delpyment of the Fluvio open source stream processing runtime. Distributed stream processing has been around for almost two decades. There are several feature-rich data streaming paradigms available in Java, Scala, Clojure, C++, and more recently Python and Go. The more mature platforms are primarily based on Java and Scala.\nTaking into account the lessons learned from software and data engineering challenges over the past couple of decades, we have developed a data streaming platform that is:\nUnified: It is a single platform to collect, transform, deduplicate, materialize, and dispatch data.\nComposable: Our approach aligns with the system-level programming principles of the Rust programming language. We have prioritized the development of core stream processor primitives, focusing on building blocks before introducing layers of abstraction. Our platform components function as Lego blocks, allowing users to construct data pipelines using various patterns on top of the core streaming runtime.\nThe implications of this approach are significant! At a high level of abstraction, you can use simple YAML files to express Domain Specific Language (DSL) for data flows. This includes using inbound connectors or clients to collect data, performing transformation operations using WASM in flight, and utilizing outbound connectors or clients to consume data through materialized views. Additionally, you can dispatch data to actionable streams or downstream data stores. Depending on your data sources and transformation requirements, you have the flexibility to choose delivery semantics such as at least once, at most once, or exactly once. You can also control partitioning behaviors, caching, mirroring, cluster profiles, and stateful aggregations based on your specific needs. At the system level, you can create custom deployments and clients using the Fluvio SDK. Furthermore, you can develop inbound or outbound data connectors using the connector development kit, as well as build transformation operators using the smart modules development kit.\nStateful: One of the biggest complaints and challenges with stream processing platforms not written in Java is the lack of state management abilities. Reliable data delivery guarantees and the ability to manage state and offset are essential for most data pipelines. However, stateful distributed stream processing remains one of the most complex aspects of the data streaming paradigm. The Fluvio streaming platform is ready to launch stateful stream processing. Our upcoming release, on-stream materialized views, is currently undergoing validation with a select group of beta customers.\nStreaming platforms are a pain! What is the level of difficulty of setup, configuration, and maintenance of the platform? The InfinyOn Cloud platform is designed for composability and simplicity. It leverages the Fluvio stream processing runtime, which is a single binary deployment featuring a well-designed command line interface (CLI) and support for clients in Rust, Python, Node, and Go.\nInteracting with the managed cloud version is easy through both the CLI and the clients. For self-managed installations, deployment and orchestration are flexible and straightforward.\nHow does the InfinyOn Cloud platform scale as the data pipelines grow? Infinyon Cloud offers a managed deployment of the Fluvio streaming runtime.\nThe Fluvio streaming runtime is a compact binary that can be run on edge devices and containers with limited memory, storage, and compute resources. It enables stateful, exactly-once stream processing on a single partition, capable of handling millions of records every minute. By utilizing multiple partitions, the platform scales linearly to process the amount of data you need to process.\nHow does InfinyOn Data Platform integrate with the remaining data stack? The composable nature of the InfinyOn Cloud platform and the focus on the primitives or building blocks allows for flexible configurations and customizations of inbound and outbound data sources to integrate seamlessly with your data stack. Connector development kit enables integration of bespoke inbound and outbound data sources by interacting with the core protocols, authentication, and API layers of web application sources, edge devices, sensors and IoT, BLOB storage, databases etc.\nIn addition to connectors, there are supported Rust, Python, Node and Go clients. Internally we have tested inbound and outbound webhook gateway as an on demand semantic data layer for accessing data.\u0026quot;\n","description":"A library of frequently asked questions collected from our users.","keywords":null,"summary":"Do we really need another data streaming platform? Our response is a resounding YES!\nThe reason for the emergence of several new real-time data streaming platforms and databases is that current data processing platforms have limitations and fail to deliver the expected value and experience. The availability of modern programming paradigms in the Rust Programming Language and Web Assembly (WASM) presents an opportunity to reinvent the wheel.\nFor the past 5 years, we have been developing our core data processing platform using Rust.","title":"Frequently Asked Questions (FAQs)","url":"http://localhost:1315/docs/resources/faqs/"},{"body":"This guide expects you to already have the Fluvio CLI installed, and InfinyOn Cloud set up. If neither of these is the case, please follow the instructions here!\nConnector Pipeline There are two main steps for this tutorial:\nCreating an Inbound HTTP Connector to collect JSON Receive data without any modifications JSON to JSON transformation before send to topic Creating an Outbound SQL Connector to insert the input JSON into a database Basic insert JSON to JSON transformation before insert We will be looking at the Inbound HTTP Connector setup, and connecting to the catfact.ninja database to ingest and store JSON data into a topic.\nThe Outbound connector will be using a PostgreSQL database. It will listen to the topic for new records and insert them into a table.\nYou can use your own PostgreSQL instance, if it can be reached over the internet. But you can still follow along by creating a PostgreSQL database at a hosting service, such as ElephantSQL.\nConnectors If you wish to automatically collect information from one source and send it to Fluvio, or send data from Fluvio to location, Connectors are the way to go. When given the information on the interface through the Connector configuration file, Fluvio can poll a multitude of input types.\nConnector Config Layout A detailed description of the Connector configuration file can be found in the Connector Config Layout page.\nInbound Connector For the HTTP-specific parameters you will need to specify the link it is polling, and the interval at which it polls.\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts create-topic: true http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s This creates a connector named cat-facts, that reads from the website https://catfact.ninja/fact every 10 seconds, and produces to the topic cat-facts.\nTesting the Inbound Connector You can register the connector to Fluvio with fluvio cloud connector create --config=\u0026lt;config-file.yaml\u0026gt;\n%copy first-line%\n$ fluvio cloud connector create --config=catfacts-basic-connector.yml You can use fluvio cloud connector list to view the status of the connector.\n%copy first-line%\n$ fluvio cloud connector list NAME TYPE VERSION CDK STATUS cat-facts http-source 0.1.0 V3 Running And fluvio consume to view the incoming data in the topic.\n%copy first-line%\n$ fluvio consume cat-facts-data -dT4 Consuming records starting 4 from the end of topic \u0026#39;cat-facts-data\u0026#39; {\u0026#34;fact\u0026#34;:\u0026#34;A cat lover is called an Ailurophilia (Greek: cat+lover).\u0026#34;,\u0026#34;length\u0026#34;:57} {\u0026#34;fact\u0026#34;:\u0026#34;British cat owners spend roughly 550 million pounds yearly on cat food.\u0026#34;,\u0026#34;length\u0026#34;:71} {\u0026#34;fact\u0026#34;:\u0026#34;Fossil records from two million years ago show evidence of jaguars.\u0026#34;,\u0026#34;length\u0026#34;:67} {\u0026#34;fact\u0026#34;:\u0026#34;Relative to its body size, the clouded leopard has the biggest canines of all animals\\u2019 canines. Its dagger-like teeth can be as long as 1.8 inches (4.5 cm).\u0026#34;,\u0026#34;length\u0026#34;:156} Inbound Connector with JSON to JSON transformation before writing to topic All Inbound Connectors support transformations which are applied before the data is sent to the topic. We can extend our config file to add an additional JSON to JSON transformation to records.\n%copy%\n# catfacts-basic-connector-with-transform.yml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts-transformed type: http-source topic: cat-facts-data-transformed http: endpoint: https://catfact.ninja/fact interval: 10s transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: default spec: source: \u0026#34;http\u0026#34; In this config, we add the field source with the static value http to every record. Note that if the field already exists, it will not be overwritten.\nBefore we create the connector we need to add infinyon/jolt@0.3.0 SmartModule to the cluster. This SmartModule uses a domain specific language (DSL) called Jolt, to specify a transformation of input JSON to another shape of JSON data.\nLet\u0026rsquo;s download this SmartModule from the SmartModule Hub.\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 Then, we create a connector just like before\n%copy first-line%\n$ fluvio cloud connector create --config catfacts-basic-connector-with-transform.yaml And fluvio consume to view the transformed data in the topic.\n%copy first-line%\n$ fluvio consume cat-facts-data-transformed -dT4 Consuming records starting 4 from the end of topic \u0026#39;cat-facts-data-transformed\u0026#39; {\u0026#34;fact\u0026#34;:\u0026#34;The Amur leopard is one of the most endangered animals in the world.\u0026#34;,\u0026#34;length\u0026#34;:68,\u0026#34;source\u0026#34;:\u0026#34;http\u0026#34;} {\u0026#34;fact\u0026#34;:\u0026#34;Some cats have survived falls of over 65 feet (20 meters), due largely to their “righting reflex.” The eyes and balance organs in the inner ear tell it where it is in space so the cat can land on its feet. Even cats without a tail have this ability.\u0026#34;,\u0026#34;length\u0026#34;:249,\u0026#34;source\u0026#34;:\u0026#34;http\u0026#34;} {\u0026#34;fact\u0026#34;:\u0026#34;In Holland’s embassy in Moscow, Russia, the staff noticed that the two Siamese cats kept meowing and clawing at the walls of the building. Their owners finally investigated, thinking they would find mice. Instead, they discovered microphones hidden by Russian spies. The cats heard the microphones when they turned on.\u0026#34;,\u0026#34;length\u0026#34;:318,\u0026#34;source\u0026#34;:\u0026#34;http\u0026#34;} {\u0026#34;fact\u0026#34;:\u0026#34;Cats can be right-pawed or left-pawed.\u0026#34;,\u0026#34;length\u0026#34;:38,\u0026#34;source\u0026#34;:\u0026#34;http\u0026#34;} Outbound Connector Setup For the SQL Outbound connector example, we will need to create a table in our Postgres database.\nRun this query in your database before starting any Outbound connectors.\n%copy%\nD create table animalfacts(length integer, raw_fact_json jsonb) We also need to run a few commands with fluvio to download some prepackaged SmartModules from the SmartModule Hub to attach to the Outbound Connector.\nThis SmartModule will do a basic mapping of the JSON input into a SQL statement for the Outbound SQL connector\n%copy first-line%\n$ fluvio hub sm download infinyon/json-sql@0.2.1 If you have not added infinyon/jolt@0.3.0 SmartModule on previous steps, we need to add it as well:\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 For more info about the SmartModule Hub, check out the Hub Overview page\nOutbound SQL with basic SQL inserts In this connector, we will listen in on the cat-facts topic. Whenever a new fact is produced to the topic, the Outbound SQL connector will insert the record into a table named animalfacts. The length in one column called length and the entire JSON in another column raw_fact_json.\n%copy%\n# sql.yaml apiVersion: 0.1.0 meta: name: simple-cat-facts-sql type: sql-sink version: 0.3.3 topic: cat-facts sql: url: \u0026#34;postgres://user:password@db.postgreshost.example/dbname\u0026#34; transforms: - uses: infinyon/json-sql@0.2.1 invoke: insert with: mapping: table: \u0026#34;animalfacts\u0026#34; map-columns: \u0026#34;length\u0026#34;: json-key: \u0026#34;length\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;raw_fact_json\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true And we create the Outbound connector just like the Inbound connector\n%copy first-line%\n$ fluvio cloud connector create --config sql-basic.yml connector \u0026#34;simple-cat-facts-sql\u0026#34; (sql-sink) created After a few seconds, we can see data in the PostgreSQL table,\n%copy first-line%\nD select * from animalfacts; +--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | length | raw_fact_json | |--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | 74 | {\u0026#34;fact\u0026#34;: \u0026#34;A cat’s jaw can’t move sideways, so a cat can’t chew large chunks of food.\u0026#34;, \u0026#34;length\u0026#34;: 74} | | 110 | {\u0026#34;fact\u0026#34;: \u0026#34;Unlike humans, cats are usually lefties. Studies indicate that their left paw is typically their dominant paw.\u0026#34;, \u0026#34;length\u0026#34;: 110} | | 114 | {\u0026#34;fact\u0026#34;: \u0026#34;A commemorative tower was built in Scotland for a cat named Towser, who caught nearly 30,000 mice in her lifetime.\u0026#34;, \u0026#34;length\u0026#34;: 114} | | 98 | {\u0026#34;fact\u0026#34;: \u0026#34;Statistics indicate that animal lovers in recent years have shown a preference for cats over dogs!\u0026#34;, \u0026#34;length\u0026#34;: 98} | | 78 | {\u0026#34;fact\u0026#34;: \u0026#34;Approximately 1/3 of cat owners think their pets are able to read their minds.\u0026#34;, \u0026#34;length\u0026#34;: 78} | | 95 | {\u0026#34;fact\u0026#34;: \u0026#34;At 4 weeks, it is important to play with kittens so that they do not develope a fear of people.\u0026#34;, \u0026#34;length\u0026#34;: 95} | | 46 | {\u0026#34;fact\u0026#34;: \u0026#34;Jaguars are the only big cats that don\u0026#39;t roar.\u0026#34;, \u0026#34;length\u0026#34;: 46} | | 31 | {\u0026#34;fact\u0026#34;: \u0026#34;Female felines are \\\\superfecund\u0026#34;, \u0026#34;length\u0026#34;: 31} | +--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Outbound SQL with JSON to JSON transformation before insert In this connector, we will listen in on the cat-facts topic.\nBut before we insert into the database, we specify a transformation. The resulting JSON we see inserted in the table has the length removed, and adds type: cat to every JSON.\n%copy%\n# sql-chain.yaml apiVersion: 0.1.0 meta: name: transform-cat-facts-sql type: sql-sink version: 0.3.3 topic: cat-facts-data sql: url: \u0026#34;postgres://user:password@db.postgreshost.example/dbname\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: fact: \u0026#34;animal.fact\u0026#34; length: \u0026#34;length\u0026#34; - operation: default spec: animal: type: \u0026#34;cat\u0026#34; - uses: infinyon/json-sql@0.2.1 with: invoke: insert mapping: table: \u0026#34;animalfacts\u0026#34; map-columns: \u0026#34;length\u0026#34;: json-key: \u0026#34;length\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;raw_fact_json\u0026#34;: json-key: \u0026#34;animal\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true Create another connector with our transformations.\n%copy first-line%\n$ fluvio cloud connector create --config sql-transform.yml connector \u0026#34;transform-cat-facts-sql\u0026#34; (sql-sink) created After a few seconds, we can see data in the PostgreSQL table with our configured transformations.\n%copy first-line%\nD select * from animalfacts; +--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | length | raw_fact_json | |--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | 58 | {\u0026#34;fact\u0026#34;: \u0026#34;A cat can spend five or more hours a day grooming himself.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 110 | {\u0026#34;fact\u0026#34;: \u0026#34;Unlike humans, cats are usually lefties. Studies indicate that their left paw is typically their dominant paw.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 163 | {\u0026#34;fact\u0026#34;: \u0026#34;Retractable claws are a physical phenomenon that sets cats apart from the rest of the animal kingdom. I n the cat family, only cheetahs cannot retract their claws.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 78 | {\u0026#34;fact\u0026#34;: \u0026#34;Approximately 1/3 of cat owners think their pets are able to read their minds.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 145 | {\u0026#34;fact\u0026#34;: \u0026#34;A sexually-active feral tom-cat \\\\owns\\\\\\\u0026#34; an area of about three square miles and \\\\\\\u0026#34;\\\u0026#34;sprays\\\\\\\u0026#34;\\\u0026#34; to mark his territory with strong smelling urine.\\\u0026#34;\\\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 149 | {\u0026#34;fact\u0026#34;: \u0026#34;It has been scientifically proven that owning cats is good for our health and can decrease the occurrence of high blood pressure and other illnesses.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | | 73 | {\u0026#34;fact\u0026#34;: \u0026#34;In relation to their body size, cats have the largest eyes of any mammal.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cat\u0026#34;} | +--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Deleting connectors To stop the traffic from all the connectors, you run fluvio cloud connector delete \u0026lt;connector name\u0026gt;.\nThis will delete the connector, but not the topic is was attached to\n%copy first-line%\n$ fluvio cloud connector delete cat-facts cat-facts-transformed simple-cat-facts-sql transform-cat-facts-sql connector \u0026#34;cat-facts\u0026#34; deleted connector \u0026#34;cat-facts-transformed\u0026#34; deleted connector \u0026#34;simple-cat-facts-sql\u0026#34; deleted connector \u0026#34;transform-cat-facts-sql\u0026#34; deleted Conclusion We used the Inbound HTTP Connector to ingest JSON data from an endpoint and save it in a topic. We configured transformation of outgoing JSON records.\nThere was a brief introduction to the SmartModule Hub, which enabled the Outbound SQL connector to consume data.\nWith the Outbound SQL Connector, we utilized SmartModules in two different ways.\nBasic insert into a table, with a simple mapping of JSON fields into columns Configured transformation of the incoming JSON before following the same mapping process Check out these Other Tutorials SmartModules with smdk Creating a Data Pipeline References Fluvio CLI Produce Fluvio CLI Consume Fluvio CLI topic Fluvio CLI profile SmartModule SmartModule Rust API Transformations ","description":"Data pipeline that periodically reads from a websitea and sends the result to a SQL database.","keywords":null,"summary":"This guide expects you to already have the Fluvio CLI installed, and InfinyOn Cloud set up. If neither of these is the case, please follow the instructions here!\nConnector Pipeline There are two main steps for this tutorial:\nCreating an Inbound HTTP Connector to collect JSON Receive data without any modifications JSON to JSON transformation before send to topic Creating an Outbound SQL Connector to insert the input JSON into a database Basic insert JSON to JSON transformation before insert We will be looking at the Inbound HTTP Connector setup, and connecting to the catfact.","title":"HTTP to SQL","url":"http://localhost:1315/docs/guides/http-to-sql/"},{"body":"This is a map-type SmartModule that converts arbitrary JSON records into SQL model, which is a self-describing representation of SQL INSERT statements. This SmartModule is intended to be used in [SQL Sink Connector][sql-sink-connector], to execute a command in a SQL database.\nMapping The mapping between incoming JSON records and the resulting SQL record is defined in the configuration of the SmartModule in the mapping init parameter. Let\u0026rsquo;s look at the example:\n%copy%\n- uses: infinyon/json-sql@0.1.0 with: mapping: table: \u0026#34;target_table\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; required: true \u0026#34;device_type\u0026#34;: json-key: \u0026#34;device.type\u0026#34; value: type: \u0026#34;text\u0026#34; default: \u0026#34;mobile\u0026#34; \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true Here, we create insert statements to target_table database table. Each statement has three columns:\ndevice_id with int type. The value for this column will looked-up at the following hierarchy $.device.device_id of the input record. If it is not present, the error will be thrown, as the mapping states, it is a required field. device_type - text field with mobile marked as default value. Not required. Will be taken from $.device.type hierarchy. record - jsonb column that contains the whole input record. With the given mapping, the Json-Sql SmartModule will convert the input:\n{ \u0026#34;device\u0026#34;: { \u0026#34;device_id\u0026#34;: 1 } } into the following output:\n{ \u0026#34;Insert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;target_table\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;{\\\u0026#34;device\\\u0026#34;:{\\\u0026#34;device_id\\\u0026#34;:1}}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Json\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;device_type\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;mobile\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;device_id\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34; } ] } } which is equivalent to the following SQL statement:\nINSERT INTO \u0026#39;target_table\u0026#39; (record, device_type, device_id) VALUES (\u0026#39;{\u0026#34;device\u0026#34;:{\u0026#34;device_id\u0026#34;:1}}\u0026#39;, \u0026#39;mobile\u0026#39;, 1) Data types The list of supported types and corresponding types from SQL model:\nMapping Model int8, bigint BigInt int4, int, integer Int int2, smallint SmallInt bool, boolean Bool bytes, bytea Bytes text Text float4, float, real Float float8, \u0026ldquo;double precision\u0026rdquo;, doubleprecision DoublePrecision decimal, numeric Numeric date Date time Time timestamp Timestamp json, jsonb Json uuid Uuid Usage example First, we need to download it to our cluster from SmartModule Hub:\n%copy first-line%\n$ fluvio hub download infinyon/json-sql@0.1.0 Second, we create a file transform.yaml with the mapping used before:\n%copy%\n# transform.yaml transforms: - uses: infinyon/json-sql@0.1.0 with: mapping: table: \u0026#34;target_table\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; required: true \u0026#34;device_type\u0026#34;: json-key: \u0026#34;device.type\u0026#34; value: type: \u0026#34;text\u0026#34; default: \u0026#34;mobile\u0026#34; \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true Now let\u0026rsquo;s call smdk test to see the result:\n%copy first-line%\n$ smdk test --text \u0026#39;{\u0026#34;device\u0026#34;:{\u0026#34;device_id\u0026#34;:1}}\u0026#39; --transforms-file ./transform.yaml {\u0026#34;Insert\u0026#34;:{\u0026#34;table\u0026#34;:\u0026#34;target_table\u0026#34;,\u0026#34;values\u0026#34;:[{\u0026#34;column\u0026#34;:\u0026#34;record\u0026#34;,\u0026#34;raw_value\u0026#34;:\u0026#34;{\\\u0026#34;device\\\u0026#34;:{\\\u0026#34;device_id\\\u0026#34;:1}}\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;Json\u0026#34;},{\u0026#34;column\u0026#34;:\u0026#34;type\u0026#34;,\u0026#34;raw_value\u0026#34;:\u0026#34;mobile\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;Text\u0026#34;},{\u0026#34;column\u0026#34;:\u0026#34;device_id\u0026#34;,\u0026#34;raw_value\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;Int\u0026#34;}]}} As mentioned at the beginning of this page, the outputed records can be consumed by SQL Sink Connector to be executed on SQL database. By convention, SQL Sink connector expects JSON inputs.\nFor additional examples checkout the tutorials:\nHTTP to SQL MQTT to SQL ","description":"","keywords":null,"summary":"This is a map-type SmartModule that converts arbitrary JSON records into SQL model, which is a self-describing representation of SQL INSERT statements. This SmartModule is intended to be used in [SQL Sink Connector][sql-sink-connector], to execute a command in a SQL database.\nMapping The mapping between incoming JSON records and the resulting SQL record is defined in the configuration of the SmartModule in the mapping init parameter. Let\u0026rsquo;s look at the example:","title":"JSON to SQL Mapping","url":"http://localhost:1315/docs/smartmodules/json-sql/"},{"body":"Role We are looking for a connector engineer to develop both open source and cloud connectors using our revolutionary SmartConnector technology, which is built on top of WebAssembly and real-time fluvio streaming technology.\nResponsibilities Build and maintain source and destination connectors using Rust. Responsible for the entire connector lifecycle from inception, development, testing, operations, and support. Engage with both open source and cloud communities. Contribute and help shape the Connector roadmap. Initiate and shape Connector pattern framework and toolings. Requirements Experiences with Distributed Systems and Cloud technology. Strong knowledge of Database technology. Rust experience in either open-source or commercial products. Proficiency in one more more languages such as Python, Node, or Java. Strong problem-solving skills Excellent teamwork and collaboration. Proficient in English (spoken and written) Excellent communication skills ","description":"The Connector Engineering team's mission is to connect the world's data through InfinyOn's real-time data platform. You will develop open-source and cloud connectors using our revolutionary SmartConnector technology built on top of WebAssembly (WASM).","keywords":null,"summary":"Role We are looking for a connector engineer to develop both open source and cloud connectors using our revolutionary SmartConnector technology, which is built on top of WebAssembly and real-time fluvio streaming technology.\nResponsibilities Build and maintain source and destination connectors using Rust. Responsible for the entire connector lifecycle from inception, development, testing, operations, and support. Engage with both open source and cloud communities. Contribute and help shape the Connector roadmap.","title":"Sr. Software Engineer (Connectors)","url":"http://localhost:1315/careers/connectors-engineer-senior-level/"},{"body":"At the end of this tutorial, we will see data starting from an MQTT broker and ending in a PostgreSQL table.\nWe\u0026rsquo;ll use 2 connectors:\nInbound MQTT connector Outbound SQL connector There will be an example of combining multiple SmartModules, known as SmartModule chaining The Outbound connector will be using a PostgreSQL database. It will listen to the topic for new records and insert them into a table.\nYou can use your own PostgreSQL instance, if it can be reached over the internet. But you can still follow along by creating a PostgreSQL database at a hosting service, such as ElephantSQL.\nSetup Start MQTT connector Install mosquito for sending test data Start SQL connector(s) Connector with no transformation Connector with extra JSON to JSON transformation Install pgcli for validating DB inserts The actual test Publish JSON to MQTT broker View output in PostgreSQL Move transformation to MQTT connector Conclusion Setup Start MQTT Connector This connector expects to take json input from the MQTT broker, from an MQTT topic named ag-mqtt-topic. These parameters will be reflected in the final JSON payload that gets produced to the fluvio topic mqtt-topic\nMQTT connector config: mqtt.yml\n%copy%\n# mqtt.yml apiVersion: 0.1.0 meta: version: 0.2.5 name: fluvio-mqtt-connector type: mqtt-source topic: mqtt-topic direction: source create-topic: true mqtt: url: \u0026#34;mqtt://test.mosquitto.org/\u0026#34; topic: \u0026#34;ag-mqtt-topic\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json Create MQTT connector %copy first-line%\n$ fluvio cloud connector create --config mqtt.yml Install mosquito - MQTT client First install mosquito to follow later steps for sending JSON to our test MQTT broker\n-\u0026gt; On MacOS, you can install mosquitto with homebrew with the following command: brew install mosquitto\nStart SQL connector(s) You can start one of both of the following connectors\nConnector with no transformation Download SmartModule for example Example connector config Start connector Connector with extra JSON to JSON transformation Download SmartModules for example Example connector config Start connector SQL Connector with no transformation Download json-sql SmartModule Example output\n%copy first-line%\n$ fluvio hub sm download infinyon/json-sql@0.2.1 downloading infinyon/json-sql@0.2.1 to infinyon-json-sql-0.2.1.ipkg ... downloading complete ... checking package trying connection to fluvio router.dev.infinyon.cloud:9003 ... cluster smartmodule install complete SQL Connector with no transformation config %copy%\n# sql.yml apiVersion: 0.1.0 meta: name: fluvio-sql-connector type: sql-sink version: 0.2.5 topic: mqtt-topic create-topic: true sql: url: \u0026#34;postgres://user:password@db.postgreshost.example/dbname\u0026#34; transforms: - uses: infinyon/json-sql@0.2.1 with: invoke: insert mapping: table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;payload.device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true Start No transformation connector SQL connector\n%copy first-line%\n$ fluvio cloud connector create --config sql.yml Connector with JSON to JSON transformation Download the Jolt and Json-Sql SmartModules used by this example connector\nExample output\n%copy first-line%\n$ fluvio hub sm download infinyon/json-sql@0.2.1 downloading infinyon/json-sql@0.2.1 to infinyon-json-sql-0.2.1.ipkg ... downloading complete ... checking package trying connection to fluvio router.infinyon.cloud:9003 ... cluster smartmodule install complete %copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 downloading infinyon/jolt@0.3.0 to infinyon-jolt-0.3.0.ipkg ... downloading complete ... checking package trying connection to fluvio router.infinyon.cloud:9003 ... cluster smartmodule install complete Connector with JSON to JSON transformation config %copy%\n# sql-chain.yml apiVersion: 0.1.0 meta: name: fluvio-sql-connector-chain type: sql-sink version: 0.2.5 topic: mqtt-topic create-topic: true sql: url: \u0026#34;postgres://user:password@db.postgreshost.example/dbname\u0026#34; rust_log: \u0026#34;sql_sink=INFO,sqlx=WARN\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: payload: device: \u0026#34;device\u0026#34; - operation: default spec: device: type: \u0026#34;mobile\u0026#34; - uses: infinyon/json-sql@0.2.1 with: invoke: insert mapping: table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true Start SQL connector with JSON transformation\n%copy first-line%\n$ fluvio cloud connector create --config sql-chain.yml Install pgcli - PostgreSQL client Install pgcli to follow the later DB validation steps https://www.pgcli.com\nOn MacOS, you can install pgcli with homebrew with the following command:\n%copy first-line%\n$ brew install pgcli The actual test 📋 Using example JSON, this is the sequence of events that will occur\n(user) Publish JSON to MQTT broker (Inbound MQTT connector) Produce data to fluvio topic mqtt-topic Produce a transformed JSON object with config parameter data with the name of the MQTT topic embedded (Outbound SQL connector) Consume the inbound record from topic mqtt-topic Apply transformations to record (JSON to JSON connector only) Insert record into DB (user) Validate JSON record in PostgreSQL database 👉 If you are starting with a new database, you will need to create the table before sending messages to MQTT. It is not created automatically.\nTable create query\n%copy first-line%\ncreate table topic_message(device_id int, record jsonb); This is what our input JSON to MQTT looks like\nexample JSON (formatted)\n{ \u0026#34;device\u0026#34;: { \u0026#34;device_id\u0026#34;: 17, \u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34; } } Publish JSON to MQTT broker Run the following to send a test JSON message to the demo MQTT broker with mosquito ([Installation steps](h#the-actual-test\nCommand:\n%copy first-line%\n$ mosquitto_pub -h test.mosquitto.org -t ag-mqtt-topic -m \u0026#39;{\u0026#34;device\u0026#34;: {\u0026#34;device_id\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;device17\u0026#34;}}\u0026#39; Produced data in topic:\n%copy first-line%\n$ fluvio consume mqtt-topic -B Consuming records from the beginning of topic \u0026#39;mqtt-topic\u0026#39; {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;ag-mqtt-topic\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;device\u0026#34;:{\u0026#34;device_id\u0026#34;:17,\u0026#34;name\u0026#34;:\u0026#34;device17\u0026#34;}}} Produced data in topic: Run the following to connect to PostgreSQL DB with pgcli (Installation steps)\nView output in PostgreSQL Use pgcli to examine the database.\n%copy first-line%\n$ pgcli -U user -h db.postgreshost.example -p 5432 dbname Check that the JSON from MQTT has been inserted into table\n%copy first-line%\nselect * from topic_message; Example output from both connectors\n+-----------+-----------------------------------------------------------------------------------------------+ | device_id | record | |-----------+-----------------------------------------------------------------------------------------------| | 17 | {\u0026#34;payload\u0026#34;: {\u0026#34;device\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;device_id\u0026#34;: 17}}, \u0026#34;mqtt_topic\u0026#34;: \u0026#34;ag-mqtt-topic\u0026#34;} | | 17 | {\u0026#34;device\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mobile\u0026#34;, \u0026#34;device_id\u0026#34;: 17}} | +-----------+-----------------------------------------------------------------------------------------------+ SELECT 2 Time: 0.080s Output explanation:\nIn both cases, we’ve used the device_id key in the MQTT JSON as the value in the column of the same name. The first row is from our No Transformation connector. The record data appears unchanged from what we saw in the topic.\nResulting record\n{ \u0026#34;payload\u0026#34;: { \u0026#34;device\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;device_id\u0026#34;: 17 } }, \u0026#34;mqtt_topic\u0026#34;: \u0026#34;ag-mqtt-topic\u0026#34; } The second row is from our JSON to JSON transformation connector We’ve shifted the topic JSON data, so it more closely resembles the original JSON.\nThen we enrich the payload by adding the .device.type key with the value mobile before inserting into the DB\n{ \u0026#34;device\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mobile\u0026#34;, \u0026#34;device_id\u0026#34;: 17 } } Move transformation to MQTT Connector Transformations in the transforms section of SQL Connector config are deliberately decoupled from connectors. We can move a SmartModule from an Inbound to an Outbound connector and accomplish the same result. The decision depends on the shape of the data you want to store in a topic. For Inbound connectors, the data is transformed before sending to Fluvio topic, while for Outbound, it happens after the data is sent to Fluvio topic but before it is sent to the connector. Let\u0026rsquo;s try it.\nModify our mqtt.yml config with one transformation that we are moving from the SQL Connector:\n%copy%\n# mqtt.yml apiVersion: 0.1.0 meta: version: 0.2.5 name: fluvio-mqtt-connector type: mqtt-source topic: mqtt-topic direction: source create-topic: true mqtt: url: \u0026#34;mqtt://test.mosquitto.org/\u0026#34; topic: \u0026#34;ag-mqtt-topic\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: payload: device: \u0026#34;device\u0026#34; - operation: default spec: device: type: \u0026#34;mobile\u0026#34; We don’t need this transformation on SQL Connector anymore, remove it from sql-chain.yml file:\n%copy%\n# sql-chain.yml apiVersion: 0.1.0 meta: name: fluvio-sql-connector-chain type: sql-sink version: 0.2.5 topic: mqtt-topic create-topic: true sql: url: \u0026#34;postgres://user:password@db.postgreshost.example/dbname\u0026#34; transforms: - uses: infinyon/json-sql@0.2.1 with: invoke: insert mapping: table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true We need to re-create connectors:\n%copy%\n$ fluvio cloud connector delete fluvio-mqtt-connector $ fluvio cloud connector create --config mqtt.yml also, we delete one now obsolete SQL connector and re-create another without the transformation that we moved to MQTT:\n%copy%\n$ fluvio cloud connector delete fluvio-sql-connector-chain $ fluvio cloud connector delete fluvio-sql-connector $ fluvio cloud connector create --config sql-chain.yml And now, if we execute command:\n%copy first-line%\n$ mosquitto_pub -h test.mosquitto.org -t ag-mqtt-topic -m \u0026#39;{\u0026#34;device\u0026#34;: {\u0026#34;device_id\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;device17\u0026#34;}}\u0026#39; The new record differs from what we saw previously:\n%copy first-line%\n$ fluvio consume mqtt-topic -B Consuming records from the beginning of topic \u0026#39;mqtt-topic\u0026#39; {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;ag-mqtt-topic\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;device\u0026#34;:{\u0026#34;device_id\u0026#34;:17,\u0026#34;name\u0026#34;:\u0026#34;device17\u0026#34;}}} {\u0026#34;device\u0026#34;:{\u0026#34;device_id\u0026#34;:17,\u0026#34;name\u0026#34;:\u0026#34;device17\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;mobile\u0026#34;}} We can see that the record was transformed before producing to Fluvio cluster.\nHowever, in the database table, the new record equals to the previous one.\n+-----------+-----------------------------------------------------------------------------------------------+ | device_id | record | |-----------+-----------------------------------------------------------------------------------------------| | 17 | {\u0026#34;payload\u0026#34;: {\u0026#34;device\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;device_id\u0026#34;: 17}}, \u0026#34;mqtt_topic\u0026#34;: \u0026#34;ag-mqtt-topic\u0026#34;} | | 17 | {\u0026#34;device\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mobile\u0026#34;, \u0026#34;device_id\u0026#34;: 17}} | | 17 | {\u0026#34;device\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;device17\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mobile\u0026#34;, \u0026#34;device_id\u0026#34;: 17}} | +-----------+-----------------------------------------------------------------------------------------------+ SELECT 3 Time: 0.080s Although the final result is the same (the same records will end up in SQL database with the same content), choosing the proper side of a pipeline where transformations should reside may significantly affect performance on high volumes of data.\nConclusion After setting up our end-to-end MQTT to SQL scenario, we were able to send JSON data to the MQTT broker and track the data to the PostgreSQL table.\nWe saw the results for the JSON just being inserted into the table with the json-sql SmartModule.\nUsing SmartModule chaining with the jolt and json-sql SmartModules, we observed that the resulting JSON was successfully transformed.\nWe can choose on which side of a pipeline we wanted to transform our data without material impact to the result.\n","description":"Data pipeline that listens for mqtt events, converts them to json, and sends them to a SQL database.","keywords":null,"summary":"At the end of this tutorial, we will see data starting from an MQTT broker and ending in a PostgreSQL table.\nWe\u0026rsquo;ll use 2 connectors:\nInbound MQTT connector Outbound SQL connector There will be an example of combining multiple SmartModules, known as SmartModule chaining The Outbound connector will be using a PostgreSQL database. It will listen to the topic for new records and insert them into a table.\nYou can use your own PostgreSQL instance, if it can be reached over the internet.","title":"MQTT to SQL","url":"http://localhost:1315/docs/guides/mqtt-to-sql/"},{"body":" This tutorial assumes that fluvio is installed, and logged-in to InfinyOn Cloud. Follow the Quick Start to get set up.\nSmartModules are the basic building blocks for transformations in Fluvio, allowing users to define custom functions for processing or transforming streaming data. They provide a flexible way to tailor data handling to meet particular needs, enhancing Fluvio\u0026rsquo;s capabilities.\nSee list of available SmartModules %copy first-line%\n$ fluvio hub sm list SMARTMODULE Visibility infinyon-labs/array-map-json@0.1.0 public infinyon-labs/dedup-filter@0.0.2 public infinyon-labs/json-formatter@0.1.0 public infinyon-labs/key-gen-json@0.1.0 public infinyon-labs/regex-map-json@0.1.1 public infinyon-labs/regex-map@0.1.0 public infinyon-labs/stars-forks-changes@0.1.2 public infinyon/jolt@0.3.0 public infinyon/json-sql@0.2.1 public infinyon/regex-filter@0.1.0 public Download SmartModules SmartModules must be downloaded before they can be used. Afterwards, downloaded SmartModules are available for your Producers and Consumers.\n%copy first-line%\n$ fluvio hub sm download infinyon/regex-filter@0.1.0 downloading infinyon/regex-filter@0.1.0 to infinyon-regex-filter-0.1.0.ipkg ... downloading complete ... checking package trying connection to fluvio router.infinyon.cloud:9003 ... cluster smartmodule install complete Use SmartModule with Producer and Consumer You can specify a SmartModule to use with fluvio produce and fluvio consume\nThis consumer is using the SmartModule we just downloaded with --smartmodule, and is also configuring it with --params/-e.\n%copy first-line%\n$ fluvio consume --smartmodule infinyon/regex-filter@0.1.0 --params regex=\u0026#39;[Cc]at\u0026#39; cat-facts You can configure SmartModules with multiple parameters by passing multiple --params/-e.\ne.g.\n%copy first-line%\n$ fluvio produce --smartmodule example/my-smartmodule@0.1.0 -e name=example -e point_made=true The --transforms-file flag are for more complex transformations defined in a YAML file. See the Transformation Chaining page for more detail.\nExample transforms.yaml with multiple transformations. Order matters here, so infinyon/jolt@0.3.0 is first and infinyon/regex-filter@0.1.0 is second.\n%copy%\ntransforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: fact: \u0026#34;animal.fact\u0026#34; length: \u0026#34;length\u0026#34; - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; %copy first-line%\n$ fluvio consume --transforms-file ./my-transforms.yaml my-topic See list of Downloaded SmartModules %copy first-line%\n$ fluvio sm list SMARTMODULE SIZE infinyon/jolt@0.3.0 611.5 KB infinyon/json-sql@0.1.0 558.4 KB infinyon/regex-filter@0.1.0 312.7 KB infinyon/jolt@0.1.0 564.0 KB infinyon/json-sql@0.2.1 559.6 KB Delete SmartModules %copy first-line%\n$ fluvio sm delete infinyon/json-sql@0.2.1 smartmodule \u0026#34;infinyon/json-sql@0.2.1\u0026#34; deleted Use SmartModule with Connector You can define transforms when you create connectors with transforms\n%copy%\n# connector-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts create-topic: true secrets: - name: AUTHORIZATION_TOKEN http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s headers: - \u0026#34;Authorization: token ${{ secrets.AUTHORIZATION_TOKEN }}\u0026#34; - \u0026#34;Cache-Control: no-cache\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: fact: \u0026#34;animal.fact\u0026#34; length: \u0026#34;length\u0026#34; - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; Everything is in the config file. You create a connector as usual.\n%copy first-line%\n$ fluvio cloud connector create --config connector-example.yaml Use SmartModule with Webhook %copy%\n# example-webhook.yaml meta: name: my-webhook topic: my-topic transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: fact: \u0026#34;animal.fact\u0026#34; length: \u0026#34;length\u0026#34; - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; webhook: outputParts: body Just like Connectors, everything is in the config file. You create a webhook as usual.\n%copy first-line%\nfluvio cloud webhook create --config example-webhook.yaml ","description":"A short tutorial on using SmartModules","keywords":null,"summary":"This tutorial assumes that fluvio is installed, and logged-in to InfinyOn Cloud. Follow the Quick Start to get set up.\nSmartModules are the basic building blocks for transformations in Fluvio, allowing users to define custom functions for processing or transforming streaming data. They provide a flexible way to tailor data handling to meet particular needs, enhancing Fluvio\u0026rsquo;s capabilities.\nSee list of available SmartModules %copy first-line%\n$ fluvio hub sm list SMARTMODULE Visibility infinyon-labs/array-map-json@0.","title":"SmartModule Basics","url":"http://localhost:1315/docs/tutorials/smartmodule-basics/"},{"body":"This tutorial expects you to already have the Fluvio CLI installed, and InfinyOn Cloud set up. If neither of these is the case, please follow the instructions here!\nThere are four main steps for this tutorial:\nInstalling the SmartModule Development Kit CLI, smdk Generating a SmartModule project Testing the SmartModule behavior with inputs Loading the SmartModule into a cluster SmartModules SmartModules are user defined functions set to run on and modify the inputs/outputs to a Fluvio database.\nIn this tutorial, we will create a configurable SmartModule that takes in a regular expression rule, and filters input based on whether it matches the rule.\nInstalling SmartModule Development Kit CLI This first example will demonstrate creating, loading, and testing a SmartModule that filters based on whether the content matches a regular expression rule.\nWe will need a custom map SmartModule project, which we can generate with the SmartModule Development Kit CLI, smdk.\nYou can install smdk with the Fluvio CLI\n$ fluvio install smdk Generate a new SmartModule project With smdk, you can generate a new SmartModule project with the guided wizard. We\u0026rsquo;re going to create a project named regex_filter in our current directory by running smdk generate regex_filter\n$ smdk generate regex_filter Generating new SmartModule project: regex_filter fluvio-smartmodule-cargo-dependency =\u0026gt; \u0026#39;0.3.0\u0026#39; ⚠️ Renaming project called `regex_filter` to `regex-filter`... ✔ 🤷 Which type of SmartModule would you like? · filter 🤷 Please set a group name : my-group ✔ 🤷 Will your SmartModule use init parameters? · true ✔ 🤷 Will your SmartModule be public? · false Ignoring: /var/folders/r8/4x6_d2rn283946frzd1gc1pr0000gn/T/.tmpQXKwnh/smartmodule/cargo_template/cargo-generate.toml [1/5] Done: Cargo.toml [2/5] Done: README.md [3/5] Done: SmartModule.toml [4/5] Done: src/lib.rs [5/5] Done: src 🔧 Moving generated files into: `/home/user/project/regex_filter`... 💡 Initializing a fresh Git repository ✨ Done! New project created /home/user/project/regex_filter hub: hubid my-group is set hubid set to my-group Navigate to the project directory and take a look at the Cargo.toml file:\n$ cd regex-filter \u0026amp;\u0026amp; cat Cargo.toml You will need to add a couple of dependencies:\n[package] name = \u0026#34;regex-filter\u0026#34; version = \u0026#34;0.1.0\u0026#34; authors = [\u0026#34;Fluvio Contributors \u0026lt;team@fluvio.io\u0026gt;\u0026#34;] edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#39;cdylib\u0026#39;] [dependencies] fluvio-smartmodule = \u0026#34;0.3.0\u0026#34; serde = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1\u0026#34; once_cell = \u0026#34;1.13.0\u0026#34; regex = \u0026#34;1.6.0\u0026#34; [profile.release-lto] inherits = \u0026#34;release\u0026#34; lto = true And copy/paste this code in place of the auto-generated code in lib.rs :\n// lib.rs use once_cell::sync::OnceCell; use regex::Regex; use fluvio_smartmodule::{ dataplane::smartmodule::{SmartModuleExtraParams, SmartModuleInitError}, eyre, smartmodule, Record, Result, }; static REGEX: OnceCell\u0026lt;Regex\u0026gt; = OnceCell::new(); #[smartmodule(init)] fn init(params: SmartModuleExtraParams) -\u0026gt; Result\u0026lt;()\u0026gt; { if let Some(regex) = params.get(\u0026#34;regex\u0026#34;) { REGEX .set(Regex::new(regex)?) .map_err(|err| eyre!(\u0026#34;regex init: {:#?}\u0026#34;, err)) } else { Err(SmartModuleInitError::MissingParam(\u0026#34;regex\u0026#34;.to_string()).into()) } } #[smartmodule(filter)] pub fn filter(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;bool\u0026gt; { let string = std::str::from_utf8(record.value.as_ref())?; Ok(REGEX.get().unwrap().is_match(string)) } Now that we have the SmartModule project created and code written, we need to build it.\nBuild SmartModule Building a SamartModule is trivial:\n$ smdk build Test SmartModule Using smdk test, we’ll use a simple regular expression that will return when the record contains only numbers.\nFilter pass example We expect the input 42 will return because there are only all numbers\n$ smdk test -e regex=\u0026#34;^[0-9]*$\u0026#34; --text 42 project name: \u0026#34;regex-filter\u0026#34; loading module at: target/wasm32-unknown-unknown/release-lto/regex_filter.wasm 1 records outputed 42 Filter drop example 1 We expect the input abc will drop because there are no numbers\n$ smdk test -e regex=\u0026#34;^[0-9]*$\u0026#34; --text abc project name: \u0026#34;regex-filter\u0026#34; loading module at: target/wasm32-unknown-unknown/release-lto/regex_filter.wasm 0 records outputed Filter drop example 2 We expect the input abc123 will also drop because there are letters mixed with numbers\n$ smdk test -e regex=\u0026#34;^[0-9]*$\u0026#34; --text abc123 project name: \u0026#34;regex-filter\u0026#34; loading module at: target/wasm32-unknown-unknown/release-lto/regex_filter.wasm 0 records outputed Load package into cluster In the previous steps, we used smdk generate to a SmartModule package. This is what the SmartModule.toml package metadata looks like.\n[package] name = \u0026#34;regex-filter\u0026#34; group = \u0026#34;my-group\u0026#34; version = \u0026#34;0.1.0\u0026#34; apiVersion = \u0026#34;0.1.0\u0026#34; description = \u0026#34;\u0026#34; license = \u0026#34;Apache-2.0\u0026#34; [[params]] name = \u0026#34;input\u0026#34; description = \u0026#34;input description\u0026#34; $ smdk load Loading package at: /home/user/project/regex_filter Found SmartModule package: regex-filter loading module at: /home/user/project/regex_filter/target/wasm32-unknown-unknown/release-lto/regex_filter.wasm Trying connection to fluvio router.infinyon.cloud:9003 Creating SmartModule: regex-filter When you list the SmartModules in your cluster, you\u0026rsquo;ll see that the regex-filter results match the name, version and group from the SmartModule.toml file.\n$ fluvio sm list SMARTMODULE SIZE my-group/regex-filter@0.1.0 316.4 KB With the SmartModule loaded in the cluster, we will test that our filter works with data from a topic.\nThis basic example we\u0026rsquo;ll create a new topic, and load it with values.\nCreate a file values.txt with the following contents:\n$ cat values.txt 42 abc abc123 Create a topic filter-test:\n$ fluvio topic create filter-test Load values.txt file to filter-test topic:\n$ fluvio produce filter-test -f values.txt $ fluvio consume filter-test -dB --smartmodule my-group/regex-filter@0.1.0 -e regex=\u0026#34;^[0-9]*$\u0026#34; Consuming records from the beginning of topic \u0026#39;filter-test\u0026#39; 42 Or you can just use the SmartModule by name, if it is unique\n$ fluvio consume filter-test -dB --smartmodule regex-filter -e regex=\u0026#34;^[0-9]*$\u0026#34; Consuming records from the beginning of topic \u0026#39;filter-test\u0026#39; 42 You now know the development workflow for SmartModules with smdk. You can now generate your own project that processes data. For additional information on how to publish and share SmartModules checkout SmartModule Hub.\nCheck out these Other Tutorials SmartModules with smdk Creating a Data Pipeline References Fluvio CLI Produce Fluvio CLI Consume Fluvio CLI topic Fluvio CLI profile SmartModule ","description":"Tutorial for Rust developers on how to use Smart Module Development Kit to build your custom transformations.","keywords":null,"summary":"This tutorial expects you to already have the Fluvio CLI installed, and InfinyOn Cloud set up. If neither of these is the case, please follow the instructions here!\nThere are four main steps for this tutorial:\nInstalling the SmartModule Development Kit CLI, smdk Generating a SmartModule project Testing the SmartModule behavior with inputs Loading the SmartModule into a cluster SmartModules SmartModules are user defined functions set to run on and modify the inputs/outputs to a Fluvio database.","title":"Build your own SmartModules","url":"http://localhost:1315/docs/tutorials/smartmodule-development/"},{"body":"This tutorial assumes that fluvio is installed, and logged-in to InfinyOn Cloud. Follow the Quick Start to get set up.\nWebhooks are special connectors with an associated external url. Users can send data to their topics via a HTTP POST request.\nCreate a Webhook via CLI Copy this example Webhook config file and save it as example-webhook.yaml\n%copy%\n# example-webhook.yaml meta: name: my-webhook topic: my-webhook-topic # optional webhook: outputParts: full outputType: json Create the webhook using the example config file\n%copy first-line%\n$ fluvio cloud webhook create --config example-webhook.yaml Your output should look similar to this. We\u0026rsquo;ll cover sending data to this url.\nWebhook \u0026#34;my-webhook\u0026#34; created with url: https://infinyon.cloud/webhooks/v1/[random string] If you need this url again, you can run this command to list your webhooks, and their urls.\n%copy first-line%\n$ fluvio cloud webhook list Example output\nNAME TOPIC URL my-webhook my-webhook-topic https://infinyon.cloud/webhooks/v1/[random string] Send data to webhook For this section, it is recommended to open second terminal in view watch data arrive.\nRun this command to create a consumer to our topic in the second terminal.\n%copy first-line%\n$ fluvio consume my-webhook-topic We\u0026rsquo;ll be sending json data {\u0026quot;key\u0026quot;: \u0026quot;value\u0026quot;} to our webhook using curl. Replace the url so [random string] matches your unique url. Keep this command close because we\u0026rsquo;ll refer to this example curl command later.\n%copy first-line%\n$ curl -v -X POST https://infinyon.cloud/webhooks/v1/[random string] -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}\u0026#39; Expected output is a long json request with the HTTP headers and body\n{\u0026#34;headers\u0026#34;:{\u0026#34;accept\u0026#34;:\u0026#34;*/*\u0026#34;,\u0026#34;accept-encoding\u0026#34;:\u0026#34;gzip\u0026#34;,\u0026#34;content-length\u0026#34;:\u0026#34;16\u0026#34;,\u0026#34;content-type\u0026#34;:\u0026#34;application/json\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;infinyon.cloud\u0026#34;,\u0026#34;user-agent\u0026#34;:\u0026#34;curl/7.88.1\u0026#34;,\u0026#34;x-forwarded-for\u0026#34;:\u0026#34;...\u0026#34;},\u0026#34;body\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}} Remove HTTP request headers from output We don\u0026rsquo;t need the headers. Just the data we sent. We can update the output so it only includes body data.\nModify the outputParts value from our webhook config example-webhook.yaml to the value body\n%copy%\n# example-webhook.yaml meta: name: my-webhook topic: my-webhook-topic # optional webhook: outputParts: body outputType: json Run this command to update your webhook.\n%copy first-line%\n$ fluvio cloud webhook update -c example-webhook.yaml Webhook \u0026#34;my-webhook\u0026#34; updated Running the example curl command output only the body of the request. Our request data is included, but it was automatically modified so it can be a valid json value.\n{\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;} Conclusion You now know how to create and configure the output of Webhooks. Check out the Webhook Config reference to see how to configure other transformations.\n","description":"A short tutorial for using Webhooks","keywords":null,"summary":"This tutorial assumes that fluvio is installed, and logged-in to InfinyOn Cloud. Follow the Quick Start to get set up.\nWebhooks are special connectors with an associated external url. Users can send data to their topics via a HTTP POST request.\nCreate a Webhook via CLI Copy this example Webhook config file and save it as example-webhook.yaml\n%copy%\n# example-webhook.yaml meta: name: my-webhook topic: my-webhook-topic # optional webhook: outputParts: full outputType: json Create the webhook using the example config file","title":"Webhook Basics","url":"http://localhost:1315/docs/tutorials/webhook-basics/"},{"body":"The InfinyOn Hub serves as a centralized repository for InfinyOn SmartConnectors, SmartModules, and other extensions. It facilitates the discovery and deployment of extensions to enhance the data streaming capabilities of Fluvio. The hub provides a platform for developers and data engineers to easily access pre-built solutions or share their own extensions.\nList Listing SmartModules will display the SmartModules available to your cluster.\n%copy first-line%\n$ fluvio hub sm list SMARTMODULE Visibility infinyon-labs/array-map-json@0.1.0 public infinyon-labs/dedup-filter@0.0.2 public infinyon-labs/json-formatter@0.1.0 public infinyon-labs/key-gen-json@0.1.0 public infinyon-labs/regex-map-json@0.1.1 public infinyon-labs/regex-map@0.1.0 public infinyon-labs/stars-forks-changes@0.1.2 public infinyon/jolt@0.3.0 public infinyon/json-sql@0.2.1 public infinyon/regex-filter@0.1.0 public Listing Connectors will display the connector packages you can download locally. %copy first-line%\n$ fluvio hub conn list CONNECTOR Visibility infinyon-labs/graphite-sink@0.1.2 public infinyon/duckdb-sink@0.1.0 public infinyon/http-sink@0.2.5 public infinyon/http-source@0.2.5 public infinyon/ic-webhook-source@0.1.2 public infinyon/kafka-sink@0.2.7 public infinyon/kafka-source@0.2.5 public infinyon/mqtt-source@0.2.5 public infinyon/sql-sink@0.3.3 public Download Downloading a SmartModule is done with respect to your cluster. SmartModules need to be downloaded before they are used in transformations.\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 downloading infinyon/jolt@0.3.0 to infinyon-jolt-0.3.0.ipkg ... downloading complete ... checking package trying connection to fluvio router.infinyon.cloud:9003 ... cluster smartmodule install complete Running fluvio hub conn download will download the package containing source code of the Connector.\n%copy first-line%\n$ fluvio hub conn download infinyon/http-source@0.2.5 downloading infinyon/http-source@0.2.5 to infinyon-http-source-0.2.5.ipkg ... downloading complete ","description":"A short tutorial on InfinyOn Hub","keywords":null,"summary":"The InfinyOn Hub serves as a centralized repository for InfinyOn SmartConnectors, SmartModules, and other extensions. It facilitates the discovery and deployment of extensions to enhance the data streaming capabilities of Fluvio. The hub provides a platform for developers and data engineers to easily access pre-built solutions or share their own extensions.\nList Listing SmartModules will display the SmartModules available to your cluster.\n%copy first-line%\n$ fluvio hub sm list SMARTMODULE Visibility infinyon-labs/array-map-json@0.","title":"Hub Basics","url":"http://localhost:1315/docs/tutorials/hub-basics/"},{"body":"Context This purpose of this document is to compare and contrast Apache Kafka with Fluvio Open Source.\nFluvio is a cloud native data streaming runtime that was architected from the ground up in Rust to solve the challenges with Kafka and alternatives to Kafka.\nFluvio is built to deliver a lean and mean data streaming platform with all the core functionality with unmatched performance and efficiency without the complexities and overhead of operating and managing Kafka.\nWhat is Apache Kafka? Apache Kafka is a ubiquitous open-source streaming platform used for real-time data processing and event-driven architectures. It allows data to be published to topics and consumed by various applications, making it popular for building scalable and fault-tolerant data pipelines.\nAccording to the official website, Apache Kafka is widely used by 80% of Fortune 100 companies as a robust distributed log solution.\nAt the same time if we look at the Confluent or the RedPanda website it is clear that there are several challenges with Kafka in terms of performance, efficiency, complexity, maintenance toil, ops overhead, and costs.\nThere are 2 types of alternatives available to solve the Kafka challenges.\nManaged cloud native solutions based on Kafka Alternative streaming platforms compatible with Kafka What is Fluvio? InfinyOn Fluvio emerges as a cutting-edge streaming platform, designed to seamlessly integrate with cloud-native architectures. It distinguishes itself through a user-centric interface and the incorporation of SmartModules for in-stream data transformations, achieving unparalleled resource efficiency via Rust and WebAssembly.\nUnlike alternative streaming platforms, Fluvio is a single small binary that consumes minimal resources to run and operates using a simple and powerful client and CLI to build streaming flows without the limitations of the JVM.\nFluvio streaming runtime is complemented with SDKs to develop protocol level connectors and web assembly based on stream transformations that offers fine grained composability and control, and is also immune to memory issues.\nIndicators of performance To run kafka cluster locally, need to download images amounting to 6.97 GB:\nenterprise-control-center - 1.31 GB enterprise-kafka - 1.01 GB kafka-rest - 1.85 GB schema-registry - 1.94 GB zookeeper - 863 MB. Fluvio is a single binary less than 150 Mb that currently has a dependency on Kubernetes that we are decoupling to get a single binary.\nWhile we are working on our implementation of the open messaging benchmark, one of our customers compared their kafka clusters with fluvio and shared their results with us.\nAs per our customer data, Fluvio turned out to be 7 times leaner in CPU utilization, consumed 50 times less memory, while delivering 5 times more throughput.\nGeneral Parameter Infinyon Fluvio Apache Kafka License Apache 2.0 Apache 2.0 Components Fluvio + Kubernetes (K8 decoupling ready by Q4 2023 to support single binary deployment) Kafka + Zookeeper/KRaft (KIP-833) Message Consumption Model Pull Pull Storage Architecture Log Log License Both Fluvio and Kafka use Apache 2.0, which is a fully open-source license.\nComponents Kafka uses Apache Zookeeper in production for its distributed coordination and configuration management, until KRaft mode is standard.\nFluvio uses Kubernetes for its cluster and configuration management on InfinyOn Cloud. For Fluvio open source we are enabling deploying local clusters with a single independent binary.\nMessage Consumption Model Both Kafka and Fluvio use pull-based message consumption models. Suitable for event streaming use-cases.\nStorage Architecture Both Kafka and Fluvio use a storage layer built on a distributed log. New entries are added to the end of the log, and data is read in a sequential fashion starting from an offset. Data is transferred using zero-copy techniques to move from the disk buffer to the network buffer.\nUsing Fluvio SmartModules, data can be transformed in-stream. For example, before it is written to the log, or before it is sent over the network. This can favorably reduce bandwidth requirements and eliminate time spent cleaning data afterwards.\nEcosystem and User Experience Parameter Infinyon Fluvio Apache Kafka Deployment Run 1 CLI command Requires experience for tuning performance Enterprise Support InfinyOn Cloud 3rd parties like Confluent, AWS MSK Managed Cloud Offerings InfinyOn Cloud 3rd parties like Confluent, AWS MSK Self-Healing Yes No Deployment Kafka is highly configurable, and may require time investment for performance. Production installations have zookeeper to maintain until KIP-833.\nFluvio requires a binary installation and a Kubernetes cluster. A cluster can be deployed with a single CLI command\nEnterprise Support and managed cloud Kafka is offered by many vendors for enterprise support, and managed cloud offerings.\nInfinyOn provides enterprise support, as well as a managed cloud offering InfinyOn Cloud\nSelf-Healing Kafka clusters can require an operator to invest in the skills or services for performance tuning, management, and monitoring.\nFluvio clusters come out of the box with internal cluster management with Kubernetes operators.\nAvailability and Messaging Parameter InfinyOn Fluvio Apache Kafka Replication Yes Yes Multi-tenancy Yes via k8 namespaces No Ordering guarantees Partition Level Partition level Permanent storage Yes Yes Delivery guarantees At least once, Exactly once At least once, Exactly once Idempotency Yes Yes Geo-Replication (Multi-region) Yes - Mirror topic Yes Replication Kafka and Fluvio replication work by having multiple copies spread over multiple brokers/SPUs for High Availability storage.\nMulti-tenancy Kafka doesn’t natively support multi-tenancy, but it can be achieved.\nFluvio supports multi-tenancy through Kubernetes namespaces.\nOrdering guarantees Both Kafka and Fluvio have partition level ordering guarantees.\nPermanent storage Both Kafka and Fluvio have durable, reliable storage. Data retention is configured at the topic level.\nDelivery guarantees Both Kafka and Fluvio support At least once, and Exactly Once.\nIdempotency Kafka supports idempotent producers, but clients are responsible for retrying transmission.\nFluvio SmartModules can implement idempotent producers and consumers with its deduplication filter.\nGeo-Replication (Multi-region) Kafka MirrorMaker supports data replication between different Kafka environments\nFluvio topics can be configured to mirror different Fluvio environments\nFeatures Parameter Infinyon Fluvio Apache Kafka GUI Yes w/ InfinyOn Cloud 3rd party Schema Management No (In development) No Message Routing Partial Yes. Using Kafka connect and KStreams Log Compaction No Yes Message Replay, time travel Yes Yes Stream enrichment SmartModules SQL-based using KStreams Pull retry mechanism Client responsibility Client responsibility GUI Kafka GUIs exist and are supported by 3rd-parties\nFluvio does not have a native GUI. InfinyOn Cloud serves as a GUI for Fluvio\nSchema Management Kafka has Schema Registry which is offered by Confluent.\nFluvio does not have schema management yet, but is planned.\nMessage routing Kafka Connect and KStreams can dynamically route events other topics based on data and also branch a single event stream into multiple new topics\nFluvio Connectors with SmartModules can filter, transform or enrich messages to downstream services. Fluvio does not support dynamic routing of messages to other topics.\nLog compaction Kafka supports log compaction to retain the last known value for each message key for a single topic partition.\nFluvio does not support log compaction\nMessage replay Kafka and Fluvio both support the ability to replay messages by consumers seeking specific offsets\nStream Enrichment Kafka Streams and Connect can provide stream enrichment\nFluvio SmartModules provide stream enrichment\nPull retry mechanism In both Kafka, and Fluvio, the client is responsible for implementing a retry mechanism based on their needs.\n","description":"","keywords":null,"summary":"Context This purpose of this document is to compare and contrast Apache Kafka with Fluvio Open Source.\nFluvio is a cloud native data streaming runtime that was architected from the ground up in Rust to solve the challenges with Kafka and alternatives to Kafka.\nFluvio is built to deliver a lean and mean data streaming platform with all the core functionality with unmatched performance and efficiency without the complexities and overhead of operating and managing Kafka.","title":"Comparison: Fluvio vs Kafka","url":"http://localhost:1315/docs/resources/compare-fluvio-vs-kafka/"},{"body":"Ensure fluvio is installed and logged in to InfinyOn Cloud before proceeding. If not set up, follow the Quick Start.\nConnectors serve to transmit data into or out of your cluster, configurable via a YAML file.\nUtilizing InfinyOn Cloud for managing connectors centralizes your data pipelines, both locally and on the cloud.\nManage your connectors efficiently with the fluvio cloud connector CLI.\nCreate Your First Connector on InfinyOn Cloud This guide demonstrates creating an Inbound HTTP connector to ingest JSON data from an HTTP endpoint.\nSign up for InfinyOn Cloud and log into the CLI to begin:\n%copy first-line%\n$ fluvio cloud login Example HTTP connector Below is the configuration for the Inbound HTTP connector:\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.1 name: cat-facts type: http-source topic: cat-facts create-topic: true http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s In this configuration, a connector named cat-facts is created. It requests data from a cat fact API every 30 seconds and receives JSON data, which is stored in a topic named cat-facts-data.\nStart a connector Create a connector using the following command:\n%copy first-line%\n$ fluvio cloud connector create --config catfacts-basic-connector.yml connector \u0026#34;cat-facts\u0026#34; (http-source) created List all connectors Once created, list the connectors and view their current status:\n%copy first-line%\n$ fluvio cloud connector list NAME TYPE VERSION CDK STATUS cat-facts http-source 0.1.0 V3 Running View connector logs Debug connector behavior by accessing the logs:\n%copy first-line%\n$ fluvio cloud connector logs cat-facts connector-startup infinyon/http-source@0.1.0 2023-03-25T03:41:29.570294Z INFO surf::middleware::logger::native: sending request 2023-03-25T03:41:29.702213Z INFO surf::middleware::logger::native: request completed 2023-03-25T03:41:29.702258Z INFO connector_startup::startup: downloading package url=\u0026#34;https://hub.infinyon.cloud/hub/v0/connector/pkg/infinyon/http-source/0.1.0\u0026#34; 2023-03-25T03:41:29.702290Z INFO surf::middleware::logger::native: sending request 2023-03-25T03:41:29.993001Z INFO surf::middleware::logger::native: request completed 2023-03-25T03:41:30.108220Z INFO connector_startup::startup: writing file file=\u0026#34;connector.ipkg\u0026#34; ... checking package 2023-03-25T03:41:30.301199Z INFO connector_startup::startup: connector binary from package path=\u0026#34;./http-source\u0026#34; 2023-03-25T03:41:30.301224Z INFO connector_startup::startup: Starting deployment Connector runs with process id: 15 2023-03-25T03:41:30.303333Z INFO http_source: Reading config file from: /home/fluvio/config.yaml 2023-03-25T03:41:30.303526Z INFO http_source: starting processing 2023-03-25T03:41:30.304337Z INFO fluvio::config::tls: Using verified TLS with certificates from paths domain=\u0026#34;odd-butterfly-0dea7a035980a4679d0704f654e1a14e.c.cloud-dev.fluvio.io\u0026#34; 2023-03-25T03:41:30.308822Z INFO fluvio::fluvio: Connecting to Fluvio cluster fluvio_crate_version=\u0026#34;0.16.0\u0026#34; fluvio_git_hash=\u0026#34;8d4023ee0dc7735aaa0c823dd2b235662112f090\u0026#34; 2023-03-25T03:41:30.369634Z INFO connect: fluvio_socket::versioned: connect to socket add=fluvio-sc-public:9003 2023-03-25T03:41:30.412895Z INFO connect:connect_with_config: fluvio::config::tls: Using verified TLS with certificates from paths domain=\u0026#34;odd-butterfly-0dea7a035980a4679d0704f654e1a14e.c.cloud-dev.fluvio.io\u0026#34; 2023-03-25T03:41:30.473242Z INFO connect:connect_with_config:connect: fluvio_socket::versioned: connect to socket add=fluvio-sc-public:9003 2023-03-25T03:41:30.582726Z INFO dispatcher_loop{self=MultiplexDisp(12)}: fluvio_socket::multiplexing: multiplexer terminated 2023-03-25T03:41:30.632722Z INFO fluvio_connector_common::monitoring: using metric path: /fluvio_metrics/connector.sock 2023-03-25T03:41:30.632795Z INFO fluvio_connector_common::monitoring: monitoring started 2023-03-25T03:41:31.172075Z INFO run:create_serial_socket_from_leader{leader_id=0}:connect_to_leader{leader=0}:connect: fluvio_socket::versioned: connect to socket add=fluvio-spu-main-0.acct-ce0c1782-ca61-4c54-a08c-3ba985524553.svc.cluster.local:9005 View data in topic The HTTP connector should now be storing data in the specified topic:\n%copy first-line%\n$ fluvio topic list NAME TYPE PARTITIONS REPLICAS RETENTION TIME COMPRESSION STATUS REASON cat-facts-data computed 1 1 7days any resolution::provisioned Verify by consuming from the topic:\n%copy first-line%\n$ fluvio consume cat-facts-data -B {\u0026#34;fact\u0026#34;:\u0026#34;Female felines are \\\\superfecund\u0026#34;,\u0026#34;length\u0026#34;:31} {\u0026#34;fact\u0026#34;:\u0026#34;Cats only sweat through their paws and nowhere else on their body\u0026#34;,\u0026#34;length\u0026#34;:65} {\u0026#34;fact\u0026#34;:\u0026#34;While many parts of Europe and North America consider the black cat a sign of bad luck, in Britain and Australia, black cats are considered lucky.\u0026#34;,\u0026#34;length\u0026#34;:146} ^C The --disable-continuous flag exits the stream after displaying the last record. Delete a connector To stop the connector, delete it:\n%copy first-line%\n$ fluvio cloud connector delete cat-facts connector \u0026#34;cat-facts\u0026#34; deleted This action won’t delete the topic. Delete it separately if needed:\n%copy first-line%\n$ fluvio topic delete cat-facts topic \u0026#34;cat-facts\u0026#34; deleted Conclusion You\u0026rsquo;ve created an Inbound HTTP connector, accessed the logs, viewed the HTTP response data in the Fluvio topic, and deleted the connector and topic post-exploration. You are now prepared to create your own connectors. Check the documentation for our supported Inbound and Outbound connectors to get started with your own data sources.\n","description":"A short tutorial for using Connectors","keywords":null,"summary":"Ensure fluvio is installed and logged in to InfinyOn Cloud before proceeding. If not set up, follow the Quick Start.\nConnectors serve to transmit data into or out of your cluster, configurable via a YAML file.\nUtilizing InfinyOn Cloud for managing connectors centralizes your data pipelines, both locally and on the cloud.\nManage your connectors efficiently with the fluvio cloud connector CLI.\nCreate Your First Connector on InfinyOn Cloud This guide demonstrates creating an Inbound HTTP connector to ingest JSON data from an HTTP endpoint.","title":"Connector Basics","url":"http://localhost:1315/docs/tutorials/connector-basics/"},{"body":"Role We are looking for a developer advocate to spread the word about Fluvio, a high-performance data streaming platform written in Rust. As a Developer Advocate, you\u0026rsquo;ll work closely with our third-party developers on developing the Fluvio community. You must thrive on new challenges as you\u0026rsquo;ll seldom face the same problem two days in a row. You are frequent public speaker and active on social media.\nResponsibilities Plan and coordinate webinars and meetups to showcase the Fluvio platform. Spread the word through blogs, presentations, videos, and podcasts. Interact with the developer community to answer questions and collect feedback. Develop use cases and demos that showcase Fluvio platform. Represent the InfinyOn brand externally when interacting with developers. Represent the developer internally when participating in roadmap planning. Requirements Minimum 3 years of experience as a Developer Advocate Software engineering background required Good understanding of Rust required Knowledge of Node, Python, and Java desirable Excellent verbal, written and interpersonal communication skills Comfortable with presenting technical information to a technical audience Self-directed and able to quickly and efficiently switch focus among multiple tasks Strong working understanding of real time data streaming ","description":"The Community Engagement team is looking for a developer advocate to spread the word about Fluvio. If you thrive on new challenges, you are frequent public speaker and active on social media, this job is for you.","keywords":null,"summary":"Role We are looking for a developer advocate to spread the word about Fluvio, a high-performance data streaming platform written in Rust. As a Developer Advocate, you\u0026rsquo;ll work closely with our third-party developers on developing the Fluvio community. You must thrive on new challenges as you\u0026rsquo;ll seldom face the same problem two days in a row. You are frequent public speaker and active on social media.\nResponsibilities Plan and coordinate webinars and meetups to showcase the Fluvio platform.","title":"Developer Advocate","url":"http://localhost:1315/careers/developer-advocate-mid-senior-level/"},{"body":"Create an end-to-end event pipeline that detects changes in github stars \u0026amp; forks and publishes the result to Slack. This guide uses two connectors:\nhttp-source: to read periodically from a github, parse the fields from the json output, and publishes the result to a topic. http-sink: to listen to the same topic, detect changes, and publish the result to Slack. Let\u0026rsquo;s get started.\nPrerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Create http-sink configuration file Download smartmodules Start Connectors Test Data Pipeline Create http-source configuration file Create an HTTP source connector configuration file called github.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: github-stars-in type: http-source topic: stars-forks secrets: - name: GITHUB_TOKEN http: endpoint: \u0026#39;https://api.github.com/repos/infinyon/fluvio\u0026#39; method: GET headers: - \u0026#39;Authorization: token ${{ secrets.GITHUB_TOKEN }}\u0026#39; interval: 30s transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;stargazers_count\u0026#34;: \u0026#34;stars\u0026#34; \u0026#34;forks_count\u0026#34;: \u0026#34;forks\u0026#34; Github rate-limits API requests to 60 per hour, which you an extend to 5000 by creating an application token. Check out github documentation on how to create an Access Tokens.\nAdd the access token secret in InfinyOn Cloud :\n%copy%\n$ fluvio cloud secret set GITHUB_TOKEN \u0026lt;access-token\u0026gt; Create http-sink configuration file Create an HTTP source connector configuration file called slack.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: slack-stars-out type: http-sink topic: stars-forks secrets: - name: SLACK_TOKEN http: endpoint: \u0026#34;https://hooks.slack.com/services/${{ secrets.SLACK_TOKEN }}\u0026#34; headers: - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon-labs/stars-forks-changes@0.1.2 lookback: last: 1 - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;result\u0026#34;: \u0026#34;text\u0026#34; Check out Slack Webhooks on how to create a channel webhook token.\nAdd the access token secret in InfinyOn Cloud :\n%copy%\n$ fluvio cloud secret set SLACK_TOKEN \u0026lt;webhook-token\u0026gt; Download Smartmodules Download the smartmodules used by the connectors to your cluster:\n%copy%\n$ fluvio hub sm download infinyon/jolt@0.3.0 $ fluvio hub sm download infinyon-labs/stars-forks-changes@0.1.2 Start Connectors Start source \u0026amp; sink connectors:\n%copy%\n$ fluvio cloud connector create -c github.yaml $ fluvio cloud connector create -c slack.yaml Check fluvio cloud connector log to ensure they have been successfully provisioned.\nTest Data Pipeline Check the last values generated by the github connector:\n%copy first-line%\n$ fluvio consume -dT 1 stars-forks {\u0026#34;stars\u0026#34;:1770,\u0026#34;forks\u0026#34;:138} Produce a new value\n%copy first-line%\n$ fluvio produce stars-forks \u0026gt; {\u0026#34;stars\u0026#34;:1769,\u0026#34;forks\u0026#34;:138} OK An alert with :star2: 1769 will show-up in your slack channel.\nReferences Create a Github Stars/Forks Event Pipeline (Blog) labs-stars-forks-changes-sm ","description":"Data pipeline that detects changes in github and publishes them as events to Slack.","keywords":null,"summary":"Create an end-to-end event pipeline that detects changes in github stars \u0026amp; forks and publishes the result to Slack. This guide uses two connectors:\nhttp-source: to read periodically from a github, parse the fields from the json output, and publishes the result to a topic. http-sink: to listen to the same topic, detect changes, and publish the result to Slack. Let\u0026rsquo;s get started.\nPrerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Create http-sink configuration file Download smartmodules Start Connectors Test Data Pipeline Create http-source configuration file Create an HTTP source connector configuration file called github.","title":"Github to Slack","url":"http://localhost:1315/docs/guides/github-to-slack/"},{"body":"This is a short introduction to fvm, the Fluvio Version Manager, which allows you to use multiple versions of the Fluvio CLI toolchain.\nThe fvm CLI is the official package manager for Fluvio, managing various fluvio binaries and development tools \u0026ndash; enabling the selection from multiple versions or release channels.\nInstall fvm The following command will install fvm and fluvio and other binaries in the development kit.\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash The installation script prints a command that adds the the fluvio binaries to your shell PATH environment variable. This is required to follow the following steps.\nOn macOS, run this command and then close and reopen your terminal.\n%copy first-line%\n$ echo \u0026#39;export PATH=\u0026#34;${HOME}/.fvm/bin:${HOME}/.fluvio/bin:${PATH}\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc If you\u0026rsquo;ve got existing installation of Fluvio (version 0.10.15 or older), please head on to the next section for additional follow-up steps. (Optional) For existing installation of Fluvio CLI must run this command before continuing further. We will cover the usage later when we talk about release channels.\nIf you have an existing fluvio installation (version 0.10.15 or older), you can choose 1 of the following:\nOption 1: Delete the fluvio directory This will clear out your existing settings.\n%copy first-line%\n$ rm -ri $HOME/.fluvio rm -ri $HOME/.fluvio examine files in directory $HOME/.fluvio? y remove $HOME/.fluvio? y After deleting this directory, you can re-run the installation instructions.\nOption 2: Install the stable channel New installs run this step as part of the installation script, but you can migrate your existing settings\n%copy%\n$ fvm install info: Downloading (1/5): fluvio@0.10.16 info: Downloading (2/5): fluvio-run@0.10.16 info: Downloading (3/5): fluvio-cloud@0.2.15 info: Downloading (4/5): smdk@0.10.16 info: Downloading (5/5): cdk@0.10.16 done: Installed fluvio version stable done: Now using fluvio version 0.10.16 What version are you running? Running fvm current should display the most recent version of fluvio toolchain installed. (The most recent version is 0.10.16 for the purposes of this tutorial)\n%copy first-line%\n$ fvm current 0.10.16 (stable) Release channels Also in the output of fvm current we see stable, which is the name of the default release channel, and the active channel in use. There are 2 channels: stable, latest.\nInstalling channels with fvm install will also make that channel active.\nThe Active channel Only one channel is active at a time. You can use fvm switch to select one of your installed channels.\nFVM updates the fluvio/smdk/cdk etc. binaries used with the active channel\u0026rsquo;s binaries.\nStable release channel The stable channel is installed by default. It is the most recent supported release.\nThe following commands are equivalent for installing or updating the stable release channel.\n%copy first-line%\n$ fvm install $ fvm install stable Latest release channel If you contact us for support in GitHub or Discord, we may request you to validate fixes from the latest channel.\nThis channel consists of most recent updates that have not yet been released, which may include experimental features or unexpected behavior.\nThe latest channel is not intended for typical usage.\n%copy first-line%\n$ fvm install latest info: Downloading (1/5): fluvio@0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 info: Downloading (2/5): fluvio-run@0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 info: Downloading (3/5): fluvio-cloud@0.2.15 info: Downloading (4/5): smdk@0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 info: Downloading (5/5): cdk@0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 done: Installed fluvio version latest done: Now using fluvio version 0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 Install a specific version Specific releases can also be installed when you provide the version.\n$ fvm install 0.10.16 info: Downloading (1/5): fluvio@0.10.16 info: Downloading (2/5): fluvio-run@0.10.16 info: Downloading (3/5): fluvio-cloud@0.2.15 info: Downloading (4/5): smdk@0.10.16 info: Downloading (5/5): cdk@0.10.16 done: Installed fluvio version 0.10.16 done: Now using fluvio version 0.10.16 Listing installed channels The fvm show command will list out the installed channels and their corresponding version. The row with the checkmark (✓) is the current active channel.\n%copy first-line%\n$ fvm show CHANNEL VERSION ✓ 0.10.16 0.10.16 stable 0.10.16 latest 0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 Switching between channels If you are on another channel, you can change between them by running fvm switch with the name of the channel.\nFor typical usage of InfinyOn Cloud, we suggest using stable.\n%copy first-line%\n$ fvm switch stable done: Now using Fluvio version stable And we verify with fvm show that we are now back on the stable release channel.\n%copy first-line%\n$ fvm show CHANNEL VERSION ✓ stable 0.10.16 latest 0.11.0-dev-1+bf4e86674ce546d6b853adbf36a97e8e3344bd17 0.10.16 0.10.16 Conclusion This wraps up the fundamental usage of fvm. While this guide covers the basics, fvm has more under the hood to explore and adapt to your workflows, ensuring a frictionless experience as you delve into Fluvio\u0026rsquo;s ecosystem.\nHowever, our expectation is that a majority of users will find the stable channel adequately meets their needs, making the transition to other channels unnecessary. For developers actively contributing through issues or code, and our design partners, fvm serves as a bridge for closer and smoother collaboration.\nHopefully this is just the beginning of your journey with fvm. We\u0026rsquo;re excited to see how it enhances your interactions with our platform.\n","description":"A short tutorial for using fvm","keywords":null,"summary":"This is a short introduction to fvm, the Fluvio Version Manager, which allows you to use multiple versions of the Fluvio CLI toolchain.\nThe fvm CLI is the official package manager for Fluvio, managing various fluvio binaries and development tools \u0026ndash; enabling the selection from multiple versions or release channels.\nInstall fvm The following command will install fvm and fluvio and other binaries in the development kit.\n%copy first-line%\n$ curl -fsS https://hub.","title":"Install Fluvio CLI toolchain","url":"http://localhost:1315/docs/tutorials/install/"},{"body":"This is a filter-type SmartModule that tests the input record against a provided regular expression. The record is returned if there is a match.\nUsage example \u0026lt;TODO: Make this example use only fluvio\u0026gt;\nFirst, we need to download it to our cluster:\n%copy first-line%\n$ fluvio hub download infinyon/regex-filter@0.1.0 Second, we create a file transform.yaml defining our regular expression:\n%copy%\n# transform.yaml transforms: - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; Let\u0026rsquo;s use smdk test to see it in action:\n%copy first-line%\n$ smdk test --text \u0026#39;{\u0026#34;fact\u0026#34;: \u0026#34;Cats have supersonic hearing\u0026#34;}\u0026#39; --transforms-file ./transform.yaml {\u0026#34;fact\u0026#34;: \u0026#34;Cats have supersonic hearing\u0026#34;} %copy first-line%\n$ smdk test --text \u0026#39;{\u0026#34;fact\u0026#34;: \u0026#34;Dogs have sweat glands at the bottom of their paws\u0026#34;}\u0026#39; --transforms-file ./transform.yaml [No output returned] ","description":"","keywords":null,"summary":"This is a filter-type SmartModule that tests the input record against a provided regular expression. The record is returned if there is a match.\nUsage example \u0026lt;TODO: Make this example use only fluvio\u0026gt;\nFirst, we need to download it to our cluster:\n%copy first-line%\n$ fluvio hub download infinyon/regex-filter@0.1.0 Second, we create a file transform.yaml defining our regular expression:\n%copy%\n# transform.yaml transforms: - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; Let\u0026rsquo;s use smdk test to see it in action:","title":"Regex","url":"http://localhost:1315/docs/smartmodules/regex/"},{"body":"Role We are looking for a solutions architect to partner as a technical advisor to the sales team organiztion. You will work very closely with the product management, marketing and engineering teams and serve as a vital product advocate in front of prospects, customers, and the wider Data Streaming communities.\nYou will be a trusted advisor to InfinyOn prospects and customers as they undertake technical demos, design and development of their end-to-end solution, be conversant with the current state and future roadmap of Fluvio in open source and be able to identify known issues and inform them of upcoming fixes or workarounds. You’ll also be responsible for clear articulation of InfinyOn’s technical and product positioning to a wide variety of business and technical stakeholders, and for forging strong relationships with them throughout the sales cycle.\nRequired Skills and Experience Experience with distributed systems, infrastructure software like databases, message queues, data-focused SaaS, or Big Data products. Confidence presenting to customers - a highly skilled, technical and business audience. Clear, consistent demonstration of self-starter behavior and a desire to learn new things and tackle hard technical problems. Proficiency in Rust or Python. Experience building or running cloud applications. What Gives You An Advantage Previous sales engineering work experience Previous experience building solutions with Apache Kafka or Apache Pulsar Work experience with data integration or application architecture Understanding of systems operations concepts (disk, network, operating systems, etc) Experience building and operating distributed systems Comfortable with presenting technical information to a technical audience Self-directed and able to quickly and efficiently switch focus among multiple tasks Technical certifications - CS degree or cloud developer/architect/administrator ","description":"The InfinyOn Solutions Architect will drive the technical evaluation stage of the overall sales process, making them critical drivers of customer success as real time data streams become more important in the modern data driven enterprise.","keywords":null,"summary":"Role We are looking for a solutions architect to partner as a technical advisor to the sales team organiztion. You will work very closely with the product management, marketing and engineering teams and serve as a vital product advocate in front of prospects, customers, and the wider Data Streaming communities.\nYou will be a trusted advisor to InfinyOn prospects and customers as they undertake technical demos, design and development of their end-to-end solution, be conversant with the current state and future roadmap of Fluvio in open source and be able to identify known issues and inform them of upcoming fixes or workarounds.","title":"Solutions Architect","url":"http://localhost:1315/careers/solutions-architect/"},{"body":"Connectors Describe connectors\nInstalling Connector Development Kit CLI Generate a new SmartModule project Build SmartModule Test SmartModule ","description":"Tutorial for Rust developers on how to use Connector Development Kit to build your custom data connectors.","keywords":null,"summary":"Connectors Describe connectors\nInstalling Connector Development Kit CLI Generate a new SmartModule project Build SmartModule Test SmartModule ","title":"Build your own Connectors","url":"http://localhost:1315/docs/tutorials/connector-development/"},{"body":"Create an end-to-end event pipeline that detects changes in github stars \u0026amp; forks and publishes the result to Discord. This guide uses two connectors:\nhttp-source: to read periodically from a github, parse the fields from the json output, and publish the result to a topic. http-sink: to listen to the same topic, detect changes, and publish the result to Discord. Let\u0026rsquo;s get started.\nPrerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Create http-sink configuration file Download smartmodules Start Connectors Test Data Pipeline Create http-source configuration file Create an HTTP source connector configuration file called github.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: github-stars-in type: http-source topic: stars-forks secrets: - name: GITHUB_TOKEN http: endpoint: \u0026#39;https://api.github.com/repos/infinyon/fluvio\u0026#39; method: GET headers: - \u0026#39;Authorization: token ${{ secrets.GITHUB_TOKEN }}\u0026#39; interval: 30s transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;stargazers_count\u0026#34;: \u0026#34;stars\u0026#34; \u0026#34;forks_count\u0026#34;: \u0026#34;forks\u0026#34; Github rate-limits API requests to 60 per hour, which you an extend to 5000 by creating an application token. Check out github documentation on how to create an Access Tokens.\nAdd the access token secret in InfinyOn Cloud :\n%copy%\n$ fluvio cloud secret set GITHUB_TOKEN \u0026lt;access-token\u0026gt; Create http-sink configuration file Create an HTTP source connector configuration file called discord.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: discord-stars-out type: http-sink topic: stars-forks secrets: - name: DISCORD_TOKEN http: endpoint: \u0026#34;https://discord.com/api/webhooks/${{ secrets.DISCORD_TOKEN }}\u0026#34; headers: - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon-labs/stars-forks-changes@0.1.2 lookback: last: 1 - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;result\u0026#34;: \u0026#34;content\u0026#34; Check out Discord Webhooks on how to create a channel webhook token.\nAdd the access token secret in InfinyOn Cloud :\n%copy%\n$ fluvio cloud secret set DISCORD_TOKEN \u0026lt;webhook-token\u0026gt; Download Smartmodules Download the smartmodules used by the connectors to your cluster:\n%copy%\n$ fluvio hub sm download infinyon/jolt@0.3.0 $ fluvio hub sm download infinyon-labs/stars-forks-changes@0.1.2 Start Connectors Start source \u0026amp; sink connectors:\n%copy%\n$ fluvio cloud connector create -c github.yaml $ fluvio cloud connector create -c discord.yaml Check fluvio cloud connector log to ensure they have been successfully provisioned.\nTest Data Pipeline Check the last values generated by the github connector:\n%copy first-line%\n$ fluvio consume -dT 1 stars-forks {\u0026#34;stars\u0026#34;:1770,\u0026#34;forks\u0026#34;:138} Produce a new value\n%copy first-line%\n$ fluvio produce stars-forks \u0026gt; {\u0026#34;stars\u0026#34;:1769,\u0026#34;forks\u0026#34;:138} OK An alert with :star2: 1769 will show-up in your discord channel. See it live at Fluvio Community - #alerts channel.\nReferences Create a Github Stars/Forks Event Pipeline (Blog) labs-stars-forks-changes-sm ","description":"Data pipeline that detects changes in github and publishes them as events to Discord.","keywords":null,"summary":"Create an end-to-end event pipeline that detects changes in github stars \u0026amp; forks and publishes the result to Discord. This guide uses two connectors:\nhttp-source: to read periodically from a github, parse the fields from the json output, and publish the result to a topic. http-sink: to listen to the same topic, detect changes, and publish the result to Discord. Let\u0026rsquo;s get started.\nPrerequisites Fluvio CLI running locally Account on InfinyOn Cloud Step-by-Step Create http-source configuration file Create http-sink configuration file Download smartmodules Start Connectors Test Data Pipeline Create http-source configuration file Create an HTTP source connector configuration file called github.","title":"Github to Discord","url":"http://localhost:1315/docs/guides/github-to-discord/"},{"body":"The data pipeline file defines the composition between services, data streams, and state objects. It describes the end-to-end application DAG, including the source and sink topics, data types, user-defined smartmodules, stateful windows, and aggregate.\nServices communicate with each other via topics, hence the service composition is defined by the topics they consume and produce.\nData Pipeline Template The data pipeline file is defined in YAML and has the following top level sections:\napiVersion: ... meta: name: ... version: ... namespace: ... config: converter: ... consumer: ... types: \u0026lt;type\u0026gt; : ... ... topics: \u0026lt;topic\u0026gt;: ... ... services: \u0026lt;service\u0026gt;: sources: ... transforms: window: ... states: ... steps: \u0026lt;operator\u0026gt;: ... ... flush: ... sinks: ... ... Where the sections are:\napiVersion - the engine version compatible with this data pipeline file. config - the configuration paramters. meta - the service metadata. types - the schema definition. topics - the data streaming topic names. services - the application composition definitions. operators - the system pre-defined operators. inline functions - the business logic definition. states - the state object interface definitions. meta Meta, short for metadata, holds the stateful service properties, such as name \u0026amp; version.\nmeta: name: string namespace: string version: semver Where:\nname - is the name of the data pipeline. namespace - is the unique namespace where the data pipeline is deployed. The tuple namespace:name becomes the WASM Component Model package name.\nconfig Config, short for configurations, holds the service default settings.\nconfig: converter: raw, json consumer: default_starting_offset: value: u64 position: start, end The convert configuration currently suports only raw and json formats, with additional types to be implemented as required. This is used to set the default serialization/deserialization method. The configuration can be overwritten in each individual topic topic configuration.\nThe consumer default starting offset can begin reading from a specific value from the start or end of the data stream.\nFor example:\nconfig: converter: json consumer: default_starting_offset: value: 0 position: end The consumer starts reading from the end of the data stream and parses the records as JSON.\ntypes The types section defines the schema of the object used in the data pipeline. The primitive types are as follows:\nnull bool u8 | u16 | u32 | u64 i8 | i16 | i32 | i64 f32 | f64 string enum key-value list object These primitives allow you to create custom types. For example, you may define user, job, and roles as follows:\ntypes: user: type: object properties: name: type: string age: type: u8 job: type: object properties: name: type: string role: type: string roles: type: list items: type: key-value properties: key: type: string value: type: u32 Types define the data formats for topics, states, and smartmodules.\ntopics Topics represent the internal and external communication layer for the services. When the Stateful Service is first initialized, the engine provisions all undefined topics before it starts the services.\nFor example a list of topics can we defined as follows:\ntopics: cars-topic: schema: value: type: Car converter: json car-events-topic: schema: value: type: CarEvent coverter: json The definitions is a list of topic names and their schema. The topics also have an optional converter if different from the converter in the configuration section.\nservices Services define the data pipeline composition, operations, states, and topics consumed and produced. Each service has a name and several sub-sections. For example a simple service would be defined as follows:\nmy-service: sources: - type: topic id: my-source-topic transforms: steps: - operator: map run: | fn to_uppercase(input: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { Ok(input.to_uppercase()) } sinks: - type: topic id: my-sink-topic In this example, the service my-service consumes the topic my-source-topic, transforms each record to uppercase, and writes the output to the topic my-sink-topic.\nServices may be chained via topics, for example:\n# fields omitted for simplicity services: service-1: sources: - id: topic-1 sinks: - id: topic-2 service-2: sources: - id: topic-2 sinks: - id: topic-3 Services with different business logic may also consume from the same topic or produce to the same topic.\nServices may also have multiple sources and sinks, and they could have multiple transform steps to manipulate the data and turn it into the desired type. For example:\n# fields omitted for simplicity services: service-1: sources: - id: topic-1 steps: - operator: filter run: | fn filter_fn(input: String) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { Ok(input.len() \u0026gt; 5) } - id: topic-2 steps: - operator: map run: | fn map_fn(input: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { Ok(input.to_uppercase()) sinks: - id: topic-3 - id: topic-4 steps: - operator: filter run: | fn filter_fn(input: String) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { Ok(input.starts_with(\u0026#34;A\u0026#34;)) } operators Operators are pre-defined functions that can safely open the system for transformations. The operators have opinionated function signatures but flexible types. Some operators may be used independently, whereas others must be chained to accomplish the task.\nThe system exposes the following operators:\nfilter map filter-map flat-map assign-timestamp assign-key update-state flush Operators are defined in detail in the Operators Section.\ninline-functions Inline Functions is where you may define your custom logic. These inline functions are suitable for simple hello world transformations. In subsequent releases, we\u0026rsquo;ll introduce external imports where you can express complex tranformations.\nInline functions are defined inside operators as follows:\n- operator: filter run: | fn user_filter_fn(user: User) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { if user.age \u0026lt; 5 { Ok(false) } else { Ok(true) } } In this example, we define a function named user_filter_fn with input user and output bool that performs a filter operation to remove users under the age of 5.\nFor additional examples checkout Stateful Services Examples in github.\nstates States are aggregate objects that accumulate data from the event streams. The state objects are defined by the users and maintained by the system. The system ensures the states object is durable and survives restarts.\nStates follow the CQRS architecture, where each state has one writer and multiple readers.\nState writer (command) The states are defined inside the transforms block of a services as follows:\ncar-colors-counter-service: transforms: states: car-color-state: type: key-value properties: key: type: string value: type: u32 steps: ... ... The states are key-value objects where the key and value can be arbitrary types. In this example, key is the car color and value is the number of cars of each color. The state object is updated by one of the functions the steps.\nState reader (query) The state can be read from any other service the pipeline. To read the state it need to be referenced first:\ncars-prediction-service: transforms: states: car-color-state: from: bcar-colors-counter-service.car-color-state steps: ... The car-color-state is now usable in any of the steps of the cars-prediction-service.\nReferences Operators SSDK ","description":"Stateful Streaming Services file definition and examples.","keywords":null,"summary":"The data pipeline file defines the composition between services, data streams, and state objects. It describes the end-to-end application DAG, including the source and sink topics, data types, user-defined smartmodules, stateful windows, and aggregate.\nServices communicate with each other via topics, hence the service composition is defined by the topics they consume and produce.\nData Pipeline Template The data pipeline file is defined in YAML and has the following top level sections:","title":"Data Pipeline File","url":"http://localhost:1315/docs/stateful-services/data-pipeline-file/"},{"body":"InfinyOn IoT edge is a ~14 Mb binary that runs on ARMv7 chips on less than 256 MB memory. We are working with teams building the future of monitoring dynamic assets to push the boundaries of edge data stream processing.\nIf connected, InfinyOn IoT edge sends telemetry and events to the InfinyOn Cloud in real-time using mirroring.\nIf disconnected, the InfinyOn IoT edge stream processor caches events locally. When the connection resumes, the InfinyOn IoT edge stream processor brings InfinyOn Cloud up to date and continues mirroring until the subsequent connection loss.\n~\u0026gt; EXPERIMENTAL FEATURE Join us on Discord to give us feedback as we make this feature production ready.\nBenefits The benefits of the InfinyOn solution are as follows:\nReliable edge-to-cloud synchronization: Real-time publishing when connected. Automatic synchronization after reconnect. Edge devices can be offline for extended periods (days). Edge collection without downtime when disconnected. Reliable local caching for gigabytes of data. Simplified logic for edge clients. Edge cluster provides a reliable connection to the local clients. Intelligent processing at the edge with InfnyOn Smartmodules filter transform enrich Hierarchical processing, where you decide where to apply the transformations. Built-in cloud connectors to push events to databases and other core products. Installation In this toutorial we\u0026rsquo;ll use VM emulator to create and edge endpoint and mirror traffic to InfinyOn Cloud.\nLet\u0026rsquo;s get started.\nSetup InfinyOn Cloud Mirroring is an experimental feature using a development cluster. Please get in touch with us on Discord to request access for your organization. Upon approval, please continue as follows:\nCreate a Cloud account Using your web browser, navigate to https://dev.infinyon.cloud/signup, where this experimental feature is available.\nAfter the account is created, you will be placed in the Dashboard. You may choose to create a cluster in the GUI. In this tutorial, we\u0026rsquo;ll create a cluster using the CLI later.\nDownload fluvio binary Download and install mirroring binary.\n~\u0026gt; Note: This is an experimental build that conflicts with other fluvio installations. To ensure proper installation, please rename or remove ~/.fluvio/ directory.\nUse curl to download and install:\n%copy%\ncurl -fsS https://hub.infinyon.cloud/install/install.sh | VERSION=\u0026#39;0.10.15-dev-2+mirroring-9961bdb\u0026#39; bash Make sure to add .fluvio/bin to the $PATHas specified in the installation script.\nLogin to InfinyOn Cloud Login to InfinyOn Cloud (dev):\n%copy%\nfluvio cloud login --remote=https://dev.infinyon.cloud --use-oauth2 Leave out --use-oauth2 if you prefer username/password method.\nProvision a new Cluster Let\u0026rsquo;s provision a new cluster in AWS eu-central using the experimental fluvio version:\n%copy%\nfluvio cloud cluster create --region aws-eu-central-1 --version 0.10.15-dev-2+mirroring-b4f07fc Check cluster status:\n%copy%\nfluvio cluster status Next, we\u0026rsquo;ll configure the cluster to receive traffic from the edge clusters.\nCreate the mirror topic Each edge cluster mirror connects a partion of a topic, where each partition has a 1-to-1 relationship with the edge cluster.\nCreate a partition assignment file with an array of edge mirros we expect to connect this cluster:\n%copy%\necho \u0026#39;[ \u0026#34;edge1\u0026#34;, \u0026#34;edge2\u0026#34; ]\u0026#39; \u0026gt; assignment_file.json Apply the configuration file to create the topic:\n%copy%\nfluvio topic create edge-topic --mirror-assignment assignment_file.json List partitions to check the assignment:\n%copy%\nfluvio partition list It should display all partitions:\nTOPIC PARTITION LEADER MIRROR REPLICAS RESOLUTION SIZE HW LEO LRS FOLLOWER OFFSETS edge-topic 0 5001 edge1 [] Online 0 B 0 0 0 0 [] edge-topic 1 5001 edge2 [] Online 0 B 0 0 0 0 [] We created 2 partitions, but we\u0026rsquo;ll only use one in this tutorial.\nRegister Edge cluster Let\u0026rsquo;s register the edge cluster edge1 to inform our Cloud cluster to accept connection requests from the remote device:\n%copy%\nfluvio cluster remote-cluster register --type mirror-edge edge1 List remote clusters to check their status:\n%copy%\nfluvio cluster remote-cluster list It should show the following:\nRemoteCluster RemoteType Paired Status Last Seen edge1 mirror-edge - - - Create a new directory In the next step, we\u0026rsquo;ll create a configuration file that we\u0026rsquo;ll need to pass along to the edge device. It\u0026rsquo;s easier if we make a clean directory and pass it along to the VM emulator:\n%copy%\nmkdir -p ~/local/projects/mirror; cd ~/local/projects/mirror Generate metadata for Edge Cluster Each edge cluster requires a unique metadata file that informs the edge cluster how to connect with the target cluster. Create the config file by running the following command:\n%copy%\nfluvio cluster remote-cluster metadata export --topic edge-topic --mirror edge1 --file edge1.json The Cloud cluster configuration is now complete. Next, we\u0026rsquo;ll create an edge cluster and configure a mirror topic that synchronizes data to the Cloud.\nInstall Edge Cluster on Local VM We\u0026rsquo;ll start an edge cluster on our local computer in a VM using OrbStack.\nInstal OrbStack We\u0026rsquo;ll use OrbStack for the VM management:\nInstall OrbStack\nStart Ubuntu VM machine.\nClick the VM to open a terminal.\nUsing the terminal, navigate to your data directory:\n%copy%\ncd local/projects/mirror All files we\u0026rsquo;ve generated on the local machines should be visible here.\nDownload fluvio binaries Download binaries:\n%copy%\ncurl -fsS https://hub.infinyon.cloud/install/install.sh | VERSION=\u0026#39;0.10.15-dev-2+mirroring-9961bdb\u0026#39; bash Add to path:\n%copy%\necho \u0026#39;export PATH=\u0026#34;${HOME}/.fluvio/bin:${PATH}\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Run the following command to double check:\n%copy%\nfluvio version Start Edge Cluster We\u0026rsquo;ll use the metadata edge1 to start the edge cluster:\n%copy%\nfluvio cluster start --read-only edge1.json Let\u0026rsquo;s check the partitions:\n%copy%\nfluvio partition list The edge device should show the following partition:\nTOPIC PARTITION LEADER MIRROR REPLICAS RESOLUTION SIZE HW LEO LRS FOLLOWER OFFSETS edge-topic 0 5001 upstream:0 [] Online 0 B 0 0 0 0 [] Test 1: Mirroring from VM Edge to Cloud Let\u0026rsquo;s produce on the Edge VM cluster and consume from the Cloud cluster.\nProduce on Edge Produce on the edge terminal:\n%copy%\nfluvio produce edge-topic \u0026gt; 1 Ok! \u0026gt; 2 Ok! Consume from Cloud Consume on the cloud terminal:\n%copy%\nfluvio consume edge-topic --mirror edge1 -B 1 2 Mirror test is successful.\nTest 2: Cloud Cluster Offline To simulate a disconnect, we\u0026rsquo;ll perform the following steps:\nTurn off the network connection to the internet.\nProduce records on the edge terminal.\n%copy%\nfluvio produce edge-topic \u0026gt; 3 Ok! \u0026gt; 4 Ok! \u0026gt; 5 Ok! Turn on the network connection and check that the data is synced. The topic on the Cloud cluster should automatically synchronize with the edge cluster.\nConsume from the cloud terminal: Wait for the connection retry interval to trigger for the new records to arrive, then consume:\n%copy%\nfluvio consume edge-topic --mirror edge1 -B 1 2 3 4 5 The disconnect test was successful.\nTest 3: Edge Cluster Offline This test ensures that the edge cluster will preserve all cached data following a power loss.\nRestart Edge Cluster On the edge terminal, shutdown the cluster:\n%copy%\nfluvio cluster shutdown --local Restart the cluster:\n%copy%\nfluvio cluster upgrade --read-only edge1.json Consume from edge cluster On the edge terminal, consume from the cluster:\n%copy%\nfluvio consume edge-topic -B 1 2 3 4 5 Produce records and observe that the mirror will resume the synchronization.\n\u0026#x1f389; Congratulations! These tests confirm that the synchronization from Edge to Cloud works as expected. It is now time to roll it out in your environment.\nJoin us on Discord if you have questions, or would like to suggest new improvements.\nRelated \u0026ldquo;IoT Mirroring - Raspberry Pi to a Local Cluster\u0026rdquo; ","description":"Reliable IoT monitoring from movable Edges sensors with poor connections to Cloud.","keywords":null,"summary":"InfinyOn IoT edge is a ~14 Mb binary that runs on ARMv7 chips on less than 256 MB memory. We are working with teams building the future of monitoring dynamic assets to push the boundaries of edge data stream processing.\nIf connected, InfinyOn IoT edge sends telemetry and events to the InfinyOn Cloud in real-time using mirroring.\nIf disconnected, the InfinyOn IoT edge stream processor caches events locally. When the connection resumes, the InfinyOn IoT edge stream processor brings InfinyOn Cloud up to date and continues mirroring until the subsequent connection loss.","title":"IoT Mirroring - Edge VM to Cloud","url":"http://localhost:1315/docs/guides/iot-mirroring-cloud/"},{"body":"This advanced tutorial reuqires a Raspberry Pi and a local installation of your collector cluster running on Kubernetes. Checkout the basic versino at: \u0026ldquo;IoT Mirroring - Edge VM to Cloud\u0026rdquo;.\nRaspberry Pi to Local Cluster This section will use Raspberry Pi v3 running Ubuntu 32-bit as the edge device and our local machine for the target cluster. Let\u0026rsquo;s start with installing and configuring the target cluster.\nInstall Target Cluster on Local Machine Installing the target cluster on Linux or Mac requires Kubernetes. Use the following instructions to set up Kubernetes on your local machine.\nInstall Rancher Desktop for Mac Install k3d, kubectl and helm for Linux -\u0026gt; Note Install Kubernetes, then use the instructions below to install the experimental fluvio binaries.\nCreate a new directory Create a clean directory for the configuration and metadata files:\n%copy%\nmkdir -p local/projects/mirror; cd local/projects/mirror Download fluvio binary Download and install mirroring binary.\n~\u0026gt; Note: This is an experimental build that conflicts with other fluvio installations. To ensure proper installation, please rename or remove ~/.fluvio/ directory.\nUse curl to download and install:\n%copy%\ncurl -fsS https://hub.infinyon.cloud/install/install.sh | VERSION=\u0026#39;0.10.15-dev-2+mirroring-9961bdb\u0026#39; bash Make sure to add .fluvio/bin to the $PATHas specified in the installation script.\nStart target cluster Use the fluvio binary to start the cluster:\n%copy%\nfluvio cluster start --local Check the result with:\n%copy%\nfluvio cluster status Create the mirror topic Mirror topics on the upstream clusters has multiple partitions, where each partition has a 1-to-1 relationship with the edge cluster.\nCreate a partition assignment file to define the edge devices:\n%copy%\necho \u0026#39;[ \u0026#34;edge1\u0026#34;, \u0026#34;edge2\u0026#34; ]\u0026#39; \u0026gt; assignment_file.json Apply the configuration file to create the topic:\n%copy%\nfluvio topic create edge-topic --mirror-assignment assignment_file.json List partitions to check the assignment:\n%copy%\nfluvio partition list It should display all partitions:\nTOPIC PARTITION LEADER MIRROR REPLICAS RESOLUTION SIZE HW LEO LRS FOLLOWER OFFSETS edge-topic 0 5001 edge1 [] Online 0 B 0 0 0 0 [] edge-topic 1 5001 edge2 [] Online 0 B 0 0 0 0 [] Register Edge clusters Use the remote-cluster CLI to register the edge clusters (edge1 and edge2) with the upstream cluster:\nEdge 1:\n%copy%\nfluvio cluster remote-cluster register --type mirror-edge edge1 Edge 2:\n%copy%\nfluvio cluster remote-cluster register --type mirror-edge edge2 List remote clusters to check their status:\n%copy%\nfluvio cluster remote-cluster list It should show the following:\nRemoteCluster RemoteType Paired Status Last Seen edge1 mirror-edge - - - edge2 mirror-edge - - - Generate Metadata for Edge Clusters Each edge cluster requires a unique metadata file that gives the edge cluster the information to connect to the upstream cluster and the topic/mirror where the data is synchronized.\nGenerate a metadata file for each cluster:\nEdge 1:\nThe target edge device is a Virtual Machine emulating an IoT device:\n%copy%\nfluvio cluster remote-cluster metadata export --topic edge-topic --mirror edge1 --upstream host.orb.internal --file edge1.json Edge 2:\nThe target edge device is a Raspberry Pi device. You may skip this if you don\u0026rsquo;t have such a device.\n-\u0026gt; Note: The IP address of our machine where the upstream server is running is 192.168.79.252. Please identify your own IP address and replace it in the command below.\n%copy%\nfluvio cluster remote-cluster metadata export --topic edge-topic --mirror edge2 --upstream 192.168.79.252 --file edge2.json We\u0026rsquo;ll transfer these files to edge devices in the following sections.\nInstall Edge Cluster on Raspberry Pi (optional) We\u0026rsquo;ll use the same procedure as before to mirror from Raspberry Pi to the same upstream cluster. The test below was performed on a Raspberry Pi v3 running Ubuntu 32-bit image.\nDownload metadata file We\u0026rsquo;ll use the metadata file edge2.json that we\u0026rsquo;ve exported above to provision this device.\n-\u0026gt; Note: Iddentify the IP address of your Raspberry Pi device and it replace below\nUsing the upstream terminal, let\u0026rsquo;s scp the edge2.json file to the edge device:\n%copy%\nscp edge2.json fluvio@192.168.79.139:~ Login into the edge device Spawn a new terminal and login into the Raspberry Pi:\n%copy%\nssh fluvio@192.168.79.139 Download fluvio binaries On the raspberry pi, run the following command:\n%copy%\ncurl -fsS https://hub.infinyon.cloud/install/install.sh | VERSION=\u0026#39;0.10.15-dev-2+mirroring-9961bdb\u0026#39; bash Run fluvio version to double check.\nStart cluster We\u0026rsquo;ll use the metadata file to start the edge cluster on the Raspberry Pi:\n%copy%\nfluvio cluster start --read-only edge2.json Let\u0026rsquo;s check the partitions:\n%copy%\nfluvio partition list The edge device should show the following partition::\nTOPIC PARTITION LEADER MIRROR REPLICAS RESOLUTION SIZE HW LEO LRS FOLLOWER OFFSETS edge-topic 0 5001 Source:upstream:5001 [] Online 0 B 11 11 11 0 [] Test 1: Mirror from Raspberry Pi Edge to Upstream Let\u0026rsquo;s produce on the Raspberry Pi and consume from the upstream cluster.\nProduce to edge cluster Produce on the pi terminal:\n%copy%\nfluvio produce edge-topic \u0026gt; A Ok! \u0026gt; B Ok! Consume from upstream Consume on the upstream terminal:\nfluvio consume edge-topic --mirror edge2 -B A B Mirror test is successful.\nTest2: Upstream Cluster Offline Shutdown the upstream cluster and check that the edge cluster can continue receiving records. Then, resume the upstream cluster and ensure the data is synchronized and can consumed on both sides.\nShutdown upstream cluster On the upstream terminal, shutdown the cluster:\n%copy%\nfluvio cluster shutdown --local kubectl delete spu custom-spu-5001 Ensure the cluster is shutdown:\n%copy%\nfluvio cluster status On edge cluster Produce a few more records on the pi terminal:\nfluvio produce edge-topic C D E Reconnect upstream cluster \u0026amp; consume from topic On the upstream terminal, restart the cluster:\n%copy%\nfluvio cluster upgrade --local The topic on the upstream cluster should automatically synchronize with the edge cluster.\n-\u0026gt; Note: Wait for the connection retry interval to trigger for the new records to arrive.\nLet\u0026rsquo;s consume:\n%copy%\nfluvio-0.10.15 consume edge-topic --mirror edge2 -B A B C D E The disconnect test was successful.\nTest 3: Edge Cluster Offline This test ensures that the edge cluster will not lose data following a power loss.\nRestart edge cluster On the edge terminal, shutdown the cluster:\n%copy%\nfluvio cluster shutdown --local Restart the cluster:\n%copy%\nfluvio cluster upgrade --read-only edge2.json Consume from edge cluster On the pi terminal, consume from the cluster:\n%copy%\nfluvio consume edge-topic -B A B C D E Produce records and observe that the mirror will resume the synchronization.\n\u0026#x1f389; Congratulations! You have successfully tested edge mirroring using Rapsberry Pi. It is now time to roll it out in your environment.\n","description":"Reliable IoT monitoring from movable or static Edges sensors.","keywords":null,"summary":"This advanced tutorial reuqires a Raspberry Pi and a local installation of your collector cluster running on Kubernetes. Checkout the basic versino at: \u0026ldquo;IoT Mirroring - Edge VM to Cloud\u0026rdquo;.\nRaspberry Pi to Local Cluster This section will use Raspberry Pi v3 running Ubuntu 32-bit as the edge device and our local machine for the target cluster. Let\u0026rsquo;s start with installing and configuring the target cluster.\nInstall Target Cluster on Local Machine Installing the target cluster on Linux or Mac requires Kubernetes.","title":"IoT Mirroring - Raspberry Pi to Local Cluster","url":"http://localhost:1315/docs/guides/iot-mirroring-local/"},{"body":"In this guide, you\u0026rsquo;ll learn how to package your custom app with a Fluvio client into a Docker image, and how to run a container with docker CLI.\nWe assume you are connected to a cluster with the CLI. If this is not the case, please login to Cloud before continuing.\nExample custom Fluvio client To help illustrate the basic development workflow for all services using a Fluvio client, we\u0026rsquo;ll create this sample Rust service.\nRun these commands to initialize the project\n%copy%\n$ cargo new fluvio-rust-client $ cd fluvio-rust-client $ cargo add async-std --features attributes $ cargo add chrono fluvio Copy this code into src/main.rs\n%copy%\nuse async_std::stream::StreamExt; use chrono::Local; use fluvio::metadata::topic::TopicSpec; use fluvio::{Fluvio, RecordKey}; const TOPIC_NAME: \u0026amp;str = \u0026#34;hello-rust\u0026#34;; const PARTITION_NUM: u32 = 0; const PARTITIONS: u32 = 1; const REPLICAS: u32 = 1; /// This is an example of a basic Fluvio workflow in Rust /// /// 1. Establish a connection to the Fluvio cluster /// 2. Create a topic to store data in /// 3. Create a producer and send some bytes /// 4. Create a consumer, and stream the data back #[async_std::main] async fn main() { // Connect to Fluvio cluster let fluvio = Fluvio::connect().await.unwrap(); // Create a topic let admin = fluvio.admin().await; let topic_spec = TopicSpec::new_computed(PARTITIONS, REPLICAS, None); let _topic_create = admin .create(TOPIC_NAME.to_string(), false, topic_spec) .await; // Create a record let record = format!(\u0026#34;Hello World! - Time is {}\u0026#34;, Local::now().to_rfc2822()); // Produce to a topic let producer = fluvio::producer(TOPIC_NAME).await.unwrap(); producer.send(RecordKey::NULL, record).await.unwrap(); // Fluvio batches outgoing records by default, so flush producer to ensure all records are sent producer.flush().await.unwrap(); // Consume last record from topic let consumer = fluvio::consumer(TOPIC_NAME, PARTITION_NUM).await.unwrap(); let mut stream = consumer.stream(fluvio::Offset::from_end(1)).await.unwrap(); if let Some(Ok(record)) = stream.next().await { let string = String::from_utf8_lossy(record.value()); println!(\u0026#34;{}\u0026#34;, string); } } Example Dockerfile Save the following Dockerfile. This Dockerfile adds our workflow an starts it with cargo run, which will always build the code before it starts.\n%copy%\nFROM rust:1.73.0 # Run as the `fluvio` user instead of root ENV USER=fluvio RUN useradd --create-home \u0026#34;$USER\u0026#34; USER $USER WORKDIR /home/fluvio # Copy your Rust project and run it COPY --chown=$USER:$USER Cargo.toml . COPY --chown=$USER:$USER src ./src CMD /usr/local/cargo/bin/cargo run Running docker image The fluvio clients search for connection information from a config file located at $HOME/.fluvio/config.\nRun with docker run In this example, we build our docker image with docker build then start a container from the image with docker run. We volume mount our host\u0026rsquo;s config file to the container when we start.\n%copy%\n$ docker build -t fluvio-client-example . $ docker run -v $HOME/.fluvio/config:/home/fluvio/.fluvio/config --init --rm fluvio-client-example Afer running you\u0026rsquo;ll see the following output:\nRunning `target/debug/fluvio-rust-client` Hello World! - Time is Tue, 10 Oct 2023 21:09:05 +0000 The same event has been published to the topic:\n%copy first-line%\n$ fluvio consume -Bd hello-rust Consuming records from \u0026#39;hello-rust\u0026#39; starting from the beginning of log Hello World! - Time is Tue, 10 Oct 2023 21:09:05 +0000 Run with docker compose (alternative) Copy this yaml file and save it as docker-compose.yaml\n%copy%\n# docker-compose.yaml version: \u0026#34;3\u0026#34; services: example: build: . volumes: - $HOME/.fluvio/config:/home/fluvio/.fluvio/config:ro In this example, we build our docker image with docker compose build then start a container from the image with docker compose up. We volume mount our host\u0026rsquo;s config file to the container when we start.\n%copy%\n$ docker compose build $ docker compose up ","description":"A guide to run InfinyOn Cloud client in Docker environment","keywords":null,"summary":"In this guide, you\u0026rsquo;ll learn how to package your custom app with a Fluvio client into a Docker image, and how to run a container with docker CLI.\nWe assume you are connected to a cluster with the CLI. If this is not the case, please login to Cloud before continuing.\nExample custom Fluvio client To help illustrate the basic development workflow for all services using a Fluvio client, we\u0026rsquo;ll create this sample Rust service.","title":"Custom Clients with Docker","url":"http://localhost:1315/docs/guides/infinyon-cloud-docker/"},{"body":"Stateful Services Developer Kit (ssdk) is a binary, shipped with fluvio, that helps developers build, test, and deploy stateful services. The first version works with Rust, and upcoming versions will enable Python and Javascript.\nCommands\nssdk setup ssdk generate ssdk build ssdk update ssdk clean ssdk log ssdk version ssdk run \u0026raquo; show state \u0026raquo; exit Download FVM and Install SSDK Download fluvio version manager (fvm), the package manager for fluvio and ssdk:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\nInstall the preview release:\n%copy first-line%\n$ fvm install 0.11.2-ssdk-preview3 SSDK has the following comamand hierarchy:\n%copy first-line%\n$ ssdk -h Stateful Service Development Kit utility Usage: ssdk \u0026lt;COMMAND\u0026gt; Commands: build Build Stateful Service clean Clean generated files from service directory generate Generate SSDK projects from data-pipeline.yaml update Update SSDK project run Run Stateful Service setup Setup pre-requisites for Stateful Service version Display version information log Print data pipeline logs help Print this message or the help of the given subcommand(s) ssdk setup Command line syntax for ssdk setup:\n%copy first-line%\n$ ssdk setup -h Setup pre-requisites for Stateful Service Usage: ssdk setup Run ssdk setup to configure your local environment.\n%copy first-line%\n$ ssdk setup ssdk generate Command line syntax for ssdk generate:\n%copy first-line%\n$ ssdk generate -h Generate SSDK projects from data-pipeline.yaml Usage: ssdk generate Example SSDK generate requires a data-pipeline.yaml the current directory:\n%copy first-line%\n$ ssdk generate The command parses the file and creates a Rust project in a subdirectory called project. For additional information, check out Data Pipeline File.\nssdk build Command line syntax for ssdk build:\n%copy first-line%\n$ ssdk build -h Build Stateful Service Usage: ssdk build Example The command builds the Rust project and generates all WASM Components.\n%copy first-line%\n$ ssdk build Checkout the project directory to see the project hierarchy and the target components.\nssdk update Command line syntax for ssdk update:\n%copy first-line%\n$ ssdk update -h Update SSDK project Usage: ssdk update [OPTIONS] Options: -f, --force force to apply all updates -d, --dry-run print changes and exit This command is used to update the project based on updated in the data-pipeline.yaml file.\nThe update command makes code changes that cannot be reversed. As a safety measure, the command generates a diff and asks for confirmation before proceeding. If you want to bypass the confirmation prompt, use the -f and -d flags.\nssdk clean Command line syntax for ssdk clean:\n%copy first-line%\n$ ssdk clean -h Clean generated files from project directory Usage: ssdk clean This command is used to clean-up the project directory and start again.\nExample Let\u0026rsquo;s say we made major changes to the data-pipeline.yaml file and the previous version is no longer relevant. We can use clean to remove the project and start over:\n%copy first-line%\n$ ssdk clean ssdk log Use ssdk log to view you print statements:\n%copy first-line%\n$ ssdk log -h Print data pipeline logs Usage: ssdk log [OPTIONS] Options: -f, --follow Specify if the logs should be streamed Use --f --follow option to watch the logs.\nExample The following code splits sentences into words and has a println statement:\n- operator: flat-map run: | fn split_sentence(sentence: String) -\u0026gt; Result\u0026lt;Vec\u0026lt;String\u0026gt;, String\u0026gt; { let result = sentence.split_whitespace().map(String::from).collect(); println!(\u0026#34;{:?}\u0026#34;, result); Ok(result) } For the sentence with This is a test, the output will be:\n%copy first-line%\n$ ssdk log 2024-01-06T02:08:09.735861+00:00 split-sentence INFO [\u0026#34;This\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;test\u0026#34;] ssdk version Command line syntax for ssdk version:\n%copy first-line%\n$ ssdk version -h Display version information Usage: ssdk version Example Find out what version you are running:\n%copy first-line%\n$ ssdk version ssdk run Run the project and open the command line in interactive mode with ssdk run:\n%copy first-line%\n$ ssdk run -h Run Stateful Service Usage: ssdk run [OPTIONS] Options: --ui start the workflow viewer ui [env: UI=] -p, --port \u0026lt;PORT\u0026gt; port for web server to listen on [env: PORT=] [default: 8000] Use --ui to view a graphical represetation of the project in a web browser. The default port is 8000, and you may change it with the --port flag.\nExample Navigate to the project directory, ensure it\u0026rsquo;s built, and run to start the project in interactive mode:\n%copy first-line%\n$ ssdk run --ui Please visit http://127.0.0.1:8000 to use SSDK Studio ... \u0026gt;\u0026gt; - interactive mode SSDK run opens the command line in interactive mode:\n%copy first-line%\n\u0026gt;\u0026gt; -h SSDK - Stateful Service Development Kit Usage: \u0026lt;COMMAND\u0026gt; Commands: show Show or List states. Use `show state --help` for more info exit Stop interactive session \u0026gt;\u0026gt; show state Use show state to peak into the internal state objects managed by the system:\nCommand line syntax for show state:\n%copy first-line%\n\u0026gt;\u0026gt; show state -h List all states or show state for given key Usage: show state [OPTIONS] [NAMESPACE] Arguments: [NAMESPACE] namespace of state Options: --key \u0026lt;KEY\u0026gt; key of state --filter \u0026lt;FILTER\u0026gt; filter regex Example For example, to show all state objects:\n%copy first-line%\n\u0026gt;\u0026gt; show state Namespace Keys Type word-processing-window/count-per-word/state 22 u32 word-processing-window/sentence/topic.offset 1 offset The states are organized by namespace where the first keyword is the service name word-processing-window, followed by the function name count-per-word or topic sentence. The state is the temporary result computed by the window function, whereas topic.offset is the offset of the last record read from the sentence topic.\nTo further inspect the objects, use the show state command with the name of the namespace:\n%copy first-line%\n\u0026gt;\u0026gt; show state word-processing-window/count-per-word/state Key Value Window book 1 2023-12-28T00:32:00Z:2023-12-28T00:32:20Z but 1 2023-12-28T00:32:00Z:2023-12-28T00:32:20Z ... You also have the option to filter the state by key (--key), or a regex (--filter) operation.\n\u0026gt;\u0026gt; exit Exit interactive mode:\n%copy first-line%\n\u0026gt;\u0026gt; exit bye! References Data Pipeline File ","description":"Stateful Services Developer Kit (ssdk) command definitions.","keywords":null,"summary":"Stateful Services Developer Kit (ssdk) is a binary, shipped with fluvio, that helps developers build, test, and deploy stateful services. The first version works with Rust, and upcoming versions will enable Python and Javascript.\nCommands\nssdk setup ssdk generate ssdk build ssdk update ssdk clean ssdk log ssdk version ssdk run \u0026raquo; show state \u0026raquo; exit Download FVM and Install SSDK Download fluvio version manager (fvm), the package manager for fluvio and ssdk:","title":"SSDK Command Line Interface","url":"http://localhost:1315/docs/stateful-services/ssdk/"},{"body":"Operators are primitive APIs that enable developers to customize their data. There are two types of operators:\nBasic Operators filter map filter-map flat-map Window Operators assign-timestamp assign-key update-state flush aggregate State Operators Checkout States section for additional information.\nBasic Operators Basic operators may look familiar, as they were previously defined in SmartModules. These operators perform simple operations and may be used independently or chained in composite operations.\nfilter The filter operator takes a record and returns a boolean value. The return value tells the system to drop or pass the record to the next operator:\ntrue - pass the record to the next operator. false - drop the record. An inline filter operator is defined as follows:\n%copy%\n- operator: filter run: | fn only_errors(log: LogRecord) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { match log.level { \u0026#34;ERROR\u0026#34; =\u0026gt; Ok(true), _ =\u0026gt; Ok(false) } In this example, the operator filters out all records with a level other than ERROR.\nmap The map operator takes an input record, applies a transformation, and then forwards the modified record to the next operator.\nAn inline map operator is defined as follows:\n%copy%\n- operator: map run: | fn sentence_to_uppercase(input: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { Ok(input.to_uppercase()) } In this example, the operator transforms each string to uppercase.\nfilter-map The filter-map operator combines a filter and a map operation; it takes a record and returns a mapped record or none. The return value tells the system to pass the records it receives to the next operator:\nSome(Record) - pass the record to the next operator. None - do nothing. An inline filter-map operator is defined as follows:\n%copy%\n- operator: filter-map run: | fn long_sentence_to_uppercase(input: String) -\u0026gt; Result\u0026lt;Option\u0026lt;String\u0026gt;, String\u0026gt; { if input.len() \u0026gt; 10 { Ok(Some(input.to_uppercase())) } else { Ok(None) } } In this example, the operator transforms sentences longer than 10 characters to uppercase.\nflat-map The flat-map operator splits records into an array of records and then forwards each record to the next operator.\nAn inline flat-map operator is defined as follows:\n%copy%\n- operator: flat-map run: | fn split_sentence(sentence: String) -\u0026gt; Result\u0026lt;Vec\u0026lt;String\u0026gt;, String\u0026gt; { Ok(sentence.split_whitespace().map(String::from).collect()) } In this example, the operator splits sentences into words.\nWindow Operators Window operators address a well-defined stream processing problem described in depth by the \u0026ldquo;The Dataflow Model\u0026rdquo; whitepaper. A window operation turns data streaming records into a group of finite records, also known as bounded context, defined by the window size computed by a watermark operation. Fluvio performs a window processing operation by chaining multiple operators to assign timestamps, group them by key, and apply custom operations.\nWhile there are several types of windows, and Fluvio will eventually implement all of them, this preview will focus on two: tumbling window and sliding window.\nTumbling windows are equal-sized, continuous and non-overlapping windows. Each record is present in exaclty one window.\nSliding windows are equal-sized, continuous and overlapping windows. Each record may be present in one or more window.\n~\u0026gt; Sliding windows is in development targetted for the next release.\nFor example, you would configure a tumbling window with a 10-second size as follows:\n%copy%\ntransforms: window: tumbling: duration: 10s Window processing operation is a configuration setting rather than an operator.\nassign-timestamp The assign-timestamp operator lets you choose a timestamp for the watermark. Watermark maps records to windows and controls when the window will flush.\nThe following example shows how to update the timestamp from the record metadata:\n%copy%\n- operator: assign-timestamp run: | fn assign_timestamp(user_event: UserEvent, event_time: i64) -\u0026gt; Result\u0026lt;i64, String\u0026gt; { Ok(event_time) } Assuming the user_event record has a timestamp field, the following example shows how to update the timestamp from the record value:\n%copy%\n- operator: assign-timestamp run: | fn assign_timestamp(user_event: UserEvent, event_time: i64) -\u0026gt; Result\u0026lt;i64, String\u0026gt; { Ok(user_event.timestamp) } The assign_timestamp operation is mandatory in window processing, as it helps assign records to specific windows.\nassign-key The assign-key operation takes streams of records and assigns them a key. Records assigned to the same key are processed together.\nThe following example shows how to use assign-key operator to group cars by their color:\n- operator: assign-key run: | fn key_by_color(car: Car) -\u0026gt; Result\u0026lt;String, String\u0026gt; { Ok(car.color) } While the assign-key operator can partition any data stream, it is most useful in window processing operations.\nupdate-state The update-state operation takes a record and updates a state object. When used in a window operation, the system automatically retrieves the state value that matches the record key.\nThe following example shows how to use update-state to count cars:\ntypes: car: type: object properties: color: type: string count: type: u32 transforms: states: car-count: type: key-value properties: key: type: string value: type: u32 - operator: process run: | fn count_cars(_car: Car) -\u0026gt; Result\u0026lt;(), String\u0026gt; { car_count().increment(1); Ok(()) } The update-state operator retrieves the state object car_count() and increments its value by one.\nflush The flush section sits in parallel with the transforms section in a window operation. This section informs the system watermark where to send the state content when the window closes. The flush operator works in tandem with the aggregate operator to compute the result.\naggregate The aggregate operator takes the full content of a window and performs the final computation before sending the result to the sink. The following example shows how to apply an aggregate function to sort the cars by color:\ntypes: car: type: object properties: color: type: string count: type: u32 cars: type: list items: type: car transforms: states: car-count: type: key-value properties: key: type: string value: type: u32 flush: operator: aggregate run: | fn sort_by_color(car_count: CarCount) -\u0026gt; Result\u0026lt;Cars, String\u0026gt; { let mut kv = car_count; kv.sort_by(|a, b| a.value.cmp(\u0026amp;b.value)); kv.reverse(); Ok(kv.iter().map( | entry | Car{ word: entry.key.clone(), count: entry.value }).collect()) } The aggregate operator reads all car-count key/value partitions, sorts them by value, and returns a list of cars sorted from highest to lowest.\nReferences Data Pipeline File ","description":"Stateful Services operators definition.","keywords":null,"summary":"Operators are primitive APIs that enable developers to customize their data. There are two types of operators:\nBasic Operators filter map filter-map flat-map Window Operators assign-timestamp assign-key update-state flush aggregate State Operators Checkout States section for additional information.\nBasic Operators Basic operators may look familiar, as they were previously defined in SmartModules. These operators perform simple operations and may be used independently or chained in composite operations.\nfilter The filter operator takes a record and returns a boolean value.","title":"Operator Definitions","url":"http://localhost:1315/docs/stateful-services/operators/"},{"body":"State is a mechanism that facilitates the collection and retrieval of data generated by streams of records. With states, pipelines can build tables, compute aggregates, join datasets, perform anomaly detections, and execute any other operation that requires collections of records.\nStates are specified within the transforms section of a service, where each state is designated with one owner/writer but can have multiple readers.\nFluvio supports two types of states:\nSimple - the value has primitive types. Table - the value has arrow type. These states can be referenced across services to look-up values or perform composite operations like joins. Check out Ref State below for details.\nSimple States A simple state has a key defining the partition and a value for saving the object. The following example shows a state object where the value is a 32 bit unsigned integer.\ntransforms: states: - name: count-per-word type: keyed-state properties: key: type: string value: type: u32 The states are accessed through functions. For example to increment the value of count-per-word, we define an operator that reads count_per_word and updates the value. Then you can perform increment , set, get operation on the state:\n- operator: map run: | fn increment_word_count(word: String) -\u0026gt; Result\u0026lt;WordCount, String\u0026gt; { let counter = count_per_word(); let value = counter.increment(1); Ok(WordCount { word: word, count: value as u32, }) } In this example, we used increment to add one to the previous value.\nTable States The table state has a partition key and a value of arrow-row type. The arrow type may look familiar, as it is a mapping of the arrow dataframe type. These dataframes are portable with several full features libraries such as Polars. Fluvio uses adapters to map dataframes with with a these libraries.\ntransforms: states: - name: temperature type: keyed-state properties: key: type: string value: type: arrow-row properties: sensor: type: string temperature: type: f32 The assign-key and update-state operator to partition and populate the table with temperature events:\nsteps: - operator: assign-key run: | fn key_by_id(event: Temp) -\u0026gt; Result\u0026lt;i32, String\u0026gt; { Ok(event.id) } - operator: update-state run: | fn update_temperature(event: Temp) -\u0026gt; Result\u0026lt;(), String\u0026gt; { let mut temp = temperature(); temp.sensor = event.sensor; temp.temperature = event.temperature; temp.update(); Ok(()) } Resulting in the following arrow representation:\nkey sensor temperature 0 SF 54 1 LA 38 3 MO 27 Tabes are used for enrichment, join operations, annomaly detection, or are periodically flushed to topics to diplay in dashboards.\nRef State Reference (aka Ref) States allows services to query states owned by other services. For example, a service interested in the temperature of a city, can query the temperature table managed by an external services.\nThe reference state is using keyword from as below:\ntransforms: states: - name: temperature from: update-temperature.temperature However, external eservices are not allowed to update states they do not own.\nState Engine Design The state engine implements the following rules:\nEach state has one service owner The state may be update in the following operators: filter, map, filter-map, flat-map Services with the sole purpose of updating the state must use update-state in the last operation. A state can be read from any number of services. ","description":"Stateful Services states definition","keywords":null,"summary":"State is a mechanism that facilitates the collection and retrieval of data generated by streams of records. With states, pipelines can build tables, compute aggregates, join datasets, perform anomaly detections, and execute any other operation that requires collections of records.\nStates are specified within the transforms section of a service, where each state is designated with one owner/writer but can have multiple readers.\nFluvio supports two types of states:\nSimple - the value has primitive types.","title":"States","url":"http://localhost:1315/docs/stateful-services/states/"},{"body":"There are 4 steps to the connector:\nProtocol: Parses data according to the wire format of the connected data platform. Extract: Extracts raw data from the protocol format and packages it neatly into data structures that may be used by subsequent stages or be produced directly to a topic. Filter (optional): A user-provided SmartModule that may determine whether a given record should be discarded before sending it over the network to Fluvio, saving bandwidth. Shape (optional): A user-provided SmartModule that may take the extracted data structures and transform them in to an application-specific format. The Protocol and Extract stages are built directly into the connector. They offer basic access to your data through the various protocols your data sources use.\nIn the Extract stage, your data is structured from whatever protocol it is sourced from.\nAdditionally, You can apply custom pre-processing or post-processing to data, before it arrives to or while it streams from a Fluvio topic. The Filter and Shape stages are provided through SmartModules.\nPowered by WebAssembly (also called wasm), SmartModules are pre-packaged or user-provided operations such as filters, maps, or aggregators that can be applied to records at various points in the streaming pipeline. Supporting access to your data while it is in transit provides you the ability to clean, transform and enrich your data before it is stored in a topic, or it exits the Fluvio cluster.\nStart a connector on InfinyOn Cloud, and let us manage the infrastructure\n","description":"","keywords":null,"summary":"There are 4 steps to the connector:\nProtocol: Parses data according to the wire format of the connected data platform. Extract: Extracts raw data from the protocol format and packages it neatly into data structures that may be used by subsequent stages or be produced directly to a topic. Filter (optional): A user-provided SmartModule that may determine whether a given record should be discarded before sending it over the network to Fluvio, saving bandwidth.","title":"Smart Connectors Core Concepts","url":"http://localhost:1315/docs/resources/connector-core-concepts/"},{"body":"Adaptors are language specific utilities and libraries that can be used in operator to simplify interaction with outside world or perform some common tasks. Following adaptors are available:\nHTTP Callout The http adaptor make it easy to make HTTP requests from your operator. It is based on the reqwest library. To use the http adaptor, you need to add adaptors section with http flag in your operator definition.\nFollowing snippet takes a sentence and translate english to spanish using external API. The Request is part of reqwest library and ssdk_http::blocking::send is used to send the request.\n- operator: map adaptors: - http run: | fn english_to_spanish(sentence: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { use ssdk_http::http::Request; let url = format!(\u0026#34;https://acme.com/translate?text={}\u0026#34;, sentence); let request = Request::builder().uri(url).body(\u0026#34;\u0026#34;).map_err(|e| e.to_string())?; let response = ssdk_http::blocking::send(request).map_err(|e| e.to_string())?; Ok(response.into_body()) } Polars Queries The polars are built-in adaptors that can be used to perform data manipulation and transformation on dataframes. Polars adaptor is enabled for operators that can access dataframe states and you don\u0026rsquo;t need to add any flag in your operator definition.\nThe polars rust api can be used when dataframe state is available. For example, following snippet show performing polar operator on state that return dataframe. Please refer to states states section for more details.\nIn here, count_per_word is a state that returns a dataframe and assign to variable df. Then any dataframe operation can be performed on df variable.\n- operator: map run: | fn map_words_to_occurrence(key: String) -\u0026gt; Result\u0026lt;String, String\u0026gt; { use polars::prelude::{col,lit,IntoLazy}; let df = count_per_word(); let val = df .clone() .lazy() .filter(col(\u0026#34;id\u0026#34;).eq(lit(key.clone()))) .collect() .expect(\u0026#34;parse\u0026#34;); println!(\u0026#34;{:#?}\u0026#34;, val); if let Some(count) = val.column(\u0026#34;occurrences\u0026#34;).unwrap().i32().unwrap().get(0) { Ok(format!(\u0026#34;key: {} count: {}\u0026#34;,key,count)) } else { Ok(format!(\u0026#34;key: {} not found\u0026#34;,key)) } } ","description":"Stateful Services adaptor definitions","keywords":null,"summary":"Adaptors are language specific utilities and libraries that can be used in operator to simplify interaction with outside world or perform some common tasks. Following adaptors are available:\nHTTP Callout The http adaptor make it easy to make HTTP requests from your operator. It is based on the reqwest library. To use the http adaptor, you need to add adaptors section with http flag in your operator definition.\nFollowing snippet takes a sentence and translate english to spanish using external API.","title":"Adaptors","url":"http://localhost:1315/docs/stateful-services/adaptors/"},{"body":"Services can merge the data from multiple topics. The merge behavior is defined in the sources section of the service definition. The only requirement is that the the output of each source must be of the same type.\nWhen merging topics with different data types, use transformations to bring them to a common schema\nThe following example shows how to merge data from two topics:\n.. types: my-body: type: object properties: text: type: string topics: topic-1: schema: value: type: my-body converter: json topic-2: schema: value: type: string converter: raw services: my-service: sources: - id: topic-1 type: topic - id: topic-2 type: topic steps: - operator: map run: | fn to_common_schema(input: String) -\u0026gt; Result\u0026lt;MyBody, String\u0026gt; { Ok(MyBody{ text: input }) } steps: .. In this example, my-service merges data from topic-1 and topic-2. The topic-2 source has a transform step that converts the data to the common schema my-body. The topic-1 source does not need a transform step because the schema is already my-body.\nThe my-service can then process the merged data as if it were a single topic.\n","description":"Stateful Services merge feature.","keywords":null,"summary":"Services can merge the data from multiple topics. The merge behavior is defined in the sources section of the service definition. The only requirement is that the the output of each source must be of the same type.\nWhen merging topics with different data types, use transformations to bring them to a common schema\nThe following example shows how to merge data from two topics:\n.. types: my-body: type: object properties: text: type: string topics: topic-1: schema: value: type: my-body converter: json topic-2: schema: value: type: string converter: raw services: my-service: sources: - id: topic-1 type: topic - id: topic-2 type: topic steps: - operator: map run: | fn to_common_schema(input: String) -\u0026gt; Result\u0026lt;MyBody, String\u0026gt; { Ok(MyBody{ text: input }) } steps: .","title":"Merge","url":"http://localhost:1315/docs/stateful-services/merge/"},{"body":"Services can split the data into multiple topics. The split behavior is defined in the sinks section of the service definition. Similar to merge, the split operation can transform the data into schemas to match a target topic.\nThe following example shows how to split data into two topics:\n.. types: person: type: object properties: name: type: string age: type: i32 topics: topic-kid: schema: value: type: person topic-adult: schema: value: type: person services: my-service: sources: .. steps: .. sinks: - id: topic-kid type: topic steps: - operator: filter run: | fn is_kid(person: Person) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { Ok(person.age \u0026lt; 18) } - id: topic-adult type: topic steps: - operator: filter run: | fn is_adult(person: Person) -\u0026gt; Result\u0026lt;bool, String\u0026gt; { Ok(person.age \u0026gt;= 18) } In this example, my-service splits the data into topic-kid and topic-adult. The topic-kid sink has a transform step that filters the data to only include records where the age is less than 18. The topic-adult sink has a transform step that filters the data to only include records where the age is greater than or equal to 18.\n","description":"Stateful Services split feature.","keywords":null,"summary":"Services can split the data into multiple topics. The split behavior is defined in the sinks section of the service definition. Similar to merge, the split operation can transform the data into schemas to match a target topic.\nThe following example shows how to split data into two topics:\n.. types: person: type: object properties: name: type: string age: type: i32 topics: topic-kid: schema: value: type: person topic-adult: schema: value: type: person services: my-service: sources: .","title":"Split","url":"http://localhost:1315/docs/stateful-services/split/"},{"body":"Filter The simplest type of SmartModule is a filter, which can examine each record in a stream and decide whether to accept or reject it. All accepted records are delivered down the pipeline, and rejected records are discarded. SmartModule applied in consumers or sink connectors filter records after they are stored in a topic, and will not impact persistence - it simply means that records filtered out are not delivered to the consumer. However, SmartModule filters applied to source connectors discard packets before they are stored in the topic and should be used with care.\nMap SmartModule Maps are used to transform or edit each Record in a stream. We say that these SmartModules \u0026ldquo;map\u0026rdquo; each input record into a new output record by applying a function to the input data. This type of SmartModule may be used for many use-cases, such as:\nNarrowing large records into a smaller subset of important fields Scrubbing sensitive fields of data to be invisible to downstream consumers Computing rich, derived fields from simple raw data FilterMap SmartModule FilterMaps are used to both transform and potentially filter records from a stream at the same time. This can be useful for a number of scenarios including working with data with nullable fields, or working with subsets of event data. In these cases, FilterMap allows us discard irrelevant data - such as records with null fields or event types that we don\u0026rsquo;t care about - while also performing meaningful work with relevant data - such as reformatting fields we\u0026rsquo;ve extracted or events we\u0026rsquo;ve gathered.\nFilterMap functions work by returning an Option of a new record. To discard a record from the stream, return None. Otherwise, transform the record according to your needs and return it as Some(record).\nArrayMap SmartModule ArrayMaps are used to break apart Records into smaller pieces. This can be very useful for working with your data at a fine granularity. Often, each record in a Topic may actually represent many data points, but we\u0026rsquo;d like to be able to analyze and manipulate those data points independently. ArrayMap allows us to dig in and break apart these composite records into the smaller units of data that we want to work with.\nAggregate SmartModule Aggregates are functions that define how to combine each record in a stream with some accumulated value. In the functional programming world, this type of operation is also known as folding, since the function \u0026ldquo;folds\u0026rdquo; each new value into the accumulator.\n","description":"","keywords":null,"summary":"Filter The simplest type of SmartModule is a filter, which can examine each record in a stream and decide whether to accept or reject it. All accepted records are delivered down the pipeline, and rejected records are discarded. SmartModule applied in consumers or sink connectors filter records after they are stored in a topic, and will not impact persistence - it simply means that records filtered out are not delivered to the consumer.","title":"SmartModule Operators","url":"http://localhost:1315/docs/resources/smartmodule-operator-reference/"},{"body":"Fluvio is an implementation of the Event-Driven Architecture (EDA), and Stateful Services are an extension of the same paradigm. The following concepts are essential for understanding the architecture:\nEvents Event Streams \u0026amp; Partitions State Operators Windowing Service Chaining (DAG) WebAssembly Component Model Events An event registers an activity that occurred in the past - an immutable fact. Events can be represented in formats like JSON, Avro, Protobuf, etc. Events can be produced by services such as databases, microservices, sensors, IoT devices, or other physical devices. Events can also be produced by other event streams. Stateful Services use events as triggers that start the chain of operations and generate one or more results.\nEvents have the following properties:\nKey - the unique identity of an event. Value - the actual data of the event. Time - timestamp when the event was produced. Schema - defines the structure of the event. Time is a core system primitive utilized by the window processing operators to order and group events. Schema is also a core primitive that ensures the events comply with specific data formats to provide accurate data extraction for subsequent operations. Schema is designed to support multiple formats: JSON, Avro, Protobuf, etc.\nEvent Streams \u0026amp; Partitions An event stream is an asynchronous unbounded collection of events. Unbounded means never-ending; for example, a temperature sensor generates events continuously. Asynchronous means events arrive at any time rather than fetched periodically from a data store. Event streams are partitioned to process high volumes of data in parallel.\nStateful Services can apply a shared business logic in parallel across multiple partitions.\nState State is an \u0026ldquo;opinion derived from a fact.\u0026rdquo; In technical terms, states are aggregate objects computed on streams of events. For example, counting cars passing an intersection, calculating profits on financial transactions, detecting fraud, etc. States are persistent and can survive restarts, ensuring the results remain accurate and deterministic. States enable users to make decision on accumulated data.\nAt present, Stateful Services support the following state types:\nKey-value Windowing The key-value state performs aggregate operations on unbounded streams, where a specific key captures the value of a computation. Fluvio offsets management uses key-value to store the last value for a client key. The windowing state performs them on bounded streams as defined by the window configuration. Check out Windowing for additional information.\nOperators Operators are system operations implemented by Fluvio and are available to use in your applications. The operations range from basic operations, such as filter, map, and flat-map, to complex windowing operations, such as group-by, aggregates, etc.\nCheck out [Operators Section] for additional information.\nSmartmodules Smartmodules are custom defined functions that applies domain logic to operators. Your Smartmodules can be programmed in any language that compiles to WebAssembly - Rust, Python, JavaScript, Go, C++, C#, etc.\nSmartmodules can be chained in Services to perform complex stateful operations.\nWindow Processing Window processing, divides the data streams into bounded sets of records that are then processed in the window context.\nWindowing builds table aggregates for many use cases - counting, trend analysis, anomaly detection, data collection for dashboards and tables, materialized views, etc.\nFor additional information, check out the window operators section.\nService Chaining (DAG) Stateful Services use DAG (Directed Acyclic Graph) to chain multiple services. The DAG, in essence, represents the logical view of the data flow between operators.\nThe DAG definition is expressed in YAML format. The Stateful Services Development Kit (ssdk) converts this YAML file into a series of Compoment Model APIs.\nWe are also considering a programmatic interface to express the DAG; don\u0026rsquo;t hesitate to contact us if you are interested.\nWebAssembly Component Model Stateful Services uses WebAssembly Component Model to combine multiple WebAssembly libraries. The component model requires WIT, a language-agnostic IDL, to represent the data model and the functional interfaces.\nIn the next section, we will describe the components of the service file and the WASM code generated by SSDK.\nStateful Services Operations Stateful Services is an extension to Fluvio, a data streaming runtime that provides a robust communication layer for event streaming services.\nIn the context of Fluvio, Stateful Services leverages Fluvio topics as their data source and the target output. Fluvio topics serve as the interface with connectors or other services, making it easier to exchange data and facilitate communication between various cluster components.\nProvisioning and operating a Stateful Service requires the following system components:\nData Pipeline File to define the schema and operations for your Stateful Service.\nFluvio Cluster is the underlying infrastructure that supports data streaming and Stateful Streaming management.\nSSDK (Stateful Services Developer Kit) is a binary toolset designed to assist developers in building, testing, and deploying Stateful Services.\nIn the preview release, a Stateful Service is built and managed locally. When we announce General Availability, it can be provisioned in a cluster and shared via InfinyOn Hub. A Stateful Service published to Hub will be one click away from running in any InfinyOn cluster installation.\nServices File The services file defines how the service should interact with Fluvio topics, what data transformations or processing should be applied, and any other relevant configuration settings. This YAML file serves as a blueprint for the behavior of your services and it has the following sections:\nmeta, types, function, and states - define the interfaces operations - defined in each service As a general pattern, each operation reads from a topic, computes a function, and passes the result to another function or writes to a topic.\nThe hierarchy of a services file is as follows:\napiVersion: 0.1.0 meta: name: \u0026lt;services-name\u0026gt; namespace: \u0026lt;services-namespace\u0026gt; version: 0.1.0 types: \u0026lt;type-name\u0026gt; : type: { u32 | u64 | string | ... } ... functions: \u0026lt;function-name\u0026gt;: type: { filter | filter-map | ... } inputs: ... output: ... ... states: \u0026lt;state-name\u0026gt;: type: \u0026lt;type-name\u0026gt; interfaces: \u0026lt;interface-name\u0026gt;: ... ... ... operations: \u0026lt;operation-name\u0026gt;: state: \u0026lt;state-name\u0026gt;: - \u0026lt;interface-name\u0026gt; - ... ... source: \u0026lt;topic-name\u0026gt;: ... ... steps: - operator: { filter | filter-map | ... } uses: \u0026lt;function-name\u0026gt; ... ... sink: \u0026lt;topic-name\u0026gt;: ... ... ... The sections are defined as follows:\nmeta - stores the metadata definition of a Stateful Service, such as the name and version number. The name and version form the unique identifier for this service. types - defines the schema for the records and the states. functions - defines the interfaces of the functions used in operations. states - defines the type and the interfaces for each state object used in operations. operations - is a group of steps each operation has the following section: state - defines the state objects and the interfaces in current operators. source - defines the source topics in current operators. sink - defines the target topics in current operators. steps - defines each named operator and its corresponding function and parameters. Use the steps to sequence operations and internal topics to link them. You may think of an internal topic as an inter-service message bus.\nTopics are accessible to multiple Stateful Services, which makes the service composition flexible. You may define the complete data streaming application in one service file or decompose it in multiple files and use topics to link them.\nFor additional information, check out the Data Pipeline File section.\nFluvio Cluster The Fluvio Cluster is responsible for managing and packaging all the necessary resources required to run Stateful Services. This includes creating and managing topics that serve as the source and sink of data for your Stateful Service to interact with. The cluser also ensures the availability, scalability, and reliability of your data streams.\nCreate an account on InfinyOn Cloud, and provision a Fluvio cluster:\nDownload FVM which also installs fluvio CLI: %copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\nSign-up for a free InfinyOn Cloud account\nLogin from the CLI, and provision a Fluvio cluster:\n$ fluvio cloud login $ fluvio cloud cluster create Check out Getting Started section for additional informmation.\nSSDK (Stateful Services Developer Kit) SSDK is a binary toolset designed to assist developers in building, testing, and deploying Stateful Services. It ensures the services can be created and integrated efficiently into the Fluvio ecosystem. The SSDK binary is part of the fluvio client package.\nSSDK takes a services file and generates a project of one or more WASM components.\n$ ssdk generate The tool also offers a runtime environment, which starts the WASM components, links them with a local cluster, and enables an interactive shell for testing and troubleshooting.\n$ ssdk run \u0026gt;\u0026gt; For additional information, check out the SSDK section.\nNext Steps Getting Started ","description":"","keywords":null,"summary":"Fluvio is an implementation of the Event-Driven Architecture (EDA), and Stateful Services are an extension of the same paradigm. The following concepts are essential for understanding the architecture:\nEvents Event Streams \u0026amp; Partitions State Operators Windowing Service Chaining (DAG) WebAssembly Component Model Events An event registers an activity that occurred in the past - an immutable fact. Events can be represented in formats like JSON, Avro, Protobuf, etc. Events can be produced by services such as databases, microservices, sensors, IoT devices, or other physical devices.","title":"Stateful Services Concepts","url":"http://localhost:1315/docs/resources/stateful-services-concepts/"},{"body":"In Fluvio, data modification is done using SmartModules, user-defined functions converted to WebAssembly (WASM). Several SmartModules can form a Transformation Chain, working in sequence—each one modifies the data and passes it to the next. Both the sending (Producer) and receiving (Consumer) ends can use these chains; for the Producer, modification happens before the data is saved to the topic, while for the Consumer, it occurs before sending the data.\nTransformation Chaining is available for:\nFluvio Client Fluvio CLI SmartConnectors Setting It Up Each transformation in the chain is a SmartModule, paired with some specific instructions. Typically, you\u0026rsquo;d set this up in a yaml file.\nHere’s a basic example:\ntransforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: default spec: source: \u0026#34;http\u0026#34; In this example, there\u0026rsquo;s one transformation done by a SmartModule named infinyon/jolt@0.3.0. Ensure this SmartModule is available in your cluster:\nfluvio sm list SMARTMODULE SIZE infinyon/jolt@0.3.0 608.4 KB The section under with in the config is the special note to the SmartModule on how to modify the data. In this case, it receives this bit of info:\n\u0026#34;spec\u0026#34; : \u0026#34;[{\\\u0026#34;operation\\\u0026#34;:\\\u0026#34;default\\\u0026#34;,\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;source\\\u0026#34;:\\\u0026#34;http\\\u0026#34;}}]\u0026#34; Lining Up More SmartModules You can arrange multiple SmartModules to work in sequence. The output from one SmartModule is used as the input for the next, so their order is important.\ntransforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: fact: \u0026#34;animal.fact\u0026#34; length: \u0026#34;length\u0026#34; - uses: infinyon/regex-filter@0.1.0 with: regex: \u0026#34;[Cc]at\u0026#34; In this setup, the jolt transformation takes place first, then its output is used as the input for regex-filter.\n","description":"","keywords":null,"summary":"In Fluvio, data modification is done using SmartModules, user-defined functions converted to WebAssembly (WASM). Several SmartModules can form a Transformation Chain, working in sequence—each one modifies the data and passes it to the next. Both the sending (Producer) and receiving (Consumer) ends can use these chains; for the Producer, modification happens before the data is saved to the topic, while for the Consumer, it occurs before sending the data.\nTransformation Chaining is available for:","title":"Transformation Chaining","url":"http://localhost:1315/docs/resources/transformation-chaining/"},{"body":"The fluvio cloud cluster family of commands is used to create, delete, and troubleshoot Fluvio clusters in cloud.\n%copy first-line%\n$ fluvio cloud cluster -h $ fluvio cloud cluster View Cluster information Usage: fluvio cloud cluster \u0026lt;COMMAND\u0026gt; Commands: create Create a new Fluvio cluster delete Delete an existing Fluvio cluster list List all Fluvio clusters sync Sync Fluvio cluster profile usage Print cluster usage stats fluvio cloud cluster create This command is used to provision a new cluster.\n%copy first-line%\n$ fluvio cloud cluster create -h $ fluvio cloud cluster create Create a new Fluvio cluster Usage: fluvio cloud cluster create \u0026lt;NAME\u0026gt; [OPTIONS] Options: --profile \u0026lt;PROFILE\u0026gt; The name of the Profile to save --version \u0026lt;VERSION\u0026gt; Fix the Fluvio version of this cluster (not recommended) --region \u0026lt;REGION\u0026gt; The ID of the region in which to create this cluster (beta) Choosing a non-default region with --region is currently in private beta and not available to the public.\nSpecifying --version fixes the Fluvio version and prevents the cluster from being auto-upgraded. This is for installing experimental releases and not generally recommended.\nfluvio cloud cluster sync This command synchronized the cluster connection info to the Fluvio config on the machine\n%copy first-line%\n$ fluvio cloud cluster sync -h $ fluvio cloud cluster delete Delete an existing Fluvio cluster USAGE: fluvio cloud cluster delete \u0026lt;NAME\u0026gt; [OPTIONS] Options: --profile \u0026lt;PROFILE\u0026gt; The name of the Profile to save Example usage:\n%copy first-line%\n$ fluvio cloud cluster delete my-cluster fluvio cloud cluster list Command to show the fluvio clusters in Cloud associated with current user.\n%copy first-line%\n$ fluvio cloud cluster list -h $ fluvio cloud cluster list List all Fluvio clusters USAGE: fluvio-cloud cluster list Example usage:\n%copy first-line%\n$ fluvio cloud cluster list ID ACTIVE STATE VERSION SPU_COUNT my-cluster true Installed 0.10.0 1 fluvio cloud cluster delete This command deletes the specified cluster\n%copy first-line%\n$ fluvio cloud cluster delete -h $ fluvio cloud cluster delete Delete an existing Fluvio cluster USAGE: fluvio-cloud cluster delete \u0026lt;CLUSTER\u0026gt; Example usage:\n%copy first-line%\n$ fluvio cloud cluster delete my-cluster ","description":"CLI commands for cluster operations.","keywords":null,"summary":"The fluvio cloud cluster family of commands is used to create, delete, and troubleshoot Fluvio clusters in cloud.\n%copy first-line%\n$ fluvio cloud cluster -h $ fluvio cloud cluster View Cluster information Usage: fluvio cloud cluster \u0026lt;COMMAND\u0026gt; Commands: create Create a new Fluvio cluster delete Delete an existing Fluvio cluster list List all Fluvio clusters sync Sync Fluvio cluster profile usage Print cluster usage stats fluvio cloud cluster create This command is used to provision a new cluster.","title":"Cluster","url":"http://localhost:1315/docs/cli/cluster/"},{"body":"Environment variables can be used to pass configuration information to the operators. The environment variables are defined in the operator definition and can be accessed in the operator code.\nThey are useful for passing configuration information such as API keys, database connection strings, and other configuration information.\nDefining Environment Variables You can define environment variables in the run command similar to Docker. The -e flag is used to define environment variables.\nssdk run --e VAR1=value1 -e VAR2=value2 Accessing Environment Variables in operator code Once the environment variables are defined thru CLI, they are available in every operator user code. In the Rust code, you can access the environment variables using thestd::env module.\nFollow code snippet illustration filter that reject a word startes with prefix defined in environment variable. %copy%\n- operator: filter-map run: | fn filter(input: word) -\u0026gt; Result\u0026lt;Option\u0026lt;String\u0026gt;, String\u0026gt; { let block_word = std::env::var(\u0026#34;BLOCKED_PREFIX\u0026#34;).map_err(|e| e.to_string())?; if input.starts_with(\u0026amp;block_word) { Ok(None) } else { Ok(Some(input)) } } They are also useful for passing configuration information such as API keys. For example, following show how to encode bearer token in environment variable and access it in the operator code.\n%copy% ```yaml - operator: map adaptors: - http run: | fn filter(input: sentence) -\u0026gt; Result\u0026lt;Option\u0026lt;String\u0026gt;, String\u0026gt; { let auth_key = std::env::var(\u0026#34;MY_API_KEY\u0026#34;).map_err(|e| e.to_string())?; let auth_bearer = format!(\u0026#34;Bearer {}\u0026#34;, auth_key); let request = ssdk_http::http::Request::builder() .uri(\u0026#34;https://myapi.com/api/v1/sentences\u0026#34;) .method(\u0026#34;POST\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .header(\u0026#34;Authorization\u0026#34;, auth_bearer) .body(body) .map_err(|e| e.to_string())?; let response = ssdk_http::blocking::send(request).map_err(|e| e.to_string())?; let body: Vec\u0026lt;u8\u0026gt; = response.into_body(); Ok(String::from_utf8(body).map_err(|e| e.to_string())?); } ","description":"Stateful Services environment variables support","keywords":null,"summary":"Environment variables can be used to pass configuration information to the operators. The environment variables are defined in the operator definition and can be accessed in the operator code.\nThey are useful for passing configuration information such as API keys, database connection strings, and other configuration information.\nDefining Environment Variables You can define environment variables in the run command similar to Docker. The -e flag is used to define environment variables.","title":"Environment Variables","url":"http://localhost:1315/docs/stateful-services/environment-variables/"},{"body":"","description":"Building a usage billing pattern using Stateful Services.","keywords":null,"summary":"","title":"Example: Usage Billing","url":"http://localhost:1315/docs/stateful-services/billing-tutorial/"},{"body":"If you\u0026rsquo;re using both Zapier and InfinyOn Cloud, integrating the two can supercharge your workflows. Zapier\u0026rsquo;s automation prowess can link up with InfinyOn Cloud\u0026rsquo;s robust data streaming, allowing you to move and transform data and trigger actions seamlessly between them. This can not only save you time but also unlock new possibilities for your real-time data.\nTo follow along in this guide you need:\nAccess to Zapier Premium apps InfinyOn Cloud cluster Google Forms trigger to Zapier Google Sheets trigger to Zapier Linking Zapier to InfinyOn Cloud This section covers sending events from Zapier Zaps to InfinyOn Cloud Webhooks.\nPre-requisites An InfinyOn Cloud account fluvio CLI installed connected to Cloud account with fluvio cloud login A Zapier account with access to Zapier Webhooks A Zap trigger. This example will use new survey responses from Google Forms On InfinyOn cloud cluster First step is to create a webhook in Infinyon Cloud.\nCopy the following example webhook config file and save as example-webhook.yaml\n%copy%\n# example-webhook.yaml meta: name: from-zapier topic: zapier-events # optional webhook: outputParts: body outputType: json Then run fluvio cloud webhook create to create the webhook\n%copy first-line%\n$ fluvio cloud webhook create --config example-webhook.yaml Webhook \u0026#34;zapier-events\u0026#34; created with url: https://infinyon.cloud/webhooks/v1/[random string] We\u0026rsquo;ll use the url for the next step in Zapier\nIn Zapier dashboard First thing is to create or modify an existing Zap. - We\u0026rsquo;ve created a new Zap that triggers whenever a new response to our Google Form survey arrives We won\u0026rsquo;t cover the configuration of Google Form trigger further in this example Configuring Webhooks step Add a step and choose the Webhooks by Zapier action Under App \u0026amp; Event, select a POST event and click continue Under Action, for the URL field, paste the URL for your InfinyOn Cloud webhook\nMap the answers from survey with keys to form into json to InfinyOn Cloud and click the continue button\nTest the Zapier to InfinyOn Cloud workflow Test the event gets sent to InfinyOn Cloud In a terminal create a CLI consumer for the webhook\u0026rsquo;s with the following command:\n%copy first-line%\n$ fluvio consume zapier-events Consuming records from \u0026#39;zapier-events\u0026#39; In our example, we have two fields Send a new Google Form response Then click the Test Step/Retest button test triggering the event with our survey response from the previous step.\nIn our consumer terminal, we get this example output:\n$ fluvio consume zapier-events Consuming records from \u0026#39;zapier-events\u0026#39; {\u0026#34;name\u0026#34;: \u0026#34;Stacey Fakename\u0026#34;, \u0026#34;feedback\u0026#34;: \u0026#34;Yes\u0026#34;} Then you can click Publish to save this Zap. It is ready for production! It is ready to send data to InfinyOn Cloud.\nThe rest of this guide will cover data flow in the opposite direction.\nLinking InfinyOn Cloud to Zapier This covers triggering Zapier Zaps with InfinyOn Cloud\u0026rsquo;s outbound HTTP Connector\nPre-requisites An InfinyOn Cloud account fluvio CLI installed connected to Cloud account with fluvio cloud login Zapier account with access to Zapier Webhooksdd An action to save event from a trigger (We\u0026rsquo;re using Google Sheets) In Zapier dashboard In Zapier, create a new Zap that is triggered by webhook. Copy the Zapier url for the next step For our example action we will add a row per InfinyOn Cloud event into an existing Google Sheet with 2 columns In the Test section, copy the url. We need this webhook url to configure InfinyOn Cloud as a data source for this Zap.\nWe\u0026rsquo;ll pause on configuring this Zap for a moment, and be back after the next section.\nOn InfinyOn Cloud cluster In InfinyOn Cloud, we need to create an outbound HTTP connector. Replace the url from the previous step for the endpoint field. %copy%\n# zapier-connector.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: zapier type: http-sink topic: cloud-event http: method: POST endpoint: https://hooks.zapier.com/hooks/catch/########/xxxxxxx/ Then create the connector with the following command\n%copy first-line%\n$ fluvio cloud connector create -c zapier-connector.yaml Data that is sent to the cloud-event topic will be sent as HTTP POST to the Zapier workflow.\nIn the next section we\u0026rsquo;ll test the end to end. But first we\u0026rsquo;ll send some data to test the event triggers the Zapier Zap\nCreate a CLI Producer to the connector\u0026rsquo;s cloud-event topic %copy first-line%\n$ fluvio produce cloud-event \u0026gt; {\u0026#34;full_name\u0026#34;: \u0026#34;Stacie Fakename\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;Hello from Cloud\u0026#34;} Test the InfinyOn Cloud to Zapier workflow Back to Zapier to edit the webhook Zap. Click Find New Records to see the data we sent from the previous step. Click the Continue with the selected record button. Create a new step. Select Google Sheets We want to create a new row in Sheets. Select Create Spreadsheet Row. In the Account section, you\u0026rsquo;ll link your Google account to link to Sheets. This step won\u0026rsquo;t be covered in detail\nIn the Action section, select a spreadsheet with column headers, and select Refresh fields to populate the section with inputs. Map the webhook fields from the trigger to fields in the spreadsheet and click continue. In the Test section, click Test/Retest step to create a row in the spreadsheet with the webhook data. Confirm in the spreadsheet that the data is mapped into the columns.\nThen you can click Publish to save this Zap to put it into production. It is ready to trigger from InfinyOn Cloud events.\nThis is the end of the guide. You should now have bi-directional data flow between Zapier and InfinyOn Cloud.\n","description":"This guide describes how to send events back and forth between InfinyOn Cloud and Zapier","keywords":null,"summary":"If you\u0026rsquo;re using both Zapier and InfinyOn Cloud, integrating the two can supercharge your workflows. Zapier\u0026rsquo;s automation prowess can link up with InfinyOn Cloud\u0026rsquo;s robust data streaming, allowing you to move and transform data and trigger actions seamlessly between them. This can not only save you time but also unlock new possibilities for your real-time data.\nTo follow along in this guide you need:\nAccess to Zapier Premium apps InfinyOn Cloud cluster Google Forms trigger to Zapier Google Sheets trigger to Zapier Linking Zapier to InfinyOn Cloud This section covers sending events from Zapier Zaps to InfinyOn Cloud Webhooks.","title":"How to Link Zapier and InfinyOn Cloud","url":"http://localhost:1315/docs/guides/zapier-cloud/"},{"body":"The fluvio cloud webhook family of commands is used to create, delete, and troubleshoot Webhooks in cloud.\n%copy first-line%\n$ fluvio cloud webhook -h Manage Webhooks Usage: fluvio-cloud webhook \u0026lt;COMMAND\u0026gt; Commands: create Create webhook delete Delete webhook list List webhooks update Update webhook logs View webhook connector logs fluvio cloud webhook create This command is used to provision a new cluster.\n%copy first-line%\n$ fluvio cloud webhook create -h Create webhook Usage: fluvio-cloud webhook create [OPTIONS] [NAME] Arguments: [NAME] Name of webhook Options: --topic \u0026lt;TOPIC\u0026gt; --config \u0026lt;CONFIG\u0026gt; Webhook config -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud webhook create --config webhook-config.yaml Webhook \u0026#34;my-webhook\u0026#34; created with url: https://infinyon.cloud/webhooks/v1/\u0026lt;random-string\u0026gt; See the Webhook config reference for more on config files.\nfluvio cloud webhook list Command to show the fluvio clusters in Cloud associated with current user.\n%copy first-line%\n$ fluvio cloud webhook list -h List webhooks Usage: fluvio-cloud webhook list Options: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud webhook list NAME TOPIC URL my-webhook my-topic https://infinyon.cloud/webhooks/v1/\u0026lt;random-string\u0026gt; fluvio cloud webhook delete This command deletes current cluster of current user.\n%copy first-line%\n$ fluvio cloud webhook delete -h Delete webhook Usage: fluvio-cloud webhook delete \u0026lt;NAME\u0026gt; Arguments: \u0026lt;NAME\u0026gt; Name of webhook Options: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud webhook delete my-webhook Webhook \u0026#34;my-webhook\u0026#34; deleted fluvio cloud webhook update %copy first-line%\n$ fluvio cloud webhook update -h Update webhook Usage: fluvio-cloud webhook update --config \u0026lt;CONFIG\u0026gt; Options: --config \u0026lt;CONFIG\u0026gt; Webhook config -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud webhook update --config webhook-config.yaml See the Webhook config reference for more on config files.\nfluvio cloud webhook logs %copy first-line%\n$ fluvio cloud webhook logs -h View webhook connector logs Usage: fluvio-cloud webhook logs \u0026lt;NAME\u0026gt; Arguments: \u0026lt;NAME\u0026gt; Name of webhook Options: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud webhook logs my-webhook [Log output] References Webhook Configuration File Webhook Basics ","description":"CLI commands for webhook operations.","keywords":null,"summary":"The fluvio cloud webhook family of commands is used to create, delete, and troubleshoot Webhooks in cloud.\n%copy first-line%\n$ fluvio cloud webhook -h Manage Webhooks Usage: fluvio-cloud webhook \u0026lt;COMMAND\u0026gt; Commands: create Create webhook delete Delete webhook list List webhooks update Update webhook logs View webhook connector logs fluvio cloud webhook create This command is used to provision a new cluster.\n%copy first-line%\n$ fluvio cloud webhook create -h Create webhook Usage: fluvio-cloud webhook create [OPTIONS] [NAME] Arguments: [NAME] Name of webhook Options: --topic \u0026lt;TOPIC\u0026gt; --config \u0026lt;CONFIG\u0026gt; Webhook config -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:","title":"Webhook","url":"http://localhost:1315/docs/cli/webhook/"},{"body":"Webhook config template # example-webhook-template.yaml meta: name: my-webhook topic: my-topic # optional transforms: - uses: smartmodule_name with: param_name: param_value # optional webhook: outputParts: [body | full (default)] outputType: [text | json (default)] Config options Meta name - The name of your webhook topic - The name of the topic you want events to be stored. It will be automatically created if it doesn\u0026rsquo;t exist. Transforms Webhook connectors support transforms. Records can be modified before they are sent to the topic. The transforms section is a list of transform objects. Each transform object has an uses and a with section.\nuses is the reference to the SmartModule used in the transform. with is the configuration for the transform The section is different for each transform See the connectors reference documentation for available configuration options Webhook The output record from the webhook request is configurable\noutputParts options:\nfull - Return the headers and body of the request (Default) body - Only return the body of the request outputType options:\njson- Output is parsed into json (Default) text - Output is plaintext References Webhook Basics ","description":"Reference for Webhook configs","keywords":null,"summary":"Webhook config template # example-webhook-template.yaml meta: name: my-webhook topic: my-topic # optional transforms: - uses: smartmodule_name with: param_name: param_value # optional webhook: outputParts: [body | full (default)] outputType: [text | json (default)] Config options Meta name - The name of your webhook topic - The name of the topic you want events to be stored. It will be automatically created if it doesn\u0026rsquo;t exist. Transforms Webhook connectors support transforms. Records can be modified before they are sent to the topic.","title":"Webhook config File","url":"http://localhost:1315/docs/cli/webhook-config/"},{"body":"The fluvio cloud connector subcommands are used to manage Connectors in InfinyOn Cloud.\n%copy first-line%\n$ fluvio cloud connector -h fluvio-cloud-connector View Fluvio Connector information USAGE: fluvio-cloud connector \u0026lt;SUBCOMMAND\u0026gt; OPTIONS: -h, --help Print help information SUBCOMMANDS: config Show the connector configuration details create Create a new Connector delete Delete a Connector help Print this message or the help of the given subcommand(s) list List all Connectors logs View connector logs update Update and restart a Connector -\u0026gt; For more info about using connectors, see the Connectors page. The available connector types are listed under the Inbound and Outbound sections.\nfluvio cloud connector create This command is used to provision a new connector.\n%copy first-line%\n$ fluvio cloud connector create -h fluvio cloud connector create Create a new Connector Usage: fluvio cloud connector create [OPTIONS] --config \u0026lt;CONFIG\u0026gt; Options: --config \u0026lt;CONFIG\u0026gt; Path to config file --log-level \u0026lt;LOG_LEVEL\u0026gt; Sets the log level, one of (error, warn, info, debug, trace) -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster -h, --help Print help To create a connector, you need to create a YAML-based connector config file.\nFor more about the connector config file, see the Cloud connectors page.\nWhen running fluvio cloud connector create, pass the path to this file using the --config option.\nExample usage:\n%copy first-line%\n$ fluvio cloud connector create --config=./cats.yaml connector \u0026#34;cat-facts\u0026#34; (http-source) created fluvio cloud connector config Command to show the configuration file used to create this connector.\n%copy first-line%\n$ fluvio cloud connector config -h fluvio-cloud-connector-config Show the connector configuration details USAGE: fluvio-cloud connector config [OPTIONS] \u0026lt;NAME\u0026gt; ARGS: \u0026lt;NAME\u0026gt; Name of connector OPTIONS: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster -h, --help Print help information Example usage:\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s fluvio cloud connector list This command show you all the existing Connectors in your cluster.\n%copy first-line%\n$ fluvio cloud connector list -h fluvio-cloud-connector-list List all Connectors USAGE: fluvio-cloud connector list [OPTIONS] OPTIONS: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster fluvio cloud connector update Command to update and restart an existing connector.\n%copy first-line%\n$ fluvio cloud connector update -h fluvio cloud connector update Update and restart a Connector Usage: fluvio-cloud connector update [OPTIONS] --config \u0026lt;CONFIG\u0026gt; Options: -c, --config \u0026lt;CONFIG\u0026gt; Name of connector --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster --log-level \u0026lt;LOG_LEVEL\u0026gt; Sets the log level [default: LogLevel::default()] Example usage:\n%copy first-line%\n$ fluvio cloud connector update --config=./cats.yaml connector \u0026#34;cat-facts\u0026#34; (http-source) updated fluvio cloud connector logs Command to view the logs written by the connector.\n%copy first-line%\n$ fluvio cloud connector logs -h fluvio-cloud-connector-logs View connector logs USAGE: fluvio-cloud connector logs [OPTIONS] \u0026lt;NAME\u0026gt; ARGS: \u0026lt;NAME\u0026gt; Name of connector OPTIONS: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud connector logs cat-facts 2022-10-21T14:55:13.508989Z INFO http_source: Starting HTTP source connector connector_version=\u0026#34;0.4.1\u0026#34; git_hash=\u0026#34;0ad913c5ceb732881fd753874e5082777bbed91e\u0026#34; 2022-10-21T14:55:13.509096Z INFO http_source: interval=10s method=GET topic=cat-facts output_parts=body output_type=text endpoint=https://catfact.ninja/fact 2022-10-21T14:55:13.510284Z INFO fluvio::config::tls: Using verified TLS with certificates from paths domain=\u0026#34;broad-union-b685e7fda03fefb3d5221d0a3b9c64c7.c.infinyon.cloud\u0026#34; 2022-10-21T14:55:13.515459Z INFO fluvio::fluvio: Connecting to Fluvio cluster fluvio_crate_version=\u0026#34;0.14.0\u0026#34; fluvio_git_hash=\u0026#34;e96d8e2738ee39ddbb64fea37134f119f97e25bf\u0026#34; 2022-10-21T14:55:13.574584Z INFO connect: fluvio::sockets: connect to socket add=fluvio-sc-public:9003 ... Configure Logging Levels By default connectors will log using the info level. You can configure the log level for connectors running in the cloud using the --log-level option.\nThe --log-level option is available for both, fluvio cloud connector \u0026lt;create | update\u0026gt;.\nAny of the following levels can be used:\nerror warn info debug trace fluvio cloud connector delete This command deletes an existing Connector.\n%copy first-line%\n$ fluvio cloud connector delete -h fluvio-cloud-connector-delete Delete a Connector USAGE: fluvio-cloud connector delete [OPTIONS] \u0026lt;name\u0026gt;... ARGS: \u0026lt;name\u0026gt;... One or more name(s) of the connector(s) to be deleted OPTIONS: -c, --cluster \u0026lt;CLUSTER\u0026gt; Name of cluster Example usage:\n%copy first-line%\n$ fluvio cloud connector delete cat-facts connector \u0026#34;cat-facts\u0026#34; deleted ","description":"CLI commands for connector operations.","keywords":null,"summary":"The fluvio cloud connector subcommands are used to manage Connectors in InfinyOn Cloud.\n%copy first-line%\n$ fluvio cloud connector -h fluvio-cloud-connector View Fluvio Connector information USAGE: fluvio-cloud connector \u0026lt;SUBCOMMAND\u0026gt; OPTIONS: -h, --help Print help information SUBCOMMANDS: config Show the connector configuration details create Create a new Connector delete Delete a Connector help Print this message or the help of the given subcommand(s) list List all Connectors logs View connector logs update Update and restart a Connector -\u0026gt; For more info about using connectors, see the Connectors page.","title":"Connectors","url":"http://localhost:1315/docs/cli/connector/"},{"body":"This is the template YAML connector file. To make it useful, it needs to be populated. In the next section we will go over the different sections of the connector configuration file.\n%copy%\n# connector.yaml # Version of the schema of the connector config # Valid value is `0.1.0` apiVersion: 0.1.0 meta: name: version: type: topic: # optional producer: # optional linger: # optional batch-size: # optional compression: # optional consumer: # optional partition: # optional max_bytes: # optional secrets: - name: secret_1 # optional transforms: - uses: smartmodule_name with: param_name: param_value # Type specific configs # key depends on connector # \u0026lt;custom_key\u0026gt;: # foo: bar # # eg. # http: # endpoint: https://example.com Connector apiVersion configuration The apiVersion is the version of the connector API that the connector uses to parse the configuration file. The current only accepted version is 0.1.0.\nConnector meta configuration The meta section contains the metadata for the connector:\nThe name is the name of the connector. e.g. my-connector. The type is the type of the connector. e.g. http-source, http-sink, mqtt-source. See the Connectors section in the navigation for the full list of connectors supported. The version is the version of the connector. e.g. 0.2.0. The topic is the topic that the connector will connect to. e.g. my-topic. The topic will be created automatically if it does not exist. The secrets(optional) is a list of secrets that the connector will use. This accepts a list of objects with the key name. See the secrets section for more information. The producer(optional) is the producer configuration for the connector. Currently, this is only used for source/inbound connectors. The current supported configurations are linger, compression and batch_size. All configurations are optional. See examples to a list of valid values for each configuration. The consumer(optional) is the consumer configuration for the connector. Currently, this is only used for sink/outbound connectors. The current supported configurations are partition and max_bytes. Both configurations are optional. See examples to a list of valid values for each configuration. An example with all the keys filled for a http-source connector:\n%copy%\napiVersion: 0.1.0 meta: name: my-http-source-connector type: http-source version: 0.2.0 topic: my-topic producer: linger: 1ms batch_size: \u0026#34;44.0 MB\u0026#34; # possible values: `none`, `gzip`, `snappy` and `lz4` compression: gzip secrets: - name: MY_SECRET An example with all the keys filled for a http-sink connector:\n%copy%\napiVersion: 0.1.0 meta: name: my-http-sink-connector type: http-sink version: 0.1.0 topic: my-topic consumer: max_bytes: 3 MB partition: 0 secrets: - name: MY_SECRET Connector transforms configuration Connectors support transforms. Records can be modified before they are sent to the topic. The transforms section is a list of transform objects. Each transform object has an uses and a with section.\nuses is the reference to the SmartModule used in the transform. with is the configuration for the transform The section is different for each transform See the connectors reference documentation for available configuration options See the Transformations section for more information.\n","description":"","keywords":null,"summary":"This is the template YAML connector file. To make it useful, it needs to be populated. In the next section we will go over the different sections of the connector configuration file.\n%copy%\n# connector.yaml # Version of the schema of the connector config # Valid value is `0.1.0` apiVersion: 0.1.0 meta: name: version: type: topic: # optional producer: # optional linger: # optional batch-size: # optional compression: # optional consumer: # optional partition: # optional max_bytes: # optional secrets: - name: secret_1 # optional transforms: - uses: smartmodule_name with: param_name: param_value # Type specific configs # key depends on connector # \u0026lt;custom_key\u0026gt;: # foo: bar # # eg.","title":"Connector config file","url":"http://localhost:1315/docs/cli/connector-config/"},{"body":"Fluvio cloud secrets are set via the cli. Each secret is a named value with all secrets sharing a namespace per account. Connector configuration files can refer to secrets by name, and the cloud connector infrastructure will provision the connector with the named secrets.\nDue to security concerns, listing actual secret values or downloading them after they have been set is not allowed. However, a listing of secret names as well as what date they were last set is accessible.\nfluvio cloud secret subcommands The secrets cli is an added subcommand fluvio cloud as \u0026lsquo;fluvio cloud secret\u0026rsquo;.\nActions possible with a fluvio cloud secret are:\nset delete list fluvio cloud secret set \u0026lt;NAME\u0026gt; \u0026lt;VALUE\u0026gt; fluvio cloud secret list fluvio cloud secret delete \u0026lt;NAME\u0026gt; fluvio cloud secret set Setting a scret of \u0026lt;NAME\u0026gt; will allow it to be refrenced by that name in connector configuration parameters that can use secret references.\n%copy first-line%\nfluvio cloud secret set \u0026lt;NAME\u0026gt; \u0026lt;VALUE\u0026gt; All secrets are in a shared connector namespace, but a specific connector is only given access to secrects named in the configuration file of the connector.\nfluvio cloud secret list fluvio cloud secret list will list only the secret names and their last update time. Once a secret has been set into fluvio cloud, it is stored so only referencing connectors may access the secret. There is no way to retreive the secret value from fluvio cloud.\n%copy first-line%\n$ fluvio cloud secret list SecretNames LastUpdate CAT_FACTS_CLIENT_ID 12-10-2022 1:07pm CAT_FACTS_SECRET 01-02-2023 12:01am fluvio cloud secret delete This will delete the named secret.\n%copy first-line%\nfluvio cloud secret delete \u0026lt;NAME\u0026gt; Connector config file references The connector config files can reference cloud secrets by NAME. They need to be referenced on meta section of connector config. And then we can use the secret name in the connector configuration parameters. The secret can be used in the configuration as ${{ secrets.\u0026lt;NAME\u0026gt; }}.\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: my-connector type: package-name topic: a-topic secrets: - name: CAT_FACTS_CLIENT_ID - name: CAT_FACTS_SECRET # named section for custom config parameters, usually a short name like \u0026#34;http\u0026#34;, or \u0026#34;mqtt\u0026#34; \u0026lt;CUSTOM\u0026gt;: param_client_id: ${{ secrets.CAT_FACTS_CLIENT_ID }} param_client_secret: ${{ secrets.CAT_FACTS_SECRET }} Example An example of a connector that can use secret parameters, the http connector might be setup and configured as follows.\nSetup a secret %copy first-line%\n$ fluvio cloud secret set AUTH_HEADER \u0026#34;1234abcd\u0026#34; Write a connector config http-config-with-secret.yaml %copy%\n$ cat \u0026lt;\u0026lt; END_CONFIG \u0026gt; http-config-with-secret.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts-secret secrets: - name: AUTH_HEADER http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s headers: - \u0026#34;Authorization: bearer ${{ secrets.AUTH_HEADER }}\u0026#34; END_CONFIG Run the connector %copy first-line%\n$ fluvio cloud connector --config http-config-with-secret.yaml This same configuration file is compatible with both fluvio cloud connectors and cdk locally run connectors. The cloud connectors are provisioned via the fluvio cloud secret ... set of commands, while the cdk secrets are provided locally.\n","description":"CLI commands for secrets management.","keywords":null,"summary":"Fluvio cloud secrets are set via the cli. Each secret is a named value with all secrets sharing a namespace per account. Connector configuration files can refer to secrets by name, and the cloud connector infrastructure will provision the connector with the named secrets.\nDue to security concerns, listing actual secret values or downloading them after they have been set is not allowed. However, a listing of secret names as well as what date they were last set is accessible.","title":"Secrets","url":"http://localhost:1315/docs/cli/secret/"},{"body":"Query the CPU and memory usage of your SPUs.\nfluvio cloud usage Example usage:\n%copy first-line%\n$ fluvio cloud usage SPU CPU Memory main-0 1066037n 3168Ki Note: CPU usage is expressed in nanocores. 1 nanocore is equal to 1 billionth of 1 core. ","description":"CLI commands to check your cluster usage status.","keywords":null,"summary":"Query the CPU and memory usage of your SPUs.\nfluvio cloud usage Example usage:\n%copy first-line%\n$ fluvio cloud usage SPU CPU Memory main-0 1066037n 3168Ki Note: CPU usage is expressed in nanocores. 1 nanocore is equal to 1 billionth of 1 core. ","title":"Usage","url":"http://localhost:1315/docs/cli/usage/"},{"body":"With this guide, you can send events from InfinyOn Cloud to Amplitude. Connecting your services to this pipeline is simpler than microservices running around\nTo follow along in this guide you need:\nThe Amplitude API key from your account. InfinyOn Cloud cluster and Fluvio CLI. Setup Create secret You can follow Amplitude\u0026rsquo;s instructions for how to collect your api token so you can create a secret the connector can use when building the json request for Amplitude.\nThe Amplitude upload requests requires an api token, and all events for the production environment use the same api token. We\u0026rsquo;ll transform the service event to include this value in the next step.\n%copy first-line%\n$ fluvio cloud secret set AMPLITUDE_API_TOKEN \u0026lt;api-token\u0026gt; Secret \u0026#34;AMPLITUDE_API_TOKEN\u0026#34; set successfully Create connector Example event:\n%copy%\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2023-09-06T12:02:29.658014825Z\u0026#34;, \u0026#34;event\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;ServiceAbcExampleEvent\u0026#34;, \u0026#34;time\u0026#34;: 1696629748241, \u0026#34;app_version\u0026#34;: \u0026#34;c738ca3\u0026#34;, \u0026#34;event_id\u0026#34;: 5, \u0026#34;session_id\u0026#34;: 9876645851321, \u0026#34;insert_id\u0026#34;: \u0026#34;d768b1b3-1055-4db8-b214-619b5a321ef5\u0026#34; } } In this example, each service sends a json object to a topic containing a timestamp for when the event occurred and an Amplitude event object.\nBefore sending to Amplitude with an HTTP outbound connector, we\u0026rsquo;ll transform the original payload into the Amplitude upload request json.\nThe transform consists of:\nRemoving the timestamp key Adding the api_key key with the value from our AMPLITUDE_API_TOKEN secret Shift the contents of the event key into an array with the key events Example connector config w/ transforms\n%copy%\n# amplitude-connector.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: amplitude-connector type: http-sink topic: service-events secrets: - name: AMPLITUDE_API_TOKEN http: endpoint: \u0026#34;https://api2.amplitude.com/2/httpapi\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: remove spec: timestamp: \u0026#34;\u0026#34; - operation: shift spec: \u0026#34;event\u0026#34;: \u0026#34;events[0]\u0026#34; - operation: default spec: api_key: \u0026#34;${{ secrets.AMPLITUDE_API_TOKEN }}\u0026#34; Save the config and run the following command to create the connector.\n%copy first-line%\n$ fluvio cloud connector create -c amplitude-connector.yaml connector \u0026#34;amplitude-connector\u0026#34; (http-sink) created Send a test event to topic The following command will send an example event to the topic our connector is watching.\n%copy first-line%\n$ echo \u0026#39;{\u0026#34;timestamp\u0026#34;:\u0026#34;2023-09-06T12:02:29.658014825Z\u0026#34;,\u0026#34;event\u0026#34;:{\u0026#34;user_id\u0026#34;:\u0026#34;user@example.com\u0026#34;,\u0026#34;event_type\u0026#34;:\u0026#34;ServiceAbcExampleEvent\u0026#34;}}\u0026#39; | fluvio produce service-events Look at Amplitude for your event In the Amplitude dashboard, you should be able to verify the test event arrive under User Look-up\nThis is the end of the guide. Once you instrument your services, you should be able to quickly send analytics events to Amplitude from InfinyOn Cloud.\n","description":"Data pipeline that collects events from multiple services and sends them to Amplitude.","keywords":null,"summary":"With this guide, you can send events from InfinyOn Cloud to Amplitude. Connecting your services to this pipeline is simpler than microservices running around\nTo follow along in this guide you need:\nThe Amplitude API key from your account. InfinyOn Cloud cluster and Fluvio CLI. Setup Create secret You can follow Amplitude\u0026rsquo;s instructions for how to collect your api token so you can create a secret the connector can use when building the json request for Amplitude.","title":"Amplitude Analytics","url":"http://localhost:1315/docs/guides/amplitude/"},{"body":"Connect your Cloudflare workers with InfinyOn Cloud for powerful event-processing data pipelines. InfinyOn Cloud\u0026rsquo;s robust data streaming allows you to seamlessly move and transform data and trigger actions.\nIn this guide, we\u0026rsquo;ll build a simple CloudFlare worker that sends events to InfinyOn Cloud through the webhook API.\nUse Cases Send form submission notifications to Slack. Send clickstream events to Amplitude. Send form submissions to HubSpot. Prerequisites To follow along you\u0026rsquo;ll need the following:\nnpm \u0026amp; curl installed locally Fluvio CLI installed locally Account on InfinyOn Cloud Let\u0026rsquo;s get started.\nCreate a Webhook External services send events to InfinyOn Cloud via webhooks, connectors, and custom clients. In this example, we\u0026rsquo;ll use webhooks.\nUse Fluvio CLI to create a webhook on InfinyOn Cloud.\nCreate a webhook configuration file cf-webhook.yaml: %copy%\nmeta: name: cf-webhook topic: cf-events webhook: outputParts: body outputType: json Create the webhook endpoint: %copy first-line%\n$ fluvio cloud webhook create -c cf-webhook.yaml Webhook \u0026#34;cf-webhook\u0026#34; created with url: https://infinyon.cloud/webhooks/v1/xyz The command returns an endpoint that tells Cloudflare where InfinyOn is listening for events.\nUse fluvio cloud webhook list to list all your webhooks.\nBuild a Cloudflare Worker Cloudflare uses Wrangler, a command-line tool that helps developers build workers.\nInstall Wrangler Use npm to install wrangler:\n%copy first-line%\n$ npm install -g wrangler Create a Worker Next, we\u0026rsquo;ll create a directory, write the worker code, and provision a configuration file for wrangler to access our code.\nCreate a project directory: %copy%\n$ mkdir cf-infinyon; cd ./cf-infinyon Create a file index.js and add the following code: %copy%\nconst WEBHOOK_URL = \u0026#34;https://infinyon.cloud/webhooks/v1/xyz\u0026#34;; addEventListener(\u0026#34;fetch\u0026#34;, (event) =\u0026gt; { event.respondWith(handleRequest(event.request)); }); async function handleRequest(request) { let jsonData = await request.json(); const response = await fetch(WEBHOOK_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-type\u0026#34;: `application/json`, }, body: JSON.stringify(jsonData) } ); const text_response = response.ok ? \u0026#34;\u0026#34; : \u0026#34;Webhook gateway error.\u0026#34;; return new Response(text_response, { status: response.status }); } The worker fetches an event, retrieves its JSON payload, and forwards it to the webhook gateway on InfinyOn Cloud.\nNote: Update the endpoint link with your own (see above).\nAdd a wrangler configuraiton file file wrangler.toml and add the following settings: %copy%\nname = \u0026#34;cf-infinyon\u0026#34; main = \u0026#34;index.js\u0026#34; compatibility_date = \u0026#34;2023-09-04\u0026#34; We are all set to run the code, but first let\u0026rsquo;s review the directory:\n├── index.js └── wrangler.toml Test Cloudflare to InfinyOn Cloud Pipeline With all the components provisioned, we should be ready to test our data pipeline end-to-end.\nStart the Cloudflare worker: %copy first-line%\n$ wrangler dev Starting local server... Ready on http://0.0.0.0:8787 Start the InfinyOn Cloud consumer: %copy first-line%\n$ fluvio consume cf-events --output json Consuming records from \u0026#39;cf-events\u0026#39; ⠤ Use curl to post an event: %copy%\n$ curl -v -X POST http://0.0.0.0:8787 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;hello\u0026#34;: \u0026#34;world!\u0026#34;}\u0026#39; The InfinyOn consumer show display the following event: { \u0026#34;hello\u0026#34;: \u0026#34;world!\u0026#34; } Congratulations! \u0026#x1f389; You have bridged Cloudflare workers with InfinyOn Cloud, the first set in building data reach event-driven services.\nNext steps:\napply smartmodule transformations inside the webhook configuration to shape the data before it is written to the topic attach sink connector that dispatches these events to other service such as Slack, Amplitude, SQL databases, etc. References Webhook to Slack Webhook Basics Webhook Configuration File ","description":"How to create a Cloudflare worker that sends events to a Webhook in InfinyOn Cloud.","keywords":null,"summary":"Connect your Cloudflare workers with InfinyOn Cloud for powerful event-processing data pipelines. InfinyOn Cloud\u0026rsquo;s robust data streaming allows you to seamlessly move and transform data and trigger actions.\nIn this guide, we\u0026rsquo;ll build a simple CloudFlare worker that sends events to InfinyOn Cloud through the webhook API.\nUse Cases Send form submission notifications to Slack. Send clickstream events to Amplitude. Send form submissions to HubSpot. Prerequisites To follow along you\u0026rsquo;ll need the following:","title":"Integrate Cloudflare Workers with InfinyOn Cloud","url":"http://localhost:1315/docs/guides/cloudflare-workers/"},{"body":"","description":"Data pipeline that sends events from a topic to your HubSpot CRM.","keywords":null,"summary":"","title":"HubSpot CRM","url":"http://localhost:1315/docs/guides/hubspot/"},{"body":"Real-time Streaming Analytics with Fluvio, DeepCausality, and Rust This blog was published in collaboration with Marvin Hansen and Deep Causality.\nIntroduction Why? Why this project? Why Fluvio? Why DeepCausality? Project Structure Architecture QD Communication Protocol QD Gateway Service Configuration Client handling Data handling Query vs. Fetch Data QD Client SBE Message Encoding Symbol Master (SYMDB) Real-Time Analytics The Model The Context Applied Contextual Causal Inference What was left out? Future of Real-Time Data Processing in Fluvio Future of DeepCausality Reflection Next Steps About Introduction Discuss this blog post on Maven, the world’s first serendipity network.\nMuch of my work gravitates around quantitative research of financial markets, for the most part working with custom-developed tools. When I started looking at risk management in the fast-moving crypto market, I wanted to try some new techniques based on the DeepCausality crate. Also, the recent SEC approval of Bitcoin ETFs made it clear that crypto is here to stay, and that’s just the perfect excuse to dive deep into quantitative research on crypto markets. All I needed was a backtesting facility to replay all trades from all markets and listen in to an exchange.\nI built just that for 700 markets listed on the Kraken exchange, with nearly one billion rows of trade data. For the impatient engineer, feel free to go to check out the project on GitHub. The system is built with Fluvio as a message system and uses the DeepCausality crate for real-time analytics. Fluvio is built from the ground up for real-time event processing, thus is an obvious choice. DeepCausality is a blazingly fast computational causality library for Rust, capable of running tens of thousands of inferences per second on a laptop at microsecond latency.\nI limited this project’s scope to building an end-to-end real-time data streaming system with a causal inference example mainly to showcase the whole picture without getting lost in any particular details. I believe any seasoned engineer can figure out the details when needed. Also, this system is not meant for 24/7 operation although it already contains a few sanity checks, observability, and error propagation. It is meant purely for educational purpose and to showcase what can be done already today with Rust, Fluvio, and DeepCausality with a sneak peak into the future at the end of this post.\nWhy? Why this project? Market data streaming systems come with some distinct requirements:\nMarket replay is always message-based, so you use event-driven architecture. As a result of that, error handling is also event-based. The database must support fetch or pagination to stream queries continuously. When you develop real-time systems that do risk assessments, measure volatility, or monitor VAR (Value at risk), you will probably explore many new ideas during the initial research. Next, you build these systems as closely to the actual market reality as you can. Therefore, you need a backtesting facility that replays the trade data as continuous streams, exactly as you would get them live from the exchange.\nHowever, building an event-based backtesting facility takes meaningful time and effort, and is rarely available as open source. There are several Python-based crypto libraries out there for live data streaming, placing orders on exchanges, or building trading bots, but nothing resembling an event-based backtesting facility. Despite its popularity in the blockchain and FinTech sphere, the Rust crate ecosystem offers surprisingly few solutions meaning I did not found an event based backtesting engine written in Rust during my initial research. While Rust may not give you a ready-to-use solution, it certainly provides handy building blocks for building a production-ready system.\nTherefore, it was time to roll up my sleeves and fill the gap. The catch is that the last time I worked on something similar, the entire system was written in Golang. That is to say, I started with zero experience with async message processing in Rust, let alone continuous async queries. Therefore, I started out this project mostly as a learning exercise to settle some questions I had about the Rust ecosystem. I wanted to know if there is a production ready message system written in Rust that can be used for this project, and if so, how well does it perform? Next, I was eager to learn how good Async works in practice. And the, it was largely unknown how well SBE message encoding would work with Rust given that Rust support was added only recently. Finally, I wanted to see how good Rust and cargo works in a mono-repo structure. A lot of questions had to be answered, and spoiler alert, I found answers to all of them and wrote the entire project in Rust.\nWhy Fluvio? Embracing the unfamiliar, I adopted Tokio as an async runtime, but then hit the next roadblock; I needed a fast and reliable message bus that wasn’t Kafka. You see, I have zero objection to deploying Kafka in a multi-million-dollar enterprise that can digest the infrastructure bill that comes with it, but when it comes to my own wallet, I want the best value for money. Kafka doesn’t cut it. Next, I had an idea: Why not make it a 100% Rust project and find a new message system that interacts nicely with Tokio Async and doesn’t cost a fortune in terms of infrastructure?\nIn response to the new challenge, I’ve looked at various new message systems written in Rust. After seeing the good, the bad, and the ugly, I’d almost given up on messaging in Rust. Almost. On a last-ditch Google search, I found the Fluvio project that ticks all three boxes: It’s written in Rust, works with Async, and is cheaper- actually a lot cheaper- to operate than Kafka.\nConventionally, for data streaming analytics, an existing Kafka deployment would be used together with Apache Spark or Flink to add real-time data analytics. For Kafka, the legacy JVM drives up most of the cost because, for any performance level, Java application requires more hardware, which is simply more expensive. Specifically, Kafka requires ~1 GB of RAM per partition, whereas Fluvio only requires ~50 MB of RAM per partition.\nWhen you use Rust and Fluvio, you accomplish real-time data streaming at a fraction of the operational cost. For example, one company migrated from Kafka to Fluvio and saw its annual cloud expenses drop by nearly half a million dollars annually.\nWhy DeepCausality? For real-time analytics, one would conventionally write Scala programs for Apache Spark or Flink, but an equal step operational cost is implied. Some industry practitioners report that the expenses required to operate Spark exceed 50% of the project budget. In this case, the high cost isn’t so much driven by the JVM but rather by costly support contracts and, even more so, by expensive GPU hardware. A state-of-the-art A100 GPU can cost you $3800 per month, and at scale you need a lot of them, so your cloud bill balloons. Supposing you are already heavily invested in the Spark / Flink ecosystem, you could look closely at alternate GPU providers such as Lambda Labs, Oblivious, or TensorDock to lower your GPU bill.\nOn the other hand, if you’re not invested in the Kafka/Spark/Flink ecosystem, you can explore the innovative Rust ecosystem to look for alternatives. Contemporary Ai comes in two categories: One is DeepLearning that gets all the headlines with capabilities of driving LLMs like ChatGPT. The other one is Computational Causality, which is lesser known, but drives the success of streaming companies like Netflix. More specifically, Netflix has built its successful recommendation engine based on three themes: Time, Context, and Causality. While Netflix’ secret causality engine remains secret for the time being, computational causality has become available to the Rust ecosystem via the DeepCausality crate. DeepCauslity pioneers a new direction of computational causality and contributes three key novelties:\nContext. You can add a context from which the causal model derives additional data. Since version 0.6, you can add multiple contexts. To my knowledge, that is the first causality library that provides built-in support for multiple contexts.\nUniform conceptualization of dimensionality. A uniform conceptualization of space, time, spacetime, and data, all uniform and adjustable, enables the bulk of contextual expressiveness.\nTransparent composability. You can define causal relations as a singleton, multiple in a collection, or a hyper-graph. However, structurally, all three are encapsulated in a monoidic entity called Causaloid, which allows causal relations to be composed freely. In other words you can define causal relations as a collection of singeletons, store that collection in a graph, then put that entire graph as a node into another causal graph and then reason freely over single parts, selected sub-graphs or the entire graph. As a result, you can break down otherwise complex models into smaller parts and freely assemble them together. As we will see later, the composability extends even further to crates,\nThe DeepCausality context guide elaborates on all three topics in more detail.\nWith Rust, Fluvio, and DeepCausality selected, I was good to go. Next, let’s look at the project structure.\nProject Structure The project follows a handful of best practices for mono-repos with the idea of scaling up the code base quickly. One critical practice is the declaration of internal dependencies in the workspace Cargo. config file, as discussed in this blog post. Another essential practice is to move shared modules into a separate crate. As a result, the project comprises of a fairly large number of crates (\u0026gt; 20) relative to the small code size (~10K LoC). The underlying reason is that incremental compilation simply runs faster when modules are separated as crates. The most important crates of the project are:\nflv_cli/data_importer – A tool to import the Kraken data into the database. flv_clients QD Client that connects to the data gateway. SYMDB Client that connects to the symbol master DB. flv_common – Types and structs commonly used in all other crates. flv_components – Contains several components that provide a coherent functionality. flv_examples – Multiple code examples for streaming data. flv_proto – Proto buffer specification for the symbol master database service. flv_sbe – Simple Binary Encoding (SBE) specification, Rust bindings, and message types. flv_services – Contains the data gateway and the symbol master service. flv_specs – Contains various specifications and configurations. In addition to these crates, there are a few more relevant folders:\ndata – Empty. Put the Kraken data here to import. Feel free to read the data import guide for details. doc – Contains relevant project documentation written in Markdown. scripts – Bash scripts that are used by make. Read the install guide for details. sql – SQL statements for exploring the data set. tools – Contains the SBE tool that generates the Rust bindings. Note, the Rust bindings are already in the repo, so you don’t need to use this tool. The SBE tool is only in the repo because it’s a patched version that irons out a few kinks in the default distribution. It’s safe to ignore unless you want to develop with the SBE format. With the project structure out of the way, let’s look at the architecture next.\nArchitecture The architecture follows the gateway pattern, meaning applications do not connect to the database directly. Instead, each application creates a QD Client that connects to the QD Gateway. The gateway handles essential tasks such as login/logout of clients. Likewise, the mapping from symbols to unique IDs happens via the Symbol Master Database (SYMDB) service. An application connects via the SYMDB client to the symbol service, resolves symbols it wants to stream data, and then connects to the QD gateway to request data streaming for the resolved symbols.\nQD Communication Protocol The communication between the QD client and gateway follows a simple protocol.\nThe client sends a login message with its client ID to the gateway. A client error message gets returned if the client is already logged in. If the client is not yet known, the login process starts. Notice that the gateway only returns error messages but no login success messages which means it is the application’s responsibility to monitor the QD client for errors. If there is no error, it is safe to proceed. Once connected, the client can send a request for either trade data or sampled OHLCV data at a resolution defined in the request message.\nThe gateway returns an error if the requested data is unavailable. If the data is available, the gateway starts the data streaming. When no further data is needed, the QD client is supposed to send a logout message to the gateway by calling the QD client’s close method. If this does not happen, the next login attempt with the same client ID will result in an error.\nQD Gateway The Quantitative Data Gateway (QDGW) is a gateway implemented as a Tokio microservice that streams financial market data from a database to connected clients. The QDGW exposes a Fluvio topic for clients to connect to. Clients can send requests and receive responses over this topic, and it handles client login/logout and maintains a state of connected clients.\nThe gateway processes clients’ requests for market data as trades or OHLCV bars by fetching the data from the database, serializing it into SBE messages, and streaming it to clients over their data channel. If there are issues processing a request, the gateway sends any error responses back over the control channel, and it maintains symbol metadata like symbol ID mappings, data types available, etc., to fulfill data requests correctly.\nService Configuration The QDGW configures itself using the configuration manager based on the service specification defined in the service_spec crate. As you add more services to a system, managing service configuration becomes increasingly more complex, and configuration mistakes tend to occur more frequently.\nIn response, I have developed a simple auto-configuration system to ensure each service self-configures correctly using a configuration manager. The component reads the service specs from a crate and provides methods that give access to various configurations. In larger systems, service specifications would probably be stored in a database, which the configuration manager would read. Because each service is uniquely identified by an Enum ServiceID, the configuration manager ensures each one gets only the configuration specified for that service. With this system, it is easy to reconfigure the service by updating its specification in the service spec crate.\nClient handling The gateway handles client login messages by extracting the client ID and checking if the they are already logged in. If so, it returns an error. Otherwise, it logs the client in. During the login process, the gateway creates a data producer for the client’s data channel to which the data will be streamed. This allows the gateway service to securely stream data to channels that only the client can receive.\nData handling The gateway similarly handles data request messages. It extracts the client ID, verifies that the client is logged in, and if so, looks up the channel name to which the data will be sent. In case of an error, a message will be sent to the client’s error channel. If there is no error, the gateway starts the streaming process.\nQuery vs. Fetch Data Conventionally, one would query a database, collect the results in a collection, and then iterate through the collection and send out each row as a message. This may work for smaller setups to some degree, but some of the markets on Kraken have well over 50 million trades, so just the query alone would take some time. There is a risk of timeout and, certainly, excessive memory usage since the entire trade history would be loaded into memory before streaming to the client could start.\nInstead, it is preferable to fetch data as an SQL stream and process each row as it becomes available and stream immediately to the client. In the background, the database probably uses pagination and batches of results until all rows have been returned. Regardless of the details, the QD gateway uses fetch mainly to prevent timeouts and excessive memory usage for larger datasets. Specifically, memory usage is at least tenfold lower for using fetch on the database.\nHowever, the QueryManager used by the QD gateway implements both query and fetch, so depending on the use case, it can do either. By observation, query allows for more sophisticated SQL queries but isn’t great at loading large datasets, whereas fetch excels at bulk data streaming, but only if the query is relatively simple and fast to execute.\nQD Client The QD (Quantitative Data) client is used to connect to the QD Gateway (QDGW) service to request and consume market data streams. Upon initialization, the QD client uses the Fluvio admin API to create client-specific topics that only the client knows. One crucial detail is that in Fluvio, only the admin API can create or delete topics, whereas the regular API cannot. In a production environment, the admin API can be secured with authentication, but in this project, I’ve skipped the security setup for simplicity. You create an admin API simply by calling the constructor:\nlet admin = FluvioAdmin::connect().await.expect(\u0026quot;Failed to connect to fluvio admin\u0026quot;); Once you have an admin API, you must construct a common request and a topic specification before sending the request to the cluster. You do that in two steps, as seen below.\nasync fn create_topic(admin: \u0026amp;FluvioAdmin, topic_name: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { // Define a new topic let name = topic_name.to_string(); let dry_run = false; let common_request = CommonCreateRequest { name, dry_run, ..Default::default() }; // Ok, specify the topic config let topic_specs = TopicSpec::new_computed(1, 1, None); // Create the topic admin .create_with_config(common_request, topic_specs) .await .expect(\u0026quot;Failed to create topic\u0026quot;); Ok(()) } The delete API is a simple, you just pass in the name of the topic to the admin API.\nasync fn delete_topic(admin: \u0026amp;FluvioAdmin, topic_name: \u0026amp;str) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { admin .delete::\u0026lt;TopicSpec\u0026gt;(topic_name.to_string()) .await .expect(\u0026quot;Failed to delete topic\u0026quot;); Ok(()) } The full implementation of create and delete topic is available in the flv_utils file of the QD client. The QD client dynamically creates its topics upon initialization. Next, the QD client initializes a connection to the Fluvio cluster. It then sends a ClientLogin message to the QDGW to log in.\nWhen the application calls a request data method, the QD client sends a data request message to the QDGW for either trades or OHLCV bars for a specific symbol. The QD client listens to its data channel topic for the responses from the gateway. The QDGW sends the requested data serialized as SBE messages. Error messages from the gateway are received over the client error channel. Sending a message in Fluvio is easy and follows the well-established producer/consumer pattern. For completeness, the send method of the QD client is shown below:\npub(crate) async fn send_message(\u0026amp;self, buffer: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { // Send the message. self. producer .send(RecordKey::NULL, buffer) .await .expect(\u0026quot;[QDClient/send_message]: Failed to send Done!\u0026quot;); // Flush the producer to ensure the message is sent instantly. self. producer .flush() .await .expect(\u0026quot;[QDClient/send_message]: Failed to flush to message bus.\u0026quot;); Ok(()) } Flush is called immediately because the QD client only sends a single message at a time and therefore the producer can flush immediately. If you were to send bulk data, Fluvio has a number of settings to optimize message batch and send from the producer, and you wouldn’t have to flush explicitly until you have the last message to send. Please consult the Fluvio documentation for more details on optimization.\nOne more detail: Fluvio usually sends key/value pair messages so that you can use the key to identify the message type. However, because SBE-encoded messages already have the message type fully accessible, each message is sent without a key, thus saving a few bytes on the wire.\nUpon shutdown, the QD client sends a ClientLogout message to the gateway to cleanly disconnect. It then deletes all previously generated client topics. See the close method in the QD client implementation for details.\nAll messages exchanged between the QD gateway and the QD client are fixed-sized binary encoded SBE messages.\nSBE Message Encoding The Financial Information Exchange Standard (FIX) has glued the entire financial industry together for decades. Interestingly, until version 4.4, the FIX format was all text-based despite the well-known deficiencies of text processing in low latency systems. Simple Binary Encoding (SBE) is a modern FIX standard for binary message encoding and is optimized for low latency and deterministic performance. The official standard and full FIX schema can be downloaded from the FIX community.\nIt is well known that in message-based systems, throughput and latency depend on the message size, meaning a smaller message size usually results in lower latency and higher throughput. The average market on Kraken has well over a million recorded trades; therefore, message throughput matters. Performance benchmarks have shown that SBE delivers among the smallest message sizes and the fastest serialization/deserialization speed compared to other binary formats. The biggest difference, though, is between text-based JSON and SBE, in which SBE delivers a full order of magnitude more operations per microsecond.\nFor this project, a much smaller and simpler custom SBE schema was designed specifically for small message sizes. See GitHub for the schema definition file. In the custom SBE schema, the largest data message counts 40 bytes, and most control messages count less than 30 bytes in size, hence allowing for high throughput and low latency processing. Enums encoded as u8 integers were repeatedly used in the SBE schema to reduce message size. Furthermore, instead of using strings for ticker symbols, u16 integer IDs were used to reduce message size further. To map between ticker symbols and the numeric integer ID corresponding to a ticker, a symbol manager was added as a component, to easily look up IDs for symbols.\nAnother trick is to avoid SBE Enums and instead use u8 fields with Rust Enums cast into u8. The idea of SBE Enums is to limit and specify the number of valid values. The problem is that you do the same thing in Rust already, and then you would have to map back and forth between SBE and Rust Enums. Therefore, in a Rust-only environment, it is safe to declare the SBE field as u8 and encode the Rust Enum as a u8 in SBE, and decode the SBE u8 as a Rust Enum. However, please don’t do this in a multilingual project where multiple programming languages exchange data via SBE because the counterpart is not guaranteed to process your raw u8 value correctly.\nWorking with SBE is straightforward: you write an XML schema that describes your types and messages, run the SBE tool to generate the Rust bindings, and then define a proper Rust type with the added SBE encoding and decoding. The XML schema is necessary because SBE generates bindings for other programming languages, and XML was chosen as the lowest common denominator that works with every supported language and platform.\nSymbol Master (SYMDB) One challenge related to binary-encoded messages is how to map symbols to the corresponding numeric ID. During the data import, numeric IDs are assigned based on a first-come, first-served principle, so there is no particular order or system in place.\nFor this project, a basic symbol master service has been implemented that maps the Kraken symbols to the corresponding numeric ID. This allows application developers to just pull a symbol list either from the database or directly from the Kraken API, select a set of symbols, and then use the symbol master to translate these symbols to numeric IDs as required for the message encoding. When SBE binary encoded data arrives at the QD client, then the same symbol master service enables the reverse translation of the numeric ID back to the corresponding symbol. For the scope of this project, this solution is sufficient and serves as a starting point to add more advanced functionality.\nThere is a noticeable difference between regulated financial markets and crypto markets. In regulated financial markets, currency symbols and stock tickers have globally standardized unique IDs. In the still-young crypto industry, no standard exists with the implication that the same cryptocurrency is listed under different symbols on different exchanges. Because of this, a Zoo of half-baked solutions exists. One solution is to pick a commercial integration service provider, such as Kaiko or CoinAPI, and simply adopt their symbol mapping. This only works reliably if you subscribe to their paid data feed. Another solution is to use a public platform such as CoinMarketCap as a reference for symbol mapping. Usually, the public API allows you to download the symbol mapping, but that still leaves you with some integration work to map these back to actual exchange symbols.\nWhen you want to support more than one exchange, you need a symbol master service to look up exchange-specific symbols. Note, the API signature of the symbol service requires an exchange ID; therefore, extending the symbol master service to support multiple exchanges is fundamentally possible but requires some engineering work to implement.\nReal-Time Analytics Once a data-stream has been established, you receive SBE encoded messages of either trade data or OHLCV bars. Trade bars usually reflect the spot price at which the last order was matched in the order book. While OHLCV bars have lower resolution, they often serve a purpose to establish points of reference. In quantitative research, these are called pivot points, reflecting the reality that at that price level, the market tends to pivot and basically make a U-turn. Pivot points remain instrumental in risk management because in markets, once the price is past a pivot point, at least you know the expected U-turn didn’t happen, so you have a starting point to make an informed guess. To make this kind of assessment, you need a model. Remember, there are three ingredients to a successful model:\nTime Context Causality The Model For this project, a fictitious monthly breakout model has been invented and implemented that showcase how to design real-time analytics that rely on temporal context to express causal relations. Please understand that the model is entirely made up and was never empirically validated, meaning there is no way you should use this model on your trading account if you don’t want to go broke. Also, the purpose of this post isn’t about financial modeling but rather about showcasing how to apply causal models to real-time data streams in Rust. With that out of the way, let’s look at the actual model:\nuse crate::prelude::{context_utils, CustomCausaloid, CustomContext, TimeIndexExt}; use deep_causality::errors::CausalityError; use deep_causality::prelude::{Causaloid, NumericalValue}; use rust_decimal::Decimal; /// Builds a custom [`Causaloid`] from a context graph. /// /// Constructs a new [`Causaloid`] with the provided context graph, /// causaloid, author, description, etc. /// /// The built model contains the full context graph and causaloid /// representing a causal model. /// /// # Arguments /// /// * `context` - Context graph to include in the model /// /// # Returns /// /// The built [`Causaloid`] . /// pub fn build_causal_model\u0026lt;'l\u0026gt;(context: \u0026amp;'l CustomContext\u0026lt;'l\u0026gt;) -\u0026gt; CustomCausaloid\u0026lt;'l\u0026gt; { let id = 42; // The causal fucntion must be a function and not a closure because the function // will be coercived into a function pointer later on, which is not possible with a closure. let contextual_causal_fn = contextual_causal_function; let description = \u0026quot;Causal Model: Checks for a potential monthly long breakout\u0026quot;; // Constructs and returns the Causaloid. Causaloid::new_with_context(id, contextual_causal_fn, Some(context), description) } fn contextual_causal_function\u0026lt;'l\u0026gt;( obs: NumericalValue, ctx: \u0026amp;'l CustomContext\u0026lt;'l\u0026gt;, ) -\u0026gt; Result\u0026lt;bool, CausalityError\u0026gt; { // Check if current_price data is available, if not, return an error. if obs.is_nan() { return Err(CausalityError( \u0026quot;Month Causaloid: Observation/current_price is NULL/NAN\u0026quot;.into(), )); } // Convert f64 to Decimal to avoid precision loss and make the code below more readable. // Unwrap is safe because of the previous null check, we know that the current price is not null. let current_price = Decimal::from_f64_retain(obs).unwrap(); // We use a dynamic index to determine the actual index of the previous or current month. // Unwrap is safe here because the build_context function ensures that the current month is always initialized with a valid value. let current_month_index = *ctx.get_current_month_index().unwrap(); let previous_month_index = *ctx.get_previous_month_index().unwrap(); // We use the dynamic index to extract the RangeData from the current and previous month. let current_month_data = context_utils::extract_data_from_ctx(ctx, current_month_index)?; let previous_month_data = context_utils::extract_data_from_ctx(ctx, previous_month_index)?; // The logic below is obviously totally trivial, but it demonstrates that you can // easily split an arbitrary complex causal function into multiple closures. // With closures in place, the logic becomes straightforward, robust, and simple to understand. // 1) Check if the previous month close is above the previous month open. let check_previous_month_close_above_previous_open = || { // Test if the previous month close is above the previous month open. // This is indicative of a general uptrend and gives a subsequent breakout more credibility. previous_month_data.close_above_open() }; // 2) Check if the current price is above the previous months close price. let check_current_price_above_previous_close = || { // Test if the current price is above the previous months close price. // gt = greater than \u0026gt; operator current_price.gt(\u0026amp;previous_month_data.close()) }; // 3) Check if the current price is above the current month open price. // This may seem redundant, but it safeguards against false positives. let check_current_price_above_current_open = || { // Test if the current price is above the current month open price. current_price.gt(\u0026amp;current_month_data.open()) }; // 4) Check if the current price exceeds the high level of the previous month. let check_current_price_above_previous_high = || { // Test if the current price is above the high price established in the previous month. current_price.gt(\u0026amp;previous_month_data.high()) }; // All checks combined: // // 1) Check if the previous month close is above the previous month open. // 2) Check if the current price is above the previous months close price. // 3) Check if the current price is above the current month open price. // 4) Check if the current price exceeds the high level of the previous month. if check_previous_month_close_above_previous_open() \u0026amp;\u0026amp; check_current_price_above_previous_close() \u0026amp;\u0026amp; check_current_price_above_current_open() \u0026amp;\u0026amp; check_current_price_above_previous_high() { // If all conditions are true, then a monthly breakout is detected and return true. Ok(true) } else { // If any of the conditions are false, then no breakout is detected and return false. Ok(false) } } This model is as straightforward as it looks. To summarize its function:\nIt defines the causal function that will check for the monthly breakout condition. The causal function takes the price observation and context as arguments. It uses the context to look up the current and previous month’s data nodes. The data is extracted from the node. The current price is compared to determine a potential monthly breakout. This takes four steps: Check if the previous month’s close is above its open. Check if the current price is above the previous month’s close price. Check if the current price exceeds the current month’s open price. Check if the current price exceeds the high level of the previous month. If all four conditions are true, then a monthly breakout is detected and returns true. At this point, it becomes evident why only certain analytics problems can be converted to causal models, whereas others that rely on predictions cannot. However, you will be amazed how many of those problems showcased in the model exist in the world. For example, IoT sensors monitoring pressure sensors at an industry facility for anomalies can be modeled in a very similar way. Conventionally, none of these problems are particularly hard to solve unless you deal with dynamic context or, worse, multiple contexts. In that case, DeepCausality with its support for multiple contexts brings a lot to the table.\nIn the model code, I want to highlight the following five lines:\nlet current_month_index = *ctx.get_current_month_index().unwrap(); let previous_month_index = *ctx.get_previous_month_index().unwrap(); // We use the dynamic index to extract the RangeData from the current and previous month. let current_month_data = context_utils::extract_data_from_ctx(ctx, current_month_index)?; let previous_month_data = context_utils::extract_data_from_ctx(ctx, previous_month_index)?; When you look at the DeepCausality context API specification, it doesn’t contain the methods that get the current or previous month’s index. Instead, a type extension defines these custom methods. In this particular case, the TimeIndexExt is written as a type extension with a default implementation against the signature of a super trait implemented in the target type context. As a result, with a single import, you add new functionality to an external type. The formidable iterator_ext crate uses a similar technique to add more functionality to the standard iterator in Rust.\nWith the TimeIndexExt extension in place, the model above works out of the box even though the DeepCausality crate only provides the building blocks. For convenience, the entire model, with its context builder, protocol, type extension, and actual causal model definition, has been put in a dedicated crate.\nThat’s another particularity of DeepCausality models: they compose with standard tools such as Cargo so you can build large causal models from various building blocks in separate crates. Because of type extensibility, you may customize any aspect as needed as long as it links back to super traits implemented in the DeepCausality crate. If something is missing, feel free to open an issue or submit a pull request.\nThe Context The context is a central piece of the model. It is the place where all related data are stored for the model.\nIn DeepCausality, a context can be static or dynamic, depending on the situation. The context structure is defined beforehand for a static context, whereas for a dynamic context, the structure is generated dynamically at runtime. Regardless of the specific structure, DeepCausality uses a hypergraph to internally represent arbitrary complex context structures.\nThe hypergraph representation of context in DeepCausality conceptualizes time as a non-linear category of unknown structure with the requirement that it is also linearly expressible to adhere to the common interpretation of time linearity under the time arrow assumption. That way, both linear and non-linear time scenarios can be represented.\nContext provides either internal, external, or both types of variables to the causal model. Furthermore, whether variables are independent or dependent doesn’t matter because any dependent variable can be updated through change propagation via the adjustable protocol. In practice, that means you can derive context data from the data stream itself, from external sources, say Twitter sentiment, or any combination of internal and external data. By convention, for dynamic contexts, you update the context first before applying the model.\nFor this project, a static context is generated that adds range data for the year and month of the incoming data. This is a significant simplification compared to the actual reality but necessary to reduce complexity. As stated in the introduction, geometric causality reduces arithmetic complexity by increasing structural complexity. Since there is nothing difficult about the model, the complexity must be elsewhere. And indeed, the structural complexity has been shifted into the context.\nGenerating a context comes down to three steps:\nLoad some data Transform data as necessary Build a context structure as required All three steps are highly depending on the project requirements. However, for this project, I chose to build a static temporal graph augmented with range data. You find the full code of the context generator is in the project repo. The context graph is built by adding nodes for each month and year to the graph. By convention, a context graph starts with a root node, that is added as shown below.\n// == ADD ROOT ==// let id = counter.increment_and_get(); let root = Root::new(id); let root_node = Contextoid::new(id, ContextoidType::Root(root)); let root_index = g.add_node(root_node); The root node is a special node that has no parents and serves a structural point of reverse when dynamically traversing large graphs. The root node links to each year represented as temporal node. The temporal nodes are added as shown below.\n// == ADD YEAR ==// let time_scale = TimeScale::Year; let elements = data.year_bars(); for data_bar in elements { // Get year from bar let year = data_bar.date_time().year(); // Augment OHLCV bar with time and data nodes let (tempoid, dataoid) = context_utils::convert_ohlcv_bar_to_augmented(data_bar, time_scale); // Create year time node let key = counter.increment_and_get(); let time_node = Contextoid::new(key, ContextoidType::Tempoid(tempoid)); let year_time_index = g.add_node(time_node); // Create year data node let data_id = counter.increment_and_get(); let data_node = Contextoid::new(data_id, ContextoidType::Datoid(dataoid)); let year_data_index = g.add_node(data_node); // .. Indexing // println!(\u0026quot;{FN_NAME}: Linking root to year.\u0026quot;); g.add_edge(root_index, year_time_index, RelationKind::Temporal) .expect(\u0026quot;Failed to add edge between root and year.\u0026quot;); // println!(\u0026quot;{FN_NAME}: Linking year to data.\u0026quot;); g.add_edge(year_data_index, year_time_index, RelationKind::Datial) .expect(\u0026quot;Failed to add edge between year and data\u0026quot;); The month nodes are added in a similar fashion. When designing a context, you should think about the data that you want and the context structure that you want to build. It’s best to draw the desired structure on a sheet of paper before implementing it. Lastly, by experience, you are going to spent more time on building and debugging context than building the model itself. Please pay meticulous attention to correct indexing of the context graph as this is what makes the the causal model work.\nApplied Contextual Causal Inference In terms of applying the model to incoming data messages, it’s as simple as writing a standard event handler. The only meaningful difference is the particularity of SBE encoding. Because SBE is fixed-sized encoding, the position of the message ID in the byte array is always known upfront; therefore, you can extract the message ID without decoding the actual message. This is true for all primitive types in SBE, but for message ID it’s so convenient that it’s worth showing in action. From the XML schema, we know that message ID is always in the third position; here is how you extract and use the message ID for incoming data bars:\npub fn handle_message_inference(\u0026amp;self, message: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error + Send\u0026gt;\u0026gt; { // The third byte of the buffer is always the message type. let message_type = MessageType::from(message[2] as u16); match message_type { // Handle first trade bar MessageType::FirstTradeBar =\u0026gt; { println!(\u0026quot;{FN_NAME}: FirstTradeBar (Data stream Starts)\u0026quot;); } MessageType::TradeBar =\u0026gt; { let trade_bar = SbeTradeBar::decode(message.as_slice()).unwrap(); // println!(\u0026quot;{FN_NAME}: TradeBar: {:?}\u0026quot;, trade_bar); // println!(\u0026quot;{FN_NAME}: Extract the price from the trade bar: {}\u0026quot;, trade_bar.price().to_f64().unwrap()); let price = trade_bar.price().to_f64().unwrap(); println!(\u0026quot;{FN_NAME}: Apply the causal model to the price for causal inference\u0026quot;); let res = self.model.verify_single_cause(\u0026amp;price).unwrap_or_else(|e| { println!(\u0026quot;{FN_NAME}: {}\u0026quot;, e); false }); // Print the result of the inference in case it detected a price breakout if res { println!(\u0026quot;DeepCausality: Detected Price Breakout!\u0026quot;); } } // Handle last trade bar MessageType::LastTradeBar =\u0026gt; { println!(\u0026quot;{FN_NAME}: LastTradeBar (Data stream stops)\u0026quot;); } // Ignore all other message types _ =\u0026gt; {} } Ok(()) } } This is it. Simplicity is the ultimate sophistication. You extract an integer, convert it to an Enum that tells you what kind of message to decode, then you pattern match over that Enum. When you get a TradeBar, you decode it, extract the current price, convert it (from Decimal) to f64, and pass it to the model.\nEverything here is standard Rust, including error handling, pattern matching, processing, and everything you find in the Rust book. By my internal measurements, the causal inference adds, at most, a single-digit microsecond latency to the message processing. You will never notice it if your real-time system operates at a millisecond level. Even if your system operates at a microsecond level, adding single-digit microseconds might be acceptable. When it’s not, you can still optimize the context with some clever lookup tables and probably get it faster.\nWhat was left out? Looking through the repo, you will unavoidably find things not mentioned in this post, simply because explaining the entire code base in a single blog post post is infeasible. However, I have good news for you because this project is exceptionally well-documented (just run make doc) and has plenty of unit tests. Browsing the code with the documentation and tests should help you understand whatever wasn’t mentioned in this post.\nI have deliberately skipped a general introduction to Rust because it’s not the focus of this post. It is assumed when you read about real-time causal inference in Rust that you already have some experience writing software in Rust. At this point, the internet provides an abundance of material to learn Rust; just use your favorite search engine to find the best Rust resources for you.\nI’ve omitted a general introduction to computational causality, mainly to keep the post readable. The DeepCausality project has documentation that covers the basics and more. For a gentle introduction to the field, read “The Book of Why?” by Judea Pearl, the grandfather of computational causality.\nLastly, I have left out a general introduction to quantitative finance and market modeling to keep the post to a reasonable length. There are several good books for any topic in quantitative finance. My top three go-to recommendations are Financial Modeling by Simon Benninga, Advances in Financial Machine Learning by Marcos López, and The Successful Trader’s Guide to Money Management by Andre Unger.\nFuture of Real-Time Data Processing in Fluvio One particularity you may encounter in real-time systems is the prevalence of the microservice pattern. While the project’s code examples all show client-side processing, you could equally put a causal model or any other form of processing into a microservice. At least, that is a common conception unless you already have a full Spark cluster deployment. The Fluvio project already supports smart modules that allow you to perform common stream processing tasks such as collecting, transforming, deduplicating, or dispatching messages within the Fluvio cluster. However, the second generation of stateful services takes that concept one step further and allows for composable event-driven data pipelines using web assembly modules. Access to stateful services is currently in private preview, and I had chance to test it out. The technology is outstanding, and with those web assembly modules, you can replace a bunch of microservices. While the stateful service may take some time to mature, I am confident it will be on a similar quality level to the existing system. I recommend Fluvio as a message bus to cut the cloud bill and see how the stateful service evolves to see if your requirements can be met by the new paradigm. Like causal models, it won’t work for everything. Still, when it works, you will discover something truly intriguing that gives you capabilities previously thought unattainable. And you get it at an absurdly low operational cost.\nFuture of DeepCausality Even though this project only scratched the surface of what DeepCausality can do, a few more features are planned. For once, a ticket is open to remove lifelines from the public API. When completed, the DeepCausality crate will work significantly better with concurrency code. Right now, these lifelines conflict with Tokio’s requirement that each task must own all its data to ensure thread safety. As a result, you cannot easily share a context between tasks via the usual Arc/Mutex wrapper, and therefore, async \u0026amp; concurrent context updates are currently too cumbersome. The lifeline removal is only a refactoring task and should be done sooner rather than later.\nModeling modular contextual causal propagation is an area that needs further exploration. Specifically, this means writing intermediate effects to the context of another downstream causal model, which then uses the updated context to reason further and writes its inferences into another downstream context. By implication, modular models result in modular reasoning, and at each stage, intermediate results remain encapsulated yet accessible to other models or monitoring systems. This approach is potent given that any of those models may have multiple contexts to feed into the causal model at each stage. The future of DeepCausality is evolving towards increasingly more advanced and sophisticated real-time causal inference.\nReflection When I started with the project, several unknowns had to be answered. For once, it wasn’t clear if there was a production-ready messaging system written in Rust. That has been fully answered because Fluvio is certainly production-ready when it comes to messaging and even its initial version of Smart Modules works. During development I didn’t encounter any difficulties, and those few questions I had were quickly answered either from the documentation, the project Discord, or simply by looking into the source code.\nAnother unknown that had to be considered was the quality of Rust Async at the moment, given its rapid evolution over the past few years. Using it confidently is a no-brainer because the Tokio ecosystem works out of the box, and for every possible situation that may come up, there is either documentation or some online question that has already been answered. On that topic, Prost is perhaps the fastest and easiest way to write gRPC services and it beats the GoLang ecosystem by a mile. It might not be the fastest implementation regarding total requests per second, but once you have your proto file, you have a functional gRPC server within 30 minutes, depending on how fast you can type. It’s really impressive and an excellent example of how much better the development experience can be in Rust.\nIn addition, it wasn’t known how well SBE would work in tandem with Rust. Again, it was of no concern and they coordinated seamlessly. I just got things done. Fixed-sized binary formats are always a bit more verbose to work with, but the net gain in message throughput and latency you get due to smaller message sizes is well worth the effort.\nLastly, there is Cargo on a mono-repo. Historically, I’ve adopted Bazel early on whenever it was clear from day one that the code base would grow very fast and multiple 10x increases could be expected. There is a strong argument for Bazel, and probably plenty against too, but once your project is past the first 50k LoC, the choices of build systems that can cope with rapidly growing code are limited. Alternatively, Cargo gives you much more breathing space before a hard decision has to be made. Specifically, building 20 Crates with Cargo is a breeze, and works way better than expected. I estimate that you may be able to kick the can further down the road by another 10X, so only when you hit a 100K LoC or about 200 crates do you have to think hard about Bazel, mainly to get your release build time down. That’s reassuring and refreshing compared to what I have experienced with other ecosystems. By the time you hit 500K LoC, you end up with Bazel anyway.\nNext Steps Even though this project concluded, there would be a few steps more steps required to expand the QDGW into a fully-fledged quantitative data research system. First, the QD client needs some polishing so that it’s easier to use. As you can see in the code examples, programming the QD client with async processing isn’t as good as it could be.\nThem adding more advanced features like the ability to backtest risk assessment or trading strategies would be another next step. There are two ways to backtest risk assessment or trading strategies. One way is to track positions on the clients’ side, which is usually more straightforward to implement. This is great for getting started, especially for single-instrument strategies. However, it isn’t very realistic as it does not consider trading fees, slippage, and order splitting. None of these matter for smaller positions because small orders rarely get split, and slippage usually doesn’t amount to much compared to trading fees. The second way is to implement a virtual exchange that emulates the execution and fee schedule of a particular exchange. If the virtual exchange solely relies on trade data, it cannot consider slippage and order splitting so there is diminishing return. If it is implemented using order book replay, then it can emulate slippage and order splitting. Be aware that order book reconstruction requires some sophisticated engineering and full quote data.\nLastly, a user interface for visualizing backtesting results would complete this project. UI in Rust remains one of the few areas where the ecosystem is still in its early days. There are a few great projects out there, such as Tauri, but I haven’t used it myself therefore I can’t comment on it.\nAbout Fluvio is an open-source data streaming platform with in-line computation capabilities. Apply your custom programs to aggregate, correlate, and transform data records in real time as they move over the network. Read more on the Fluvio website.\nDeepCausality is a hyper-geometric computational causality library that enables fast and deterministic context-aware causal reasoning in Rust. Learn more about DeepCausality on GitHub and join the DeepCausality-Announce Mailing List.\nThe LF AI \u0026amp; Data Foundation supports an open artificial intelligence (AI) and data community, and drive open source innovation in the AI and data domains by enabling collaboration and the creation of new opportunities for all the members of the community. For more information, please visit lfaidata.foundation.\nThe author, Marvin Hansen, is the director of Emet-Labs, a FinTech research company specializing in applying computational causality to financial markets. He is the author and maintainer of the DeepCausality project. Connect on Maven, the worlds first serendipity network.\nYou can contact us through Github Discussions or our Discord if you have any questions or comments, we welcome any early feedback about fvm.\n","description":"real time streaming analytics with fluvio, deepcausality, and rust","keywords":null,"summary":"Real-time Streaming Analytics with Fluvio, DeepCausality, and Rust This blog was published in collaboration with Marvin Hansen and Deep Causality.\nIntroduction Why? Why this project? Why Fluvio? Why DeepCausality? Project Structure Architecture QD Communication Protocol QD Gateway Service Configuration Client handling Data handling Query vs. Fetch Data QD Client SBE Message Encoding Symbol Master (SYMDB) Real-Time Analytics The Model The Context Applied Contextual Causal Inference What was left out? Future of Real-Time Data Processing in Fluvio Future of DeepCausality Reflection Next Steps About Introduction Discuss this blog post on Maven, the world’s first serendipity network.","title":"Real-time Streaming Analytics with Fluvio, DeepCausality, and Rust","url":"http://localhost:1315/blog/2024/02/fluvio-deep-causality-rs/"},{"body":"This is a short story from engineering about how we saved ourselves a lot of time and effort. Instead of spending more time tuning the code for a new microservice, we replaced it with a general purpose connector.\nWe started off wanting to add analytics to better understand how people use our product. Our default assumption is that creating a microservice was the way to go. But as we moved forward, we found that a simpler and more flexible solution was right in our own product - the connector.\nThis story shares our journey, the challenges we faced, and how switching to a connector-based model made our analytics and our work processes better and easier.\nChoosing the Right Path for Analytics We decided to use Amplitude for analytics. The biggest impact of this detail was we had to manually instrument our production code for InfinyOn Cloud.\nAfter the the modifications were made to our existing services, we needed a simple pattern for sending events that would be easy to adapt future additional microservices.\nWith urgency to quickly see our data, it was natural that a special microservice was quickly implemented to link our services and Amplitude and added to our deployment automation. The way it worked is that our existing services would send HTTP POST requests with json event data to this new Analytics service. The Analytics service would construct the json request for Amplitude and then pass it along.\nUnveiling the Hitches Post-Deployment When we started integrating into production, we ran into some intermittent problems with our event streaming to Amplitude. This was a classic case of \u0026ldquo;works on my machine\u0026rdquo; failing to account for the realities of the production environment.\nOur analytics microservice occasionally lagged behind the startup of other services, causing a race condition that affected the startup configuration. During this period of time, events were not being emitted. We are losing this data we\u0026rsquo;re trying to collect.\nThe lack of a retry mechanism for service initialization exacerbated the issue, turning our analytics collection into a manual chore rather than an automated process. These problems were easily resolved once identified, but this is toil for the Ops team.\nThe microservice, meant to simplify analytics, was instead adding layers of operational overhead. A long-term solution would require investing more dev time before this process meets our resilience needs.\nEmbracing Our Own Product: The Connector Revelation We thought about using our own product to fix the issues. We started considering using a connector instead of a microservice.\nWhile testing this idea, we found that our http-sink connector could do the job, replacing the special analytics connector we were using. Using the http-sink connector, along with Jolt, a SmartModule to transform json data, we could do what we planned initially and even improve it.\nCoincidentally, we prevent the deployment-time race conditions that caused us to lose event data. Our services have to connect to our Fluvio instance, and as infrastructure, it is deployed and available before microservices are deployed.\nThe following is an example connector config from our Amplitude setup guide.\n%copy%\n# amplitude-connector.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: amplitude-connector type: http-sink topic: service-events secrets: - name: AMPLITUDE_API_TOKEN http: endpoint: \u0026#34;https://api2.amplitude.com/2/httpapi\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: remove spec: timestamp: \u0026#34;\u0026#34; - operation: shift spec: \u0026#34;event\u0026#34;: \u0026#34;events[0]\u0026#34; - operation: default spec: api_key: \u0026#34;${{ secrets.AMPLITUDE_API_TOKEN }}\u0026#34; Conclusion The journey illustrated the power of adapting and opting for simplicity.\nReplacing a microservice with a connector not only resolved our operational issues but also presented a pattern that was easy to integrate and scale.\nThe connector model reduced the manual work, eliminated a good chunk of unnecessary code, and tests, paving the way for a more streamlined analytics integration.\n","description":"Short development story about how we use Fluvio for our analytics","keywords":null,"summary":"This is a short story from engineering about how we saved ourselves a lot of time and effort. Instead of spending more time tuning the code for a new microservice, we replaced it with a general purpose connector.\nWe started off wanting to add analytics to better understand how people use our product. Our default assumption is that creating a microservice was the way to go. But as we moved forward, we found that a simpler and more flexible solution was right in our own product - the connector.","title":"Simplifying Analytics Integration","url":"http://localhost:1315/blog/2023/10/simplify-analytics-integration/"},{"body":"We are excited to present fvm (Fluvio Version Manager), a new tool designed to streamline the management of multiple versions of the Fluvio CLI tools. This tool is currently in beta, but you are welcome to try fvm out now and enjoy a simplified setup process along with the ease of switching between different Fluvio versions.\nHere’s what changes are introduced with fvm:\nSimplified Installation: The installation script now installs fvm first, which in turn, automatically installs the rest of Fluvio for you. For newcomers, the installation procedure remains the same. However, existing users can transition to this new setup by following a straightforward guide detailed in the installation instructions. Streamlined Version Management: With fvm, the earlier challenges faced by developers and our design partners regarding Release Channels are significantly reduced, making it much simpler to handle various Fluvio versions. Seamless Integration with InfinyOn Hub: This change aligns with our goals of enhancing the capabilities of our tools and delivering them through the InfinyOn Hub. fvm is the standard method for distributing our CLI binaries moving forward. This transition not only minimizes developer friction but also paves the way for exciting features that we plan to deliver through InfinyOn Hub.\nWe’re quickly working towards making fvm the standard installation method.\nHere is a preview of what’s on the horizon. This is fvm installing the most recent release.\n$ fvm install info: Downloading (1/5): fluvio@0.11.4 info: Downloading (2/5): fluvio-cloud@0.2.17 info: Downloading (3/5): fluvio-run@0.11.4 info: Downloading (4/5): cdk@0.11.4 info: Downloading (5/5): smdk@0.11.4 done: Installed fluvio version 0.11.4 done: Now using fluvio version 0.11.4 For a more detailed introduction check out the installation tutorial.\nYou can contact us through Github Discussions or our Discord if you have any questions or comments, we welcome any early feedback about fvm.\n","description":"Announcement of new tool `fvm` - the Fluvio Version Manager CLI","keywords":null,"summary":"We are excited to present fvm (Fluvio Version Manager), a new tool designed to streamline the management of multiple versions of the Fluvio CLI tools. This tool is currently in beta, but you are welcome to try fvm out now and enjoy a simplified setup process along with the ease of switching between different Fluvio versions.\nHere’s what changes are introduced with fvm:\nSimplified Installation: The installation script now installs fvm first, which in turn, automatically installs the rest of Fluvio for you.","title":"Fluvio gets a version manager","url":"http://localhost:1315/blog/2023/10/introducing-fvm/"},{"body":"Content Increase in Connected Devices Challenges of a Large IoT ecosystem A First Principle Solution Benefits of a Lightweight Data Streaming Solution Experience Edge Data Streaming Workflow in 10 minutes Increase in Connected Devices Internet of Things (IoT) refers to a network of connected devices, tools, machines, and sensors that communicate with each other and share data. These devices can be remotely controlled and receive commands from the cloud, providing users with a convenient way to manage and monitor their smart systems.\nBy combining various components such as hardware, connectivity means, cloud-native software, and user interfaces, IoT enables efficient asset and operation management, improved work safety and productivity, and more. By leveraging IoT, businesses have been working to optimize their operations, reduce costs, and improve sustainability. For example, smart devices can monitor energy consumption, flow rate, pressure, temperature, speed and more data points which provide insights on how to reduce waste, improve efficiency, quality etc..\nAccording to Finances Online - The number of connected Internet of Things (IoT) devices crossed the global population in 2020. Currently, we have well over 13 billion IoT devices and growing.\nIn recent years the number of movable edge sensors has grown exponentially, driven by smart technology in cars, buses, trains, ships, aircraft, drones, and robots. These sensors provide rich information about their operating environment and open the door for new industrial use cases such as real-time asset monitoring, performance management, predictive maintenance, and more. The new world of digital representation of industrial assets is known as digital twins.\nThe purpose of these devices are to improve various aspects of society and human life by increasing the efficiency of the infrastructure that we as humans rely upon.\nChallenges of a Large IoT ecosystem There are 4 obvious challenges of the amount of connected devices:\nCollecting and Processing such large volumes of data at scale is not cheap or easy Connected infrastructures have additional challenges with security and data integrity Device maintenance and longevity of of edge devices is challenging with both internal and external threats All of IoT infrastructure has constrained system resources like compute processor, memory, on device storage etc. On top of that movable sensors present developers and system architects with unique challenges for building a scalable infrastructure that processes data efficiently at the edge and in the cloud and ensures transmission without data loss. Sensors often face unstable or low-bandwidth networks that restrict sending data to the cloud to small time windows.\nA First Principle Solution In a small segment on the The Fully Charged Podcast, Ford CEO Jim Farley articulates the problem clearly. Based on the historical patterns of automotive manufacturing, Jim Says, \u0026ldquo;We have about a 150 of these modules of semiconductors all through the car, the problem is the software is written by 150 different companies, and they don\u0026rsquo;t talk to each other.\u0026rdquo;\nAn ideal data infrastructure for movable sensors need to give developers simple primitives and a unified interface for data collection and transformations at the edge and in the cloud. And a clean communication mechanism that handles networking and memory challenges under the hood.\nThe InfinyOn IoT edge data streaming runtime complements the InfinyOn Cloud data streaming platform to provide a solution to these challenges. Developers can now build efficient data streaming pipelines with reliable communication layers for pushing events to a downstream cluster.\nIf connected, InfinyOn IoT edge sends telemetry and events to the InfinyOn Cloud in real-time using mirroring.\nIf disconnected, the InfinyOn IoT edge stream processor caches events locally. When the connection resumes, the InfinyOn IoT edge stream processor brings InfinyOn Cloud up to date and continues mirroring until the subsequent connection loss.\nInfinyOn IoT edge is a ~14 Mb binary that runs on ARMv7 chips on less than 256 MB memory. We are working with teams building the future of monitoring dynamic assets to push the boundaries of edge data stream processing.\nBenefits of a Lightweight Data Streaming Solution Our solution for IoT and edge data analytics is a lightweight datastreaming platform, that is cloud native and can be deployed on the edge. It is the first unified edge ready cloud native data streaming platform buit with Rust and extended by web assembly. It is compatible with MQTT and Apache Kafka for smooth transition experince.\nThe benefits of the InfinyOn solution are as follows:\nReliable edge-to-cloud synchronization: Real-time publishing when connected. Edge collection without downtime when disconnected. Automatic synchronization after reconnect. Edge devices can be offline for extended periods (days). Reliable local caching for gigabytes of data. Simplified logic for edge clients. Reliable connection to the local clients. Intelligent processing at the edge with InfnyOn Smartmodules Experience Edge Data Streaming Workflow in 10 minutes Follow the tutorial in the documentation to experience the product. IoT caching and mirroring documentation (https://infinyon.com/docs/guides/iot-mirroring-cloud/)\nExplore the Use Case page and let us know how we could help you with your edge analytics use cases.\nConnect with us: Please, be sure to join our Discord server if you want to talk to us or have questions.\nSubscribe to our YouTube channel\nFollow us on Twitter\nFollow us on LinkedIn\nTry InfinyOn Cloud a fully managed Fluvio service\n","description":"Challenges and solutions related to edge data analytics.","keywords":null,"summary":"Content Increase in Connected Devices Challenges of a Large IoT ecosystem A First Principle Solution Benefits of a Lightweight Data Streaming Solution Experience Edge Data Streaming Workflow in 10 minutes Increase in Connected Devices Internet of Things (IoT) refers to a network of connected devices, tools, machines, and sensors that communicate with each other and share data. These devices can be remotely controlled and receive commands from the cloud, providing users with a convenient way to manage and monitor their smart systems.","title":"4 key challenge for IoT Analytics and a First Principle Solution","url":"http://localhost:1315/blog/2023/09/edge-data-streaming/"},{"body":"Content Rust or Go Go Rust or go home Teleological argument Advantages Performance Concurrency Scalability Memory Safety Developer Experience Why we chose Rust to build Fluvio Conclusion Rust or Go Someone on the data engineering subReddit recently posted a discussion starter with the title - \u0026ldquo;Rust or Go.\u0026rdquo; The author wanted the community\u0026rsquo;s recommendation on picking up a new programming language for data engineering. Like every dichotomy, this is a simple question at the surface, but the answer to the question is complex.\nIn this post, we will compare Rust and Go and rationalize a solution in the context of data engineering. Unlike most comparison articles that provide no definitive solution, this article will provide a clear taxonomy of constraints and our choice for data engineering.\nGo Rust or go home It\u0026rsquo;s impossible to resist the temptation of making puns with names like Rust and Go! There would be a few puns in this article. If you are not into puns, you have been warned!\nI want to layout our biases upfront! InfinyOn Cloud is powered by Fluvio a data streaming runtime written entirely in Rust.\nThe ideal belief is that \u0026ldquo;Developers strive to be language agnostic and use the ideal tool for the ideal use case.\u0026rdquo;\nIn reality, there are several other factors that influence the tools we choose beyond logic, data, and hard facts. Behavioral factors like resistance to change, defaulting to familiar tools and mental models, hard trades between functionality, cost, available talent capital, time and complexity of building and running the infrastructure, tech stack, codebase etc. impact tooling decisions.\nHere is the bottomline: Rust and Go are both really powerful languages.\nRust is our choice for data applications.\nFor data engineering, there are several elements in the way Rust is built that make it a better choice above Go.\nThat\u0026rsquo;s our take. The cards are on the table. Let\u0026rsquo;s dig into why we have come to that conclusion!\nTeleological arguments for Go and Rust Telos (greek τέλος) is a term used by Aristotle to refer to the ultimate cause. In this article, the telelogical argument identifies the purpose of existence of GoLang and RustLang. Using the term teleology brings in a bit of philosophy into the equation.\nGo was open sourced back in November 2009 by Google. The pithy announcement describes Go as follows - \u0026ldquo;Go is a great language for systems programming with support for multi-processing, a fresh and lightweight take on object-oriented design, plus some cool features like true closures and reflection.\u0026rdquo;\nThe current website for GoLang promises the following:\n\u0026ldquo;Build simple, secure, scalable systems with Go\u0026rdquo;\nThe biggest arguments for Go are:\nGoLang has built-in concurrency and robust standard library. GoLang is supported by a large ecosystem. GoLang is supported by Google. Go shines for web and API development and building server side applications and microservices. Go is an ideal choice for:\nEase of writing code. Simplicity and readability. Fast build times and lean syntax. Flexibility in web and API development. Rust was a side project for 3 years before Mozilla sponsored Rust in 2009. The FAQs associated with the Rust project states - \u0026ldquo;Rust exists as an alternative that provides both efficient code and a comfortable level of abstraction, while improving on all four points(safety, concurrency, practical affordance, control over reseources).\u0026rdquo;\nMozilla created a project called Oxidation to integrate Rust, and there are precise guidelines on the benefits to using Rust.\nThe current Rust website describes Rust as -\n\u0026ldquo;A language empowering everyone to build reliable and efficient software.\u0026rdquo;\nThe biggest arguments for Rust:\nMemory and thread safety. Performance and Efficiency. Great documentation and error messages. Rust shines in data and platform engineering scenarios that need to deliver robust backend data processing. Rust is ideal choice for:\nReliability. Performance. Memory safety. Fine grained control. Of the two websites, one of them is more mundane. In case you are wandering which one - it is the Rust one. Even the comment promoted by the Rust website by an engineer from npm, compliments Rust for being \u0026ldquo;boring!\u0026rdquo;\nBut that\u0026rsquo;s just their website and narrative! Let\u0026rsquo;s look at their advantages and performance.\nAdvantages of Go and Rust One of the amazing things about Go is the ability to run functions as subprocesses using Goroutines. The simplicity of adding go syntax in a function to run as a subprocess is magical. Add to it the ability to use multiple CPU cores to deploy workloads and you have a solid and efficient language.\nRust however is quite different. It has a bit of a learning curve to get started and used to. Rust uses references, traits, and the concept of borrowing to utilize memory objects effectively without the need for replicas or copies. Rust does not have a garbage collector and while it takes longer to get used to and front load compile times, the resulting software removes the dreaded out of memory exceptions!\nPerformance benchmarks to compare Rust and Go The top 5 language comparisons on Benchmarks Game are between C#, Java, Go, Ruby, Python, C++, and Rust.\nIt\u0026rsquo;s interesting to cluster these programs based on the comparisons. A significant chunk of data projects especially in the Apache Software Foundation directory is written in Java.\nJava is to the data ecosystem as Wordpress is to the content management and web development ecosystem.\nAnyways, back on topic. A peek into the Rust vs Go performance shows that Rust fares pretty well!\nThe comparison between Rust and Go in terms of performance shows that Rust consistently outperforms Go by at least 30% in most cases, with some instances like the binary tree reaching as much as 12 times faster than Go.\nBeyond benchmarks, practical applications are a great source to see if the benefits are real. In early 2020 Discord announced that they were switching form Go to Rust. Their blog is consistent with the benchmarks.\nBesides benchmarks, it is helpful to consider how each of these languages deal with concurrency, scalability, and memory safety.\nConcurrency Go approach to concurrency and parallelism emphasizes goroutines and channels. Goroutines are lightweight execution units managed by the Go runtime, requiring fewer resources than traditional threads. This enables developers to easily create thousands or even millions of goroutines for highly concurrent and scalable applications.\nChannels in Go provide a safe means of communication between goroutines, minimizing boilerplate and synchronization overhead when writing concurrent code. The select statement allows for the simultaneous handling of multiple channels, further simplifying parallel programming.\nRust offers a unique combination of threads, channels, and asynchronous programming to facilitate concurrency and parallelism. Rust\u0026rsquo;s threads are similar to those in other languages, enabling multiple tasks to run concurrently. However, Rust\u0026rsquo;s channels provide an additional layer of safety when communicating between threads, preventing data races and other synchronization issues.\nRust also supports asynchronous programming through its async/await syntax, which enables non-blocking I/O and efficient handling of tasks that may take a long time to complete. The Rust async ecosystem, including popular libraries like async_std and Tokio, provides powerful tools for building high-performance, concurrent applications.\nRust is the better choice if the goal is to have fine grained control over concurrency and parallelism. And distributed systems are about concurrency and parallelism.\nScalability Both languages excel at harnessing the power of multiple CPU cores to process data in parallel. In Go, you can leverage Goroutines to handle each piece of data and use a WaitGroup to synchronize their completion.\nMeanwhile, Rust offers an elegant solution through the rayon crate, which simplifies parallel iteration over containers.\nMemory Safety In a guest post on the Bitbucket blog, the author calls Rust\u0026rsquo;s compiler very strict and pedantic! This is a key differentiator for Rust. Rust\u0026rsquo;s unwavering commitment to memory safety means that you won\u0026rsquo;t encounter buffer overflows or race conditions, thanks to the language\u0026rsquo;s rigorous enforcement of safe programming practices. While this provides significant benefits in terms of stability and reliability, it also requires developers to be meticulous about memory management when writing code. In other words, Rust\u0026rsquo;s safety features are a double-edged sword - while they offer great protection against common errors, they can also make coding more challenging due to the need for constant attention to detail regarding memory allocation.\nGo shares some similarities with Rust when it comes to memory safety, but it falls short of Rust\u0026rsquo;s level of rigor. While Go does block dangling pointers and prevents memory leaks to some extent, its garbage collection mechanism doesn\u0026rsquo;t quite match the depth of Rust\u0026rsquo;s ownership and borrowing system. In Go, various data structures, such as interfaces and slices, are typically implemented using non-atomic multi-word structs. However, this can lead to issues with data races, where invalid values may be produced because of memory corruption.\nDeveloper Experience The developer experience is perhaps the oddest category of comparison.\nWhile Rust is the most admired language, more than 80% of developers that use it want to use it again next year. This category was updated in 2023. As of 2022, Rust was on its seventh year as the most loved language with 87% of developers saying they want to continue using it. Rust has been crushing the metric of retention since majority of the developers have been wanting to continue using the language. There is a major caveat to the survey results.\nDespite extensive documentation and amazing level of detail in error messages, Rust is considered by many folks to be a tough language to get started with.\nIn terms of Developer Experience, syntax simplicity or compile times are areas where Go is preferred over Rust.\nThe development cycles and iterations are also faster in Go compared to Rust. It is easier for teams with a mix of people working on different project to work with Go in terms of simplicity of syntax.\nThe Rust compiler is known for it\u0026rsquo;s error handling. It is trickier for developers to compile Rust code and they need to have a decent understanding of the design patterns of the language. This may be initially an uphill climb, but Rust developers are more confident of working code that compiles for the same reason.\nThere is not a clear winner here. It depends on the team composition, and project complexity. Go is easier for teams to start and iterate with. Rust is hard for beginners, but iterations become progressively easier.\nWhy we chose Rust to build Fluvio For this section, I asked Sehyo our CTO the question - Why did we choose Rust? And the following is what I captured in conversation:\nWe were building a data streaming runtime with a goal of building stateful stream processing to offer a better alternative to Kafka and Flink in a single platform.\nWe needed to pick a language that will enable us to build the building blocks of a distributed system that will be edge and cloud native and deliver high performance in data processing at scale.\nWe needed to write high performance code but didn’t want to use C++.\nAs data platform builders, we were always thinking about garbage in garbage out. And we liked the fact that we did not have to write any garbage collection logic!\nThere are no free lunches and we were willing to front load the development work needed to write high quality and performance code before mucking around with abstractions and integrations.\nRust offered:\nHigh performance. Excellent documentation and error handling. Good tooling infrastructure with Cargo and Crates. Every technical decision has trade-offs. We had to go through the learning curve and embrace the development experience in Rust. It has been well worth it for us.\nOur platform is for software engineers and developers who are ready to embrace the power of Rust and Web Assembly.\nWe support Go, Python, and Typescript clients, and our domain specific language construct is YAML.\nWe have over indexed building the primitives and abstractions to deliberately build a system that is built for data streaming in the wasm paradigm.\nWhy? Because we are convinced that the future of data centric software is on Rust and WASM. It\u0026rsquo;s our all-in bet. The stakes are high. The odds are dynamically changing. But we are all-in on Rust and Web Assembly to enable data engineering.\nConclusion Hopefully, this article won\u0026rsquo;t cause some irreversible damage to your decision-making process and bias you towards Rust over Go as a programming language. Remember there are a pros and cons of every tool.\nGo and Rust have distinct strengths and tradeoffs Go shines with its Goroutine-based architecture when it comes to building web APIs and small services. Rust excels in high-performance computing tasks, such as data processing and algorithm execution.\nRust\u0026rsquo;s focus on safety and memory management pays off, making it a better choice for projects that require top-notch security and performance.\nData processing at scale is all about high performance computing and algorithm execution. With the steady increase in the demand for artificial intelligence applications and the rise of data products, data as a service, we have to improve the unit economics of developing data intensive solutions.\nThe economic aspects of the equation at scale are huge. An article in the AWS Open Source Blog has already discussed the sustainability issues that needs to be addressed at scale.\nRust is a language that is currently best positioned to deliver the goods to the data ecosystem.\nLet us know your take on our social channels below.\nConnect with us: Please, be sure to join our Discord server if you want to talk to us or have questions.\nSubscribe to our YouTube channel\nFollow us on Twitter\nFollow us on LinkedIn\nTry InfinyOn Cloud a fully managed Fluvio service\n","description":"Which modern programming language between Rust and Go is better suited to power the data ecosystem.","keywords":null,"summary":"Content Rust or Go Go Rust or go home Teleological argument Advantages Performance Concurrency Scalability Memory Safety Developer Experience Why we chose Rust to build Fluvio Conclusion Rust or Go Someone on the data engineering subReddit recently posted a discussion starter with the title - \u0026ldquo;Rust or Go.\u0026rdquo; The author wanted the community\u0026rsquo;s recommendation on picking up a new programming language for data engineering. Like every dichotomy, this is a simple question at the surface, but the answer to the question is complex.","title":"Rust vs Go - Which programming language will power the data ecosystem?","url":"http://localhost:1315/blog/2023/09/rust-or-Go/"},{"body":"Nothing is more exhilarating than watching the github stars and forks go up on a newly launched github project. But constantly clicking on stars and forks tends to grow old. If you want to get notified in Slack or Discord anytime you receive a new star 🌟 or fork 🎏, this blog is for you!\nThis blog is a step-by-step tutorial on how to create a data pipeline that watches github for changes and notifies you in Slack or Discord.\nThe blog has an Advanced Topics optional section. There, we\u0026rsquo;ll show you how to build your own stars/forks SmartModule instead of using the one from the Hub.\nLet\u0026rsquo;s get started.\nRequirements This blog is a step-by-step tutorial producing a fully functional InfinyOn Cloud data pipeline. If you want to follow along, there are there are a few prerequisites:\nFluvio CLI running on your local machine. An account on InfinyOn Cloud. Admin access to Slack or Discord to generate a webhook API key. Optional: If you want to access Github more often, you\u0026rsquo;ll need to generate a token, which also requires admin access.\nCreate a Data Pipeline In fluvio, a data pipeline intermediating data exchanges between services requires a source and a sink connector. In our use case, we\u0026rsquo;ll need a source connector that periodically queries github and write to a topic and a sink connector that reads from the topic and notifies github/discord when it detects a change. So, we\u0026rsquo;ll tackle this problem in two parts:\nBuild a Github Producer Build a Slack Consumer Finally, we\u0026rsquo;ll build a Discord Consumer, which is virtually identical to the Slack consumer.\nBuild a Github Producer Source connectors are responsible for reading from external services and publishing the results to a fluvio topic. For our use case, we\u0026rsquo;ll use the http-source connector.\nBefore we start, let\u0026rsquo;s examine the data we get from github and determine what we want to write to the topic.\n%copy first-line%\n$ curl https://api.github.com/repos/infinyon/fluvio { \u0026#34;id\u0026#34;: 205473061, \u0026#34;node_id\u0026#34;: \u0026#34;MDEwOlJlcG9zaXRvcnkyMDU0NzMwNjE=\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;fluvio\u0026#34;, \u0026#34;full_name\u0026#34;: \u0026#34;infinyon/fluvio\u0026#34;, \u0026#34;owner\u0026#34;: { \u0026#34;login\u0026#34;: \u0026#34;infinyon\u0026#34;, \u0026#34;id\u0026#34;: 52172389, ... }, ... \u0026#34;stargazers_count\u0026#34;: 1754, \u0026#34;forks_count\u0026#34;: 137, ... } The query response returns a lot of data, but we only need a couple of fields, so we want to:\nDiscard all fields other than stargazers_count and forks_count. Rename the fields to stars and forks, for better readability. On each query response, we write an event into the data stream:\n{ \u0026#34;stars\u0026#34;: 1890, \u0026#34;forks\u0026#34;: 142 } { \u0026#34;stars\u0026#34;: 1891, \u0026#34;forks\u0026#34;: 143 } Define the http-source Smart Connector At InfinyOn, we call the connectors Smart to emphasize that they accept transformations. Let\u0026rsquo;s go over to the http-source connector to see how to set things up.\nA quick look at the https-source connector documentation shows that the connector manages the request and response and hands off the data to transformation to smartmodules for additional processing. Quick note for beginners; SmartModules are custom programs compiled to WebAssembly that shape data before sending it to the data stream.\nAt first, sight, performing data transformations may feel like a lot of work, but in reality, it\u0026rsquo;s not too bad. The InfinyOn team built several smartmodules that we can download from Smartmodule Hub and use DSL to perform most of the work. For complex transformations, we can develop our own smartmodules and publish them to the Hub for sharing. The cool part is that we can mix and match public and custom smartmodules and seldom need to start from scratch.\nOk then, open an editor, create the following configuration file, and name it github.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: github-stars-inbound type: http-source topic: github-stars http: endpoint: \u0026#39;https://api.github.com/repos/infinyon/fluvio\u0026#39; method: GET interval: 300s transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;stargazers_count\u0026#34;: \u0026#34;stars\u0026#34; \u0026#34;forks_count\u0026#34;: \u0026#34;forks\u0026#34; Below is a brief description of the most relevant fields:\nmeta - for connector and data streaming information type - connector type topic - the target data streaming topic http - for connector-specific configurations endpiont - endpoint to query interval - the frequency of the query 5 minutes to avoid the github rate-limit threshold (without a token) transforms for smarmodule definitions jolt - to express json transformations in DSL. The connector users jolt smartmodule for the transformation. As jolt is available in the Hub, we need to download it to the cluster:\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 downloading infinyon/jolt@0.3.0 to infinyon-jolt-0.3.0.ipkg ... downloading complete Great, we are ready to provision our first connector.\nRun the connector in InfinyOn Cloud We need Fluvio CLI to provision the connector in InfinyOn Cloud:\n%copy first-line%\n$ fluvio cloud connector create -c github.yaml connector \u0026#34;github-stars-inbound\u0026#34; (http-source) created Let\u0026rsquo;s check the health of the connector:\n%copy first-line%\n$ fluvio cloud connector list NAME TYPE VERSION CDK STATUS LOG-LEVEL github-stars-inbound http-source 0.2.5 V3 Running info The connector invokes a get request that generates a record, which it sends to jolt for the transformation, and it writes the result to the github-stars topic.\nLet\u0026rsquo;s check what\u0026rsquo;s in the topic:\n%copy first-line%\n$ fluvio consume github-stars -B Consuming records from \u0026#39;github-stars\u0026#39; starting from the beginning of log {\u0026#34;stars\u0026#34;:1890,\u0026#34;forks\u0026#34;:143} ⠂ -\u0026gt; Note: Fork and star counts are updated at 5-minute intervals. There is a way to increase the read frequency with a github token, which we\u0026rsquo;ll describe below.\nGreat the producer is up and running, it\u0026rsquo;s time to build the sonsumer.\nBuild a Slack Consumer The Slack consumer reads from the data stream, detects if there is a change in the number of forks or stars, and pushes a notification to Slack. We\u0026rsquo;ll use the [http-sink] connector and a couple of smartmodules to do the job.\nInfinyOn offers several Smartmodule that operate generic json fields. However, we want to build cusotm logic that generates a string with emojis for our Slack channel. So, let\u0026rsquo;s go ahead and build our Smartmodule one.\nDownload the stars/forks smartmodule from Hub Download the pre-built version from the Hub. If you prefer to build your own, checkout the Advanced Topics section:\n%copy first-line%\n$ fluvio hub sm download infinyon-labs/stars-forks-changes@0.1.2 %copy first-line%\n$ fluvio sm list SMARTMODULE SIZE infinyon/jolt@0.3.0 612.7 KB infinyon-labs/stars-forks-changes@0.1.2 52.4 KB This smartmodule is downloaded from infinyon-labs, used for tutorial projects. InfinyOn supports multiple global identifiers, including custom ones, enabling you to build and publish your own smartmodules.\nSetup Slack Webhook The last piece of the puzzle is the Slack webhook. Check out Incoming Webhooks on Slack on how to set it up.\nSlack requries that we send the notification in a specific format:\nPOST https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Content-type: application/json { \u0026#34;text\u0026#34;: \u0026#34;Hello, world.\u0026#34; } In this example we\u0026rsquo;ll chain stars-forks-changes smartmodule with jolt to rename the result to text as required by Slack syntax.\nDefine the http-sink Smart Connector Create the following configuration file, and name it slack.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: slack-stars-outbound type: http-sink topic: github-stars secrets: - name: SLACK_TOKEN http: endpoint: \u0026#34;https://hooks.slack.com/services/${{ secrets.SLACK_TOKEN }}\u0026#34; headers: - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon-labs/stars-forks-changes@0.1.2 lookback: last: 1 - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;result\u0026#34;: \u0026#34;text\u0026#34; Next, we\u0026rsquo;ll need to upload SLACK_TOKEN to InfinyOn Cloud.\nAdd your Slack secret to InfinyOn Cloud InfinyOn Cloud implements a secure vault for storing and referencing secrets.\nGrab your slack webhook token to write it to InfinyOn Cloud:\n%copy first-line%\n$ fluvio cloud secret set SLACK_TOKEN T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Run the connector in InfinyOn Cloud Let\u0026rsquo;s run the connector:\n%copy first-line%\n$ fluvio cloud connector create -c slack.yaml connector \u0026#34;slack-stars-outbound\u0026#34; (http-sink) created Check the logs to ensure it has been provisioned:\n%copy first-line%\n$ fluvio cloud connector logs slack-stars-outbound Your end-to-end pipeline up and running;\nthe sink connector reads the last record from github-stars the smartmdule that detects changes and formats the output for Slack Let\u0026rsquo;s produce a fake record to test it:\n%copy first-line%\n$ fluvio produce github-stars \u0026gt; {\u0026#34;stars\u0026#34;:1891,\u0026#34;forks\u0026#34;:144} Ok Awesome! The Slack bot sent us an alert:\nmy-stars-bot \u0026lt;APP\u0026gt; 12:35 PM :flags: 144 :star2: 1891``` \u0026#x1f389; Congratulations!\nThe following section focuses on Discord.\nBuild a Discord Consumer In fluvio, you can deploy multiple connectors reading form the same stream in parallel. Let\u0026rsquo;s create a Discord connector tat reads from the same topic and notifies Discord. Check out Discord Webhooks on how to create one.\nCreate the following configuration file, and name it discord.yaml :\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: discord-stars-outbound type: http-sink topic: github-stars secrets: - name: DISCORD_TOKEN http: endpoint: \u0026#34;https://discord.com/api/webhooks/${{ secrets.DISCORD_TOKEN }}\u0026#34; headers: - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon-labs/stars-forks-changes@0.1.2 lookback: last: 1 - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;result\u0026#34;: \u0026#34;content\u0026#34; This configuration is similar to the Slack configuration, except for the name, endpoint and jolt transformation. Discords expects the json field to be named content instead of text.\nAdd your discord webhook token to InfinyOn Cloud:\n%copy first-line%\n$ fluvio cloud secret set DISCORD_TOKEN \u0026lt;webhook-token\u0026gt; Start the connector:\n%copy first-line%\n$ fluvio cloud connector create -c discord.yaml connector \u0026#34;discord-stars-outbound\u0026#34; (http-sink) created Your end-to-end pipeline is up and running.\nLet\u0026rsquo;s produce a fake record to test it:\n%copy first-line%\n$ fluvio produce github-stars \u0026gt; {\u0026#34;stars\u0026#34;:1892,\u0026#34;forks\u0026#34;:145} Ok If the webhooks has been configured correctly the alert is now published to Discord.\n\u0026#x1f389; Congratulations! You\u0026rsquo;ve created two end-to-end data pipelines that are continuously monitoring github for changes.\nAdvanced Topics In this section, we\u0026rsquo;ll cover a couple of advanced topics. The first one is catered toward developers and provides instructions on how to build your own smartmodule for start/forks. The second shows you how to apply a Github Token to increase the read frequency from github.\nBuild a custom stars/forks smartmodule We\u0026rsquo;ll use Smartmodule Development Kit (smdk) to build, test, and deploy our smartmodule to the cluster.\nInstall smkd SMDK is a separate CLI for Smartmodule developers, let\u0026rsquo;s install it:\n%copy first-line%\n$ fluvio install smdk Generate a new project Run the generator to create a filter-map smartmodule called stars-forks-changes:\n%copy first-line%\n$ smdk generate stars-forks-changes Generating new SmartModule project: stars-forks-changes ✔ 🤷 Will your SmartModule use init parameters? · false ✔ 🤷 Will your SmartModule be public? · false ✔ 🤷 Which type of SmartModule would you like? · filter-map [1/7] Done: .gitignore [2/7] Done: Cargo.toml [3/7] Done: README.md [4/7] Done: SmartModule.toml [5/7] Done: rust-toolchain.toml [6/7] Done: src/lib.rs [7/7] Done: src Note You may be prompted to add a group a globally unique namespace that you\u0026rsquo;ll need to load to cluster and publish to Smartmodule Hub. In this blog we\u0026rsquo;ll skip publishing to Hub.\nNavigate to the project directory:\n%copy first-line%\n$ cd stars-forks-changes Add the custom logic Replace the content of src/lib.rs with our custom logic:\n%copy%\nuse std::sync::atomic::{AtomicU32, Ordering}; use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; use serde::{Deserialize, Serialize}; // use u32 to represent the metric type Metric = u32; type AtomicMetric = AtomicU32; /// Incoming record from Github #[derive(Default, Deserialize)] struct GithubRecord { stars: Metric, forks: Metric, } /// Outgoing record #[derive(Default, Serialize)] struct GithubOutgoing { result: String, } /// Accumulator for stars and forks static STARS_FORKS: StarsForks = StarsForks::new(); /// Use Atomic to update internal state #[derive(Default, Debug, Deserialize)] struct StarsForks { stars: AtomicMetric, forks: AtomicMetric, } impl StarsForks { const fn new() -\u0026gt; Self { Self { stars: AtomicMetric::new(0), forks: AtomicMetric::new(0), } } fn get_stars(\u0026amp;self) -\u0026gt; Metric { self.stars.load(Ordering::SeqCst) } fn set_stars(\u0026amp;self, new: Metric) { self.stars.store(new, Ordering::SeqCst); } fn get_forks(\u0026amp;self) -\u0026gt; Metric { self.forks.load(Ordering::SeqCst) } fn set_forks(\u0026amp;self, new: Metric) { self.forks.store(new, Ordering::SeqCst); } fn set_both(\u0026amp;self, github_record: GithubRecord) { self.set_stars(github_record.stars); self.set_forks(github_record.forks); } // generate emoji string based on the new stars and forks fn update_and_generate_moji_string(\u0026amp;self, new: \u0026amp;GithubRecord) -\u0026gt; Option\u0026lt;GithubOutgoing\u0026gt; { let current_stars = self.get_stars(); let current_forks = self.get_forks(); if new.stars != current_stars \u0026amp;\u0026amp; new.forks != current_forks { // if both stars and forks are changed, generate new emoji on prev stats let emoji = GithubOutgoing { result: format!(\u0026#34;:flags: {} \\n:star2: {}\u0026#34;, new.forks, new.stars), }; self.set_forks(new.forks); self.set_stars(new.stars); Some(emoji) } else if new.forks != current_forks { // if only forks are changed, generate new emoji on prev stats let emoji = GithubOutgoing { result: format!(\u0026#34;:flags: {}\u0026#34;, new.forks), }; self.set_forks(new.forks); Some(emoji) } else if new.stars != current_stars { let emoji = GithubOutgoing { result: format!(\u0026#34;:star2: {}\u0026#34;, new.stars), }; self.set_stars(new.stars); Some(emoji) } else { // no changes None } } } #[smartmodule(look_back)] pub fn look_back(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;()\u0026gt; { let last_value: GithubRecord = serde_json::from_slice(record.value.as_ref())?; STARS_FORKS.set_both(last_value); Ok(()) } #[smartmodule(filter_map)] pub fn filter_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Option\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { let new_data: GithubRecord = serde_json::from_slice(record.value.as_ref())?; if let Some(emoji) = STARS_FORKS.update_and_generate_moji_string(\u0026amp;new_data) { let output = serde_json::to_vec(\u0026amp;emoji)?; Ok(Some((record.key.clone(), output.into()))) } else { Ok(None) } } The code reads the github records from the data stream, builds an accumulator, and generates a formatted string if the number of stars or forks has changed. It also uses a look_back API to initalize the internal state from the last value.\nThis project is also available for download on github.\n%copy first-line%\n$ smdk build Let\u0026rsquo;s do a quick test; save the following in a file test-data.txt:\n%copy%\n{\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1890} {\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1890} {\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1890} {\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1889} {\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1889} {\u0026#34;forks\u0026#34;:144,\u0026#34;stars\u0026#34;:1889} {\u0026#34;forks\u0026#34;:144,\u0026#34;stars\u0026#34;:1889} {\u0026#34;forks\u0026#34;:145,\u0026#34;stars\u0026#34;:1890} {\u0026#34;forks\u0026#34;:146,\u0026#34;stars\u0026#34;:1890} {\u0026#34;forks\u0026#34;:146,\u0026#34;stars\u0026#34;:1891} Run the test:\n%copy first-line%\n$ smdk test --file ./test-data.txt --lookback-last 1 --record \u0026#39;{\u0026#34;forks\u0026#34;:143,\u0026#34;stars\u0026#34;:1890}\u0026#39; {\u0026#34;result\u0026#34;:\u0026#34;:star2: 1889\u0026#34;} {\u0026#34;result\u0026#34;:\u0026#34;:flags: 144\u0026#34;} {\u0026#34;result\u0026#34;:\u0026#34;:flags: 145 \\n:star2: 1890\u0026#34;} {\u0026#34;result\u0026#34;:\u0026#34;:flags: 146\u0026#34;} {\u0026#34;result\u0026#34;:\u0026#34;:star2: 1891\u0026#34;} Great! Our smartmodule produces a json record result: \u0026ldquo;\u0026hellip;\u0026rdquo; on changes, and ignores everything else.\nLoad the smartmodule to the cluster:\n%copy first-line%\n$ smdk load Loading package at: ~/stars-forks-changes Found SmartModule package: stars-forks-changes loading module at: ~/stars-forks-changes/target/wasm32-unknown-unknown/release-lto/stars_forks_changes.wasm Trying connection to fluvio router.infinyon.cloud:9003 Creating SmartModule: stars-forks-changes Let\u0026rsquo;s double-check that the smartmodule is indeed on the cluster:\n%copy first-line%\n$ fluvio sm list SMARTMODULE SIZE infinyon/jolt@0.3.0 612.7 KB \u0026lt;group\u0026gt;/stars-forks-changes@0.1.0 52.4 KB \u0026#x1f389; Congratulations, you built your first smartmodule! We\u0026rsquo;ll leave it as an exercise to replace the pre-built smartmodule with your own. It\u0026rsquo;s simple: delete the existing connector and re-create it with the updated configuration file.\nIncrease github refresh interval (optional) Github rate-limit can be extended from 60 to 5000 queries per hour by creating an application token. Check out github documentation on Access Tokens.\nLet\u0026rsquo;s update the github.yaml configuration with the access token:\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.5 name: github-stars-inbound type: http-source topic: github-stars secrets: - name: GITHUB_TOKEN http: endpoint: \u0026#39;https://api.github.com/repos/infinyon/fluvio\u0026#39; method: GET headers: - \u0026#39;Authorization: token ${{ secrets.GITHUB_TOKEN }}\u0026#39; interval: 30s transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: \u0026#34;stargazers_count\u0026#34;: \u0026#34;stars\u0026#34; \u0026#34;forks_count\u0026#34;: \u0026#34;forks\u0026#34; Note, with the access token, we can increse the query interval to 30 seconds.\nAdd the access token to InfinyOn Cloud :\n%copy first-line%\n$ fluvio cloud secret set GITHUB_TOKEN \u0026lt;access-token\u0026gt; To refresh, let\u0026rsquo;s delete \u0026amp; create the connector again:\n%copy first-line%\n$ fluvio cloud connector delete github-stars-inbound %copy first-line%\n$ fluvio cloud connector create -c github.yaml Conclusion This blog taught us how to build data pipelines with Fluvio via InfinyOn Cloud. Now that we built and published all the smartmodules, creating a new pipeline that reads external sources and notifies Slack or Discord is a 2 step process:\nCreate and run the http-source connector Create and run the http-sink connector All the data magic is done via smartmodules. As your custom smartmodule library grows, adding new data pipelines becomes a trivial exercise.\n","description":"Create a data pipeline that notifies you in Slack or Discord anytime we get a new star or fork.","keywords":null,"summary":"Nothing is more exhilarating than watching the github stars and forks go up on a newly launched github project. But constantly clicking on stars and forks tends to grow old. If you want to get notified in Slack or Discord anytime you receive a new star 🌟 or fork 🎏, this blog is for you!\nThis blog is a step-by-step tutorial on how to create a data pipeline that watches github for changes and notifies you in Slack or Discord.","title":"Create a Github Stars/Forks Event Pipeline","url":"http://localhost:1315/blog/2023/09/github-stars-to-slack/"},{"body":"Introduction We are excited to announce the release of the Motherduck/DuckDB connector for InfinyOn Cloud. This connector lets you to stream data from the InfinyOn cloud to MotherDuck in real time. MotherDuck is a cloud data analytics platform powered by DuckDB, an open-source OLAP engine. InfinyOn Cloud is the next generation of data streaming platform allowing anyone to connect, transform and dispatch data to anywhere. InfinyOn Cloud has built-in connectors that can connect to various data sources such as HTTP, MQTT, and SQL. Using a combination of InfinyOn and MotherDuck, you can build complete real-time data analytics solutions for use cases such as fraud detection, inventory management, and a real-time recommendation engine.\nThis blog post will show you how to use InfinyOn Cloud to stream data to MotherDuck. We will use the MQTT connector to stream data from Helsinki transit data and transform data suitable for MotherDuck. You can stream data from InfinyOn Cloud to MotherDuck without any additional steps.\nPrerequisite This blog assumes that you have an InfinyOn Cloud account. You can sign-up for InfinyOn Cloud if you don\u0026rsquo;t have one. You must also install Fluvio CLI on your local machine. InfinyOn cloud allows you to run managed fluvio cluster on the cloud. You can use Fluvio CLI to interact with it or a built-in UI dashboard to monitor ongoing activities. At this time, please log in to InfinyOn Cloud and set up the default cluster.\nDemo Scenario In this example, we will consume live data from transit vehicles in Helsinki, Finland. The city publishes real-time metrics such as speed, acceleration, route, etc., and makes this data publicly available via MQTT at mqtt://mqtt.hsl.fi. The data is in JSON format. For more information, please see Helsinki City\u0026rsquo;s MQTT documentation.\nThe demo pipeline consists of the following:\nUse MQTT connector to stream data from Helsinki MQTT endpoint to Fluvio topic as JSON. The DuckDB connector will trim and transform JSON data and insert SQL records to MotherDuck/DuckDB. Use MotherDuck/DuckDB to perform SQL analysis on the data. Creating a Fluvio topic to store MQTT data We will use a fluvio topic to stream MQTT data from Helsinki City. The fluvio topic is an immutable store of events. Helsinki MQTT stream is a high-volume stream. So we will use a short retention time of 2 hours to ensure data fit into the default topic volume quota.\n%copy first-line%\n$ fluvio topic create helsinki --retention-time 2h Starting MQTT connector Create the following configuration file conn-mqtt.yaml on your local directory, which specific the MQTT connector configuration:\n%copy%\napiVersion: 0.1.0 meta: version: 0.2.3 name: helsinki-mqtt type: mqtt-source topic: helsinki mqtt: url: \u0026#34;mqtt://mqtt.hsl.fi\u0026#34; topic: \u0026#34;/hfp/v2/journey/ongoing/vp/+/+/+/#\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json Then use fluvio CLI to start the connector:\n%copy first-line%\n$ fluvio cloud connector create --config conn-mqtt.yaml This will start the MQTT connector and connect to the Helsinki MQTT broker. It will subscribe to the topic /hfp/v2/journey/ongoing/vp/+/+/+/# and publish the data to the Fluvio topic helsinki.\nYou can verify that the connector is running by running the following command to see the latest MQTT data stream into Fluvio topic helsinki. You can use the following command to see live data.\n%copy first-line%\n$ fluvio consume helsinki Consuming records from \u0026#39;helsinki\u0026#39; {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/bus/0012/02244/1098/1/Rastila(M)/19:03/1453126/5/60;25/20/07/96\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;VP\u0026#34;:{\u0026#34;desi\u0026#34;:\u0026#34;98\u0026#34;,\u0026#34;dir\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;oper\u0026#34;:12,\u0026#34;veh\u0026#34;:2244,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T17:00:15.231Z\u0026#34;,\u0026#34;tsi\u0026#34;:1675357215,\u0026#34;spd\u0026#34;:0.0,\u0026#34;hdg\u0026#34;:244,\u0026#34;lat\u0026#34;:60.209415,\u0026#34;long\u0026#34;:25.076423,\u0026#34;acc\u0026#34;:0.0,\u0026#34;dl\u0026#34;:179,\u0026#34;odo\u0026#34;:90,\u0026#34;drst\u0026#34;:0,\u0026#34;oday\u0026#34;:\u0026#34;2023-02-02\u0026#34;,\u0026#34;jrn\u0026#34;:304,\u0026#34;line\u0026#34;:145,\u0026#34;start\u0026#34;:\u0026#34;19:03\u0026#34;,\u0026#34;loc\u0026#34;:\u0026#34;GPS\u0026#34;,\u0026#34;stop\u0026#34;:1453126,\u0026#34;route\u0026#34;:\u0026#34;1098\u0026#34;,\u0026#34;occu\u0026#34;:0}}} ..... If you are using InfinyOn Cloud, check out the Dashboard:\nThe Dashboard should show that InfinyOn Cloud is processing lots of data.\nNote that this connector does not perform any transformation or filtering. It simply streams the data from the MQTT broker to the Fluvio topic. We will use the MotherDuck connector to transform the data and insert it into MotherDuck/DuckDB.\nStarting MotherDuck/DuckDB connector. Make sure you have MotherDuck account and request an API Token. The API Token is needed by the MotherDuck connector to authenticate with MotherDuck.\nBefore we start the MotherDuck connector, we need to create a table in MotherDuck to receive data from InfinyOn Cloud. Use MotherDuck UI or DuckDB CLI. Please run this command to create a table:\n%copy first-line%\ncreate table speed( lat float, long float, vehicle integer, speed float, time timestamp ); Ensure you have successfully created the table by running the following command:\n%copy first-line%\nselect count(*) from speed; You can also use the same command to see the progress of the data being inserted into the table.\nCreate secrets to store token MotherDuck connector needs to authenticate with MotherDuck. We can securely store the authentication token using InfinyOn cloud\u0026rsquo;s secret store. To create a secret, run the following command:\n%copy first-line%\n$ fluvio cloud secret set MD_TOKEN \u0026lt;token_value\u0026gt; Where \u0026lt;token_value\u0026gt; is the token value you get from MotherDuck. The connector will use the token name MD_TOKEN to retrieve the token value.\nDownloading SmartModules into InfinyOn Cloud MotherDuck connector uses SmartModules to transform the data. SmartModules are reusable data transformation components that can transform data in real time. In this demo, we download two SmartModules, jolt and json-sql, from the InfinyOn Hub. The Hub is a central repository of SmartModules.\nThe jolt SmartModule transforms JSON data into another JSON data. The raw MQTT JSON from Helsinki transit is complex. In this scenario, we only need small subsets of the data. The transformation step will pick a few fields in the nested object and flatten them out to simplify the downstream transformation.\nYou can download the jolt SmartModule using the CLI:\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 The json-sql SmartModule is used to transform a JSON into SQL data which can be inserted into MotherDuck/DuckDB.\n%copy first-line%\n$ fluvio hub sm download infinyon/json-sql@0.2.1 Creating a MotherDuck connector Similar to the MQTT connector, create the configuration file conn-md.yaml on your local directory:\n%copy%\napiVersion: 0.1.0 meta: version: 0.1.0 name: md-helsinki type: duckdb-sink topic: helsinki secrets: - name: MD_TOKEN duckdb: url: \u0026#34;md:?token=${{ secrets.MD_TOKEN }}\u0026#34; transforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: payload: VP: lat: \u0026#34;lat\u0026#34; long: \u0026#34;long\u0026#34; veh: \u0026#34;vehicle\u0026#34; route: \u0026#34;route\u0026#34; spd: \u0026#34;speed\u0026#34; tst: \u0026#34;tst\u0026#34; - uses: infinyon/json-sql@0.2.1 with: mapping: table: \u0026#34;speed\u0026#34; map-columns: \u0026#34;lat\u0026#34;: json-key: \u0026#34;lat\u0026#34; value: type: \u0026#34;float\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;long\u0026#34;: json-key: \u0026#34;long\u0026#34; value: type: \u0026#34;float\u0026#34; required: true \u0026#34;vehicle\u0026#34;: json-key: \u0026#34;vehicle\u0026#34; value: type: \u0026#34;int\u0026#34; required: true \u0026#34;speed\u0026#34;: json-key: \u0026#34;speed\u0026#34; value: type: \u0026#34;float\u0026#34; required: true \u0026#34;time\u0026#34;: json-key: \u0026#34;tst\u0026#34; value: type: \u0026#34;timestamp\u0026#34; required: true To start the connector, run the following command in the local directory:\n%copy first-line%\n$ fluvio cloud connector create --config conn-md.yaml To check the status of the connector Once both connectors are running, you can check the status of the connector by running the following command:\n%copy first-line%\n$ fluvio cloud connector list You can also follow the progress of how many records are inserted into MotherDuck/DuckDB by running the following command:\n%copy first-line%\nselect count(*) from speed; Performing analytics With data flowing into MotherDuck, you can perform any analytics using SQL. For example, we can compute the average speed of vehicles by running this on the MotherDuck UI query tool or DuckDB CLI:\n%copy first-line%\nselect vehicle, avg(speed) from speed group by vehicle; ┌─────────┬────────────────────┐ │ vehicle │ avg(speed) │ │ int32 │ double │ ├─────────┼────────────────────┤ │ 1407 │ 5.878316847404631 │ │ 1823 │ 12.250563307966985 │ │ 2244 │ 4.29509804763046 │ │ 1334 │ 3.4598148077450417 │ │ 1606 │ 5.9982051315725355 │ │ 1347 │ 2.4579999693802423 │ │ 1211 │ 5.198823541402817 │ │ 25 │ 10.653684204542323 │ │ 1156 │ 5.878526310583479 │ │ 1382 │ 8.446666672116233 │ │ 1173 │ 12.593883457005893 │ │ 1170 │ 4.158035727151271 │ │ 1831 │ 8.553492042753431 │ │ 2215 │ 9.572083353996277 │ │ 1391 │ 0.8761818246407942 │ │ 285 │ 2.562727277929133 │ │ 1410 │ 9.533480930366094 │ │ 1037 │ 11.678289494558907 │ │ 1114 │ 5.06574714663385 │ │ 1534 │ 8.432022506936212 │ │ · │ · │ │ · │ · │ │ · │ · │ │ 790 │ 8.17142847606114 │ │ 964 │ 8.327142868723188 │ │ 1503 │ 16.708571434020996 │ │ 1602 │ 8.161858405687113 │ │ 6326 │ 29.88338432312012 │ │ 1049 │ 12.826333268483479 │ │ 454 │ 3.5763491903032576 │ │ 1343 │ 0.9916250079870224 │ │ 320 │ 17.509583353996277 │ │ 266 │ 3.7031999796628954 │ │ 1023 │ 7.556637190084542 │ │ 418 │ 1.9262499809265137 │ │ 1900 │ 8.037222094006008 │ │ 1916 │ 5.429999947547913 │ │ 921 │ 22.95133336385091 │ │ 2208 │ 11.416666603088379 │ │ 1103 │ 11.645714351109095 │ │ 1535 │ 12.809999942779541 │ │ 988 │ 3.474313719599855 │ │ 1341 │ 9.315308665787732 │ ├─────────┴────────────────────┤ │ 920 rows (40 shown) │ └──────────────────────────────┘ The current version of the connector provides default mapping of JSON data to SQL, such as int, float, string, and timestamp. For detailed configuration parameters, please see details of MotherDuck connector properties.\nClean-up This demo transforms quite a bit of traffic in real-time, which will rapidly consume your free InfinyOn Cloud credits. After completing the demo, please delete the connectors and topic to avoid unnecessary charges.\n%copy first-line%\n$ fluvio cloud connector delete helsinki-mqtt md-helsinki Conclusion In this blog post, we demonstrated that building real-time OLAP analytics solutions can be done easily and quickly using InfinyOn Cloud and MotherDuck. We can\u0026rsquo;t wait for you to try and create your real-time analytics solution.\nFurther reading Handling XML data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly ","description":"Using InfinyOn DuckDB/MotherDuck connector to send data to MotherDuck","keywords":null,"summary":"Introduction We are excited to announce the release of the Motherduck/DuckDB connector for InfinyOn Cloud. This connector lets you to stream data from the InfinyOn cloud to MotherDuck in real time. MotherDuck is a cloud data analytics platform powered by DuckDB, an open-source OLAP engine. InfinyOn Cloud is the next generation of data streaming platform allowing anyone to connect, transform and dispatch data to anywhere. InfinyOn Cloud has built-in connectors that can connect to various data sources such as HTTP, MQTT, and SQL.","title":"OLAP for Event Streaming with MotherDuck Connector","url":"http://localhost:1315/blog/2023/07/infinyon-motherduck/"},{"body":"Streaming data from external data sources outside of the reader\u0026rsquo;s control often produce undesirable duplicates in the data set. One common method for dealing with such a situation is to offload deduplication to the database using SQL upserts.\nThis blog will show how to use the upsert operation with the sql-connector. You will learn how to set up an environment to use the SQL connector and how to apply the new upsert functionality.\nLet\u0026rsquo;s get started.\nWhat is Upsert In summary, upsert means to insert this record into the database if it doesn\u0026rsquo;t already exist. And If it already exists, update the existing record using the given data.\nIt translates into an SQL statement like this for PostgreSQL:\nINSERT INTO users ( name, age ) VALUES ( \u0026#39;John Doe\u0026#39;, 35 ) ON CONFLICT (name) DO UPDATE; So if we try to upsert a record with a name that already existed in the database, it would just update the age of the existing record instead of trying to create another record with the same name.\nRequired Environment Postgres Server Setup We need a PostgreSQL instance to run this example. If you don\u0026rsquo;t have it, I prepared a docker-compose file to set it up quickly:\n%copy%\nversion: \u0026#39;3.8\u0026#39; services: db: image: postgres:14.1-alpine restart: always environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres - POSTGRES_DB=sql-connector-test ports: - \u0026#39;15432:5432\u0026#39; Just save this as docker-compose.yaml and then run docker-compose up -d:\n%copy first-line%\n$ docker-compose up -d The db should be accessible on localhost:15432 after running this.\nRunning SQL queries This section explains how to run the sql queries included in the blog, if you already have a setup, you might not need this.\nWe can use psql to run our queries from the command line. On MacOS, you can install it with this command:\n%copy first-line%\n$ brew install libpq Note: homebrew is required to run this command.\nThen we can connect to the postgres instance we created by running this command:\n%copy first-line%\n$ psql -h localhost -p 15432 -d sql-connector-test -U postgres It should ask for password when connecting, the password is also postgres.\nAfter this it should show the psql cli, it looks like this:\npsql (15.3, server 14.1) Type \u0026#34;help\u0026#34; for help. sql-connector-test=# It can be closed by typing exit or \\q and pressing enter.\nWARNING: When running sql queries in psql, we have to terminate them using ; or psql will keep waiting for input, this might be confusing.\nCreating the Table In order to run the example, we need a table.\n%copy%\nCREATE TABLE users ( user_id SERIAL PRIMARY KEY, name TEXT UNIQUE, age INT ); Copy pasting this into psql and pressing enter should work.\nSetting up fluvio We also need a local fluvio cluster to run this example, see the getting started guide if you don\u0026rsquo;t have that already.\nCreate a test topic with:\n%copy first-line%\n$ fluvio topic create dedup-upsert-example Might want to use a better name than dedup-upsert-example for the topic.\nSetup sql-connector (saved as sql-connector-dedup-example.yaml):\napiVersion: 0.1.0 meta: version: 0.3.3 name: check-upsert-sql type: sql-sink topic: dedup-upsert-example create-topic: true secrets: - name: DB_USERNAME - name: DB_PASSWORD log_level: debug sql: url: \u0026#39;postgres://${{ secrets.DB_USERNAME }}:${{ secrets.DB_PASSWORD }}@localhost:15432/sql-connector-test\u0026#39; Secrets file (saved as secrets.txt):\n%copy%\nDB_USERNAME=postgres DB_PASSWORD=postgres Download ipkg file for the connector:\n%copy first-line%\n$ fluvio hub connector download infinyon/sql-sink@0.3.0 Install cdk\n%copy first-line%\n$ fluvio install cdk Deploy connector\n%copy first-line%\n$ cdk deploy start --config sql-connector-dedup-example.yaml --ipkg infinyon-sql-sink-0.3.0.ipkg --secrets secrets.txt Check that the connector is running\n%copy first-line%\n$ cdk deploy list Should print something like:\nsql-connector-dedup-example Running Running upsert Create a json file to produce records from (saved as produce.json):\n%copy%\n{ \u0026#34;Insert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;John Michael\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;66\u0026#34; } ] } } { \u0026#34;Insert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;Christian Jackson\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;33\u0026#34; } ] } } Run produce to create the records in the database:\n%copy first-line%\n$ fluvio produce -f produce.json dedup-upsert-example We have the records in the db (can use SELECT * FROM users; in psql to see this):\n2 \u0026#34;John Michael\u0026#34; 66 3 \u0026#34;Christian Jackson\u0026#34; 33 Now we can run upsert to update the existing records and create a new one (file saved as produce_upsert.json):\n%copy%\n{ \u0026#34;Upsert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;uniq_idx\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;John Michael\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;67\u0026#34; } ] } } { \u0026#34;Upsert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;uniq_idx\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;Christian Jackson\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;34\u0026#34; } ] } } { \u0026#34;Upsert\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;uniq_idx\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;column\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Text\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;Hillary Bonhart\u0026#34; }, { \u0026#34;column\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Int\u0026#34;, \u0026#34;raw_value\u0026#34;: \u0026#34;99\u0026#34; } ] } } Run produce to create/update the records in the database:\n%copy first-line%\n$ fluvio produce -f produce_upsert.json dedup-upsert-example Now we have the new record (\u0026ldquo;Hillary Bonhart\u0026rdquo;) and the old records with the updated ages in our db:\n2 \u0026#34;John Michael\u0026#34; 67 3 \u0026#34;Christian Jackson\u0026#34; 34 6 \u0026#34;Hillary Bonhart\u0026#34; 99 Clean-Up Shutdown the connector:\n%copy first-line%\n$ cdk deploy shutdown --name check-upsert-sql Destroy the database (this will delete data as well since there is no persistent docker volume attached):\n%copy first-line%\n$ docker-compose down ","description":"Step-by-step guide on how to use SQL connector upsert operation for deduplication.","keywords":null,"summary":"Streaming data from external data sources outside of the reader\u0026rsquo;s control often produce undesirable duplicates in the data set. One common method for dealing with such a situation is to offload deduplication to the database using SQL upserts.\nThis blog will show how to use the upsert operation with the sql-connector. You will learn how to set up an environment to use the SQL connector and how to apply the new upsert functionality.","title":"Deduplicate data streaming events with SQL Upsert","url":"http://localhost:1315/blog/2023/07/sql-upsert/"},{"body":"SANTA CLARA, CA \u0026ndash; Today, InfinyOn, a leading provider of real-time event stream processing, and MotherDuck, an industry pioneer in serverless data analytics, are excited to announce a strategic partnership designed to drive end-to-end streaming and analytics for real-time data.\nThe partnership underscores the commitment of both InfinyOn and MotherDuck to provide users with enhanced data flow capabilities and seamless integrations, enabling them to optimize their analytics workflows, reduce data friction, and unlock new levels of performance and scalability.\nUsing a combination of InfinyOn and MotherDuck, users can build complete real-time data analytics solutions for a wide variety of use cases including fraud detection, real-time inventory management, and real-time recommendation engines.\n\u0026ldquo;We\u0026rsquo;re thrilled to partner with MotherDuck, a company that shares our vision for democratizing data by giving users a seamless, end-to-end solution for collecting, streaming, transforming, and analyzing real-time data.\u0026rdquo; said A.J. Hunyady, CEO of InfinyOn. \u0026ldquo;With the introduction of our MotherDuck sink connector, we\u0026rsquo;re making it easier than ever for our customers to harness the power of their data.\u0026rdquo;\nJordan, CEO and Co-Founder of MotherDuck, said \u0026ldquo;We are excited that InfinyOn\u0026rsquo;s integration provides a streaming data sink for MotherDuck users. Our users\u0026rsquo; requirements are diverse, but we see an increasing desire in organizations to have very recent data accessible to analysts and applications.\u0026rdquo;\nThe strategic partnership between InfinyOn and MotherDuck sets a precedent for future collaborations, promising to inspire innovation and progress in the data streaming and analytics world. For more information on this exciting new partnership please visit www.infinyon.com or www.motherduck.com.\nAbout InfinyOn InfinyOn, a real-time event streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. Founded in 2019, InfinyOn aims to accelerate organizations\u0026rsquo; journey to the real-time economy. Its Cloud product lets enterprises quickly correlate events, apply business intelligence, and derive value from their data.\nAbout MotherDuck MotherDuck makes analytics fun, frictionless, and ducking awesome. In partnership with the team building open source database DuckDB, MotherDuck was founded by former leaders at some of the most innovative companies in data. It is on a mission to combine the elegance and speed of DuckDB with the collaboration and scalability of the cloud to provide a serverless, easy to use analytics platform for data small and large.\n","description":"InfinyOn Cloud and MotherDuck integration brings analytics to real-time event streaming.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; Today, InfinyOn, a leading provider of real-time event stream processing, and MotherDuck, an industry pioneer in serverless data analytics, are excited to announce a strategic partnership designed to drive end-to-end streaming and analytics for real-time data.\nThe partnership underscores the commitment of both InfinyOn and MotherDuck to provide users with enhanced data flow capabilities and seamless integrations, enabling them to optimize their analytics workflows, reduce data friction, and unlock new levels of performance and scalability.","title":"InfinyOn and MotherDuck Announce Strategic Partnership to Drive End-to-End Data Streaming and Analytics Pipelines","url":"http://localhost:1315/press-releases/infinyon-duckdb-announcement/"},{"body":"InfinyOn leverages the power of WebAssembly SmartModules to execute real-time data transformations. By harnessing the capabilities of WebAssembly, InfinyOn is able to efficiently process and manipulate data in a dynamic and responsive manner. These SmartModules enable seamless integration with web applications, allowing for on-the-fly data transformations without sacrificing performance. With InfinyOn\u0026rsquo;s innovative use of WebAssembly, organizations can achieve enhanced real-time data processing capabilities, unlocking new possibilities for data-driven decision-making and analysis.\nWatch this short video to learn more about the benefits of WASM -\nPublished by Darcy DeClute's YouTube channel","description":"Explore the top reasons why WebAssembly has the potential to eradicate the latency problems with web applications that utilize JavaScript.","keywords":null,"summary":"InfinyOn leverages the power of WebAssembly SmartModules to execute real-time data transformations. By harnessing the capabilities of WebAssembly, InfinyOn is able to efficiently process and manipulate data in a dynamic and responsive manner. These SmartModules enable seamless integration with web applications, allowing for on-the-fly data transformations without sacrificing performance. With InfinyOn\u0026rsquo;s innovative use of WebAssembly, organizations can achieve enhanced real-time data processing capabilities, unlocking new possibilities for data-driven decision-making and analysis.","title":"Why Webassembly - Top WASM Benefits","url":"http://localhost:1315/resources/why-webassembly-top-wasm-benefits/"},{"body":"JSON is ubiquitous in applications that need data and transforming data is very useful in almost every data pipeline. So the capability to transform JSON is nice to have when building data pipelines. We though about this before and developed fluvio-jolt which makes it easy to define and execute JSON-to-JSON mappings. It is based on the original java library. . Our use cases for fluvio-jolt have grown since, so we added more capabilities to it.\nThis blog explains these new capabilities, how they were implemented and how they can be used to map JSON in fluvio.\nLet\u0026rsquo;s get started.\nWhat is Jolt Jolt is a library developed in java to map JSON-to-JSON. It lets the user define specifications in JSON, which describe how to map data from JSON to JSON. Jolt is capable of performing several operations, namely:\nshift default remove cardinality sort We focus on the shift operation since it is the one that does most of the work. shift operation consists of matching the input keys, and outputting the keys and values. shift matches the input keys based on the keys of the spec.\nSo if you had this input:\n%copy%\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;account\u0026#34;: { \u0026#34;id\u0026#34;: 1000, \u0026#34;type\u0026#34;: \u0026#34;Checking\u0026#34; } } And you defined this spec:\n%copy%\n[ { \u0026#34;operation\u0026#34;: \u0026#34;shift\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;__data.id\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;__data.name\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;__data.account\u0026#34; } } ] It would produce this output:\n%copy%\n{ \u0026#34;__data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;account\u0026#34;: { \u0026#34;id\u0026#34;: 1000, \u0026#34;type\u0026#34;: \u0026#34;Checking\u0026#34; } } } This example already worked on the older versions of fluvio-jolt so let\u0026rsquo;s move on to the new features.\nWhat is new? Previously fluvio-jolt didn\u0026rsquo;t have the capability to use @ and $ wilcards. Also it didn\u0026rsquo;t have the capability to process arrays fluently. In the latest changes we implemented missing wildcards and added capability to process arrays. Although this seemed like a easy change, it proved to require big changes in the implementation.\nFirst we needed do define what we wanted to implement, since the original implementation didn\u0026rsquo;t have a spec, so we created a simple description Then we implemented a parser that parses the expressions into an abstract syntax tree and an interpreter that interprets this ast. And for the last step, we used the parser to parse the keys and values in the jolt spec. We used the interpreter to traverse the spec and the input in order to produce an output. So now the user can execute this spec:\n%copy%\n{ \u0026#34;items\u0026#34;: { \u0026#34;*\u0026#34;: { \u0026#34;@(guid.value)\u0026#34;: \u0026#34;data[\u0026amp;(1)].guid\u0026#34;, \u0026#34;*\u0026#34;: { \u0026#34;$\u0026#34;: \u0026#34;data[\u0026amp;(2)].keys[]\u0026#34; } } } } Given this input:\n%copy%\n{ \u0026#34;description\u0026#34;: \u0026#34;top description\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;inner description 1\u0026#34;, \u0026#34;guid\u0026#34;: { \u0026#34;permalink\u0026#34;: true, \u0026#34;value\u0026#34;: \u0026#34;https://example.com/link1-1\u0026#34; }, \u0026#34;link\u0026#34;: \u0026#34;https://example.com/link1\u0026#34;, \u0026#34;pub_date\u0026#34;: \u0026#34;Tue, 18 Apr 2023 14:59:04 GMT\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Title 1\u0026#34; }, { \u0026#34;description\u0026#34;: \u0026#34;inner description 2\u0026#34;, \u0026#34;guid\u0026#34;: { \u0026#34;permalink\u0026#34;: true, \u0026#34;value\u0026#34;: \u0026#34;https://example.com/link2-1\u0026#34; }, \u0026#34;link\u0026#34;: \u0026#34;https://example.com/link2\u0026#34;, \u0026#34;pub_date\u0026#34;: \u0026#34;Tue, 19 Apr 2023 14:20:04 GMT\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Title 2\u0026#34; } ], \u0026#34;last_build_date\u0026#34;: \u0026#34;Tue, 18 Apr 2023 15:00:01 GMT\u0026#34;, \u0026#34;link\u0026#34;: \u0026#34;https://example.com/top-link\u0026#34;, \u0026#34;namespaces\u0026#34;: { \u0026#34;blogChannel\u0026#34;: \u0026#34;http://example.com/blogChannelModule\u0026#34; }, \u0026#34;title\u0026#34;: \u0026#34;Blog-Recent Entries\u0026#34; } It gives this output:\n%copy%\n{ \u0026#34;data\u0026#34;: [ { \u0026#34;guid\u0026#34;: \u0026#34;https://example.com/link1-1\u0026#34;, \u0026#34;keys\u0026#34;: [ \u0026#34;description\u0026#34;, \u0026#34;guid\u0026#34;, \u0026#34;link\u0026#34;, \u0026#34;pub_date\u0026#34;, \u0026#34;title\u0026#34; ] }, { \u0026#34;guid\u0026#34;: \u0026#34;https://example.com/link2-1\u0026#34;, \u0026#34;keys\u0026#34;: [ \u0026#34;description\u0026#34;, \u0026#34;guid\u0026#34;, \u0026#34;link\u0026#34;, \u0026#34;pub_date\u0026#34;, \u0026#34;title\u0026#34; ] } ] } Lets go over how it works:\nThe items in the spec matches the items key in the input. The * in the spec matches every array element. @(guid.value) indexes into the array element using guid.value expression which means go into guid key and then go into value key. It outputs the result to data[\u0026amp;(1)].guid which means go into data field of the output and go into \u0026amp;(1)th index of it and then output to the guid field. Please note that it is inferred that the output is an object, and it should have a data key, and the value of data should be an array from this expression. Then the inner * in the spec matches every key in the input object. $ means output the matched key. data[\u0026amp;(2)].keys[] means output should be an object, it should have data key which has a array value, and that array has a keys field which is an array. It pushes the matched key to this array. More resources:\nSee fluvio-jolt repository for more usage examples and documentation. For more usage examples, see test cases. SmartModule Example Here we define a transform that implements the same transformation we defined in json format in the What is new? section.\ntransforms: - uses: infinyon/jolt@0.3.0 with: spec: operation: shift spec: items: \u0026#34;*\u0026#34;: \u0026#34;@(guid.value)\u0026#34;: data[\u0026amp;(1)].guid \u0026#34;*\u0026#34;: \u0026#34;$\u0026#34;: data[\u0026amp;(2)].keys[] How to use the new jolt capabilities? We already deployed a new version of jolt smartmodule (0.3.0). So these new capabilites are available on infinyon cloud, users just need to use the newer versions of the smartmodule.\nWe also published a new version of the rust crate on crates.io.\nSource code of the project can be found at the fluvio-jolt repo.\nWhat is next? We are looking into adding even more capabilities into Jolt and expanding data transformation capabilities of fluvio beyond JSON.\n","description":"New capabilities of the jolt smartmodule and how to use them.","keywords":null,"summary":"JSON is ubiquitous in applications that need data and transforming data is very useful in almost every data pipeline. So the capability to transform JSON is nice to have when building data pipelines. We though about this before and developed fluvio-jolt which makes it easy to define and execute JSON-to-JSON mappings. It is based on the original java library. . Our use cases for fluvio-jolt have grown since, so we added more capabilities to it.","title":"New Jolt Capabilities on Fluvio and How To Use Them","url":"http://localhost:1315/blog/2023/06/new-jolt/"},{"body":" Prev Next Page: / Download ","description":"Harness the power of Industrial Automation and IoT with our solution for rapid and cost-effective real-time data pipelines.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Industrial Automation and IoT Solution Brief","url":"http://localhost:1315/resources/data-pipelines-energy-sector/"},{"body":"This week on The Data Stack Show, Eric and Kostas chat with A.J. Hunyady, Founder and CEO of InfinyOn. During the episode, A.J talks about his experience with Hadoop and the challenges he faced with getting data out of the ecosystem. He also discusses the need for innovation in the data industry, real-time data, streaming, IoT, the benefits of Rust, and more.\n","description":"Paradigm Shift: Batch to Data Streaming.","keywords":null,"summary":"This week on The Data Stack Show, Eric and Kostas chat with A.J. Hunyady, Founder and CEO of InfinyOn. During the episode, A.J talks about his experience with Hadoop and the challenges he faced with getting data out of the ecosystem. He also discusses the need for innovation in the data industry, real-time data, streaming, IoT, the benefits of Rust, and more.","title":"The DataStack Show with A.J. Hunyady","url":"http://localhost:1315/resources/batch-to-data-streaming/"},{"body":" Prev Next Page: / Download ","description":"Rapidly and Cost-Effectively Build Real-Time Data Pipelines for the Energy Sector.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Energy Sector Solution Brief","url":"http://localhost:1315/resources/data-pipelines-energy-sector/"},{"body":"SmartModules have long provided Fluvio users a way to manipulate data in real-time. Until recently, the Stream Processing Unit (SPU) performed these transformations just after reading messages from storage, before delivering them to Consumers. Now the SPU can apply SmartModule transformations just after receiving them from Producers, before writing to disk.\nThis post will demonstrate how the SPU can apply multiple types of SmartModules for Producers in order to save storage space, save compute, validate data, and offload latency from the consumption flow to the production flow. If you choose to follow along while reading, it is assumed you have the Fluvio CLI installed (version 0.10.7 or higher), as well as the SmartModule Development Kit and a running Fluvio cluster connected.\nScenario: Crowdsourced weather data For the purpose of this demo, let\u0026rsquo;s imagine that you are the head of a team of researchers studying (and monitoring for) sudden weather changes across your country. To collect your data, you\u0026rsquo;ve designed a low cost, wifi connected device complete with a thermometer, barometer, and sunlight sensor and named it \u0026ldquo;weather-button\u0026rdquo;. You\u0026rsquo;ve sent weather-buttons to thousands of volunteers across the country who have connected them to power sources and wifi, so they can produce data to your Fluvio cluster.\nBecause you were uncertain what data would be needed when shipping the initial weather-buttons, you designed them to send all the data read from the sensors, and some identifying metadata:\n%copy%\n{ \u0026#34;device_id\u0026#34;: 3407, \u0026#34;device_version\u0026#34;: \u0026#34;1.8.0\u0026#34;, \u0026#34;resident_email\u0026#34;: \u0026#34;johndoe@test.com\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;weather-button\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:55.312Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Houston\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;TX\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;zip\u0026#34;: 77002, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;thermometer_reading\u0026#34;: { \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;humidity\u0026#34;: 52 }, \u0026#34;barometer_reading\u0026#34;: { \u0026#34;absolute_pressure\u0026#34;: 30.01, \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;humidity\u0026#34;: 52, \u0026#34;computed_altitude\u0026#34;: 81 }, \u0026#34;sunlight_sensor_reading\u0026#34;: { \u0026#34;illuminance\u0026#34;: 9582.5 } } The useless and redundant data in the payloads could present a problem for you. Having spent the majority of your budget on manufacturing the weather-buttons, you can afford just enough storage to give your data team the context they need. Fortunately, you had the foresight to program the weather-buttons to invoke a SmartModule by name when producing to Fluvio:\n%copy first-line%\n$ fluvio produce weather-event --smartmodule weather-event-cleaner Now that it\u0026rsquo;s time to start recording data, we\u0026rsquo;ll create a SmartModule named \u0026ldquo;weather-event-cleaner\u0026rdquo; and use it to reduce the storage burden of each request. We can use the smdk to generate a Map type SmartModule:\n%copy multiline-bash%\n$ mkdir weather_research_group $ cd weather_research_group $ smdk generate \\ --no-params \\ --sm-type map \\ --project-group weather-research-group \\ --sm-public false \\ weather-event-cleaner This leaves us with a simple Map SmartModule example:\n%copy%\n// weather-event-cleaner/src/lib.rs use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; #[smartmodule(map)] pub fn map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { let key = record.key.clone(); let string = std::str::from_utf8(record.value.as_ref())?; let int = string.parse::\u0026lt;i32\u0026gt;()?; let value = (int * 2).to_string(); Ok((key, value.into())) } The sample SmartModule attempts to parse each record into an integer and if successful, doubles it. It also maintains the optional key for each record if one is provided.\nAdditionally, all the configs (Cargo.toml, SmartModule.toml and rust-toolchain.toml) that we\u0026rsquo;ll need to build the SmartModule are included.\nNow let\u0026rsquo;s update the SmartModule for our use case. We\u0026rsquo;ll use serde, which is already included in the generated Cargo.toml, to model the JSON data being produced by our weather-buttons:\n%copy%\n// weather-event-cleaner/src/input_weather_event.rs use serde::{Deserialize, Serialize}; #[derive(Default, Serialize, Deserialize, Debug)] pub(crate) struct InputWeatherEvent { pub device_id: usize, pub device_version: String, pub resident_email: String, pub source: String, pub timestamp: String, pub observation_location: InputObservationLocation, pub thermometer_reading: InputThermometerReading, pub barometer_reading: InputBarometerReading, pub sunlight_sensor_reading: InputSunlightSensorReading, } #[derive(Default, Serialize, Deserialize, Debug)] pub struct InputObservationLocation { pub city: String, pub state: String, pub country: String, pub latitude: f64, pub longitude: f64, pub zip: usize, pub altitude: usize, } #[derive(Default, Serialize, Deserialize, Debug)] pub struct InputThermometerReading { pub temperature: f64, pub humidity: usize, } #[derive(Default, Serialize, Deserialize, Debug)] pub struct InputBarometerReading { pub absolute_pressure: f64, pub temperature: f64, pub humidity: usize, pub computed_altitude: usize, } #[derive(Default, Serialize, Deserialize, Debug)] pub struct InputSunlightSensorReading { pub illuminance: f64, } Next let\u0026rsquo;s use serde again to describe our SmartModule\u0026rsquo;s output type. This time we\u0026rsquo;ll define our structure without the unwanted metadata and redundancy, and we\u0026rsquo;ll reduce the nesting:\n%copy%\n// weather-event-cleaner/src/output_weather_event.rs use serde::{Deserialize, Serialize}; #[derive(Default, Serialize, Deserialize)] pub(crate) struct OutputWeatherEvent { pub device_id: usize, pub timestamp: String, pub observation_location: OutputObservationLocation, pub temperature: f64, pub absolute_pressure: f64, pub humidity: usize, pub illuminance: f64, } #[derive(Default, Serialize, Deserialize)] pub struct OutputObservationLocation { pub latitude: f64, pub longitude: f64, pub altitude: usize, } -\u0026gt; Note: The InfinyOn certified SmartModule Jolt is available to allow developers to implement JSON to JSON transformations like ours using a simple DSL.\nFinally, we can implement the From trait to convert between our two types. This is all we need to fill in our map() method, which will just deserialize the input JSON, call the from method, and then serialize the output JSON:\n%copy%\n// weather-event-cleaner/src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; mod output_weather_event; mod input_weather_event; use output_weather_event::{OutputWeatherEvent, OutputObservationLocation}; use input_weather_event::InputWeatherEvent; impl From\u0026lt;InputWeatherEvent\u0026gt; for OutputWeatherEvent { fn from(input_event: InputWeatherEvent) -\u0026gt; Self { OutputWeatherEvent { device_id: input_event.device_id, timestamp: input_event.timestamp, observation_location: OutputObservationLocation { latitude: input_event.observation_location.latitude, longitude: input_event.observation_location.longitude, altitude: input_event.observation_location.altitude, }, temperature: input_event.thermometer_reading.temperature, absolute_pressure: input_event.barometer_reading.absolute_pressure, humidity: input_event.barometer_reading.humidity, illuminance: input_event.sunlight_sensor_reading.illuminance, } } } #[smartmodule(map)] pub fn map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { let key = record.key.clone(); let input_event: InputWeatherEvent = serde_json::from_slice(record.value.as_ref())?; let output_event = OutputWeatherEvent::from(input_event); let value = serde_json::to_vec_pretty(\u0026amp;output_event)?; Ok((key, value.into())) } Now our Map SmartModule is ready to build using the smdk:\n%copy multiline-bash%\n$ cd weather-event-cleaner $ smdk build And add to the cluster:\n%copy%\n$ smdk load -\u0026gt; Note: You can also use the fluvio smartmodule create command to add smartmodules to your cluster.\nTo test what happens when our cluster receives records from the weather-buttons, we\u0026rsquo;ll save a sample weather_event.json:\n%copy%\n{ \u0026#34;device_id\u0026#34;: 3407, \u0026#34;device_version\u0026#34;: \u0026#34;1.8.0\u0026#34;, \u0026#34;resident_email\u0026#34;: \u0026#34;johndoe@test.com\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;weather-button data collector\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:55.312Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Houston\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;TX\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;zip\u0026#34;: 77002, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;thermometer_reading\u0026#34;: { \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;humidity\u0026#34;: 52 }, \u0026#34;barometer_reading\u0026#34;: { \u0026#34;absolute_pressure\u0026#34;: 30.01, \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;humidity\u0026#34;: 52, \u0026#34;computed_altitude\u0026#34;: 81 }, \u0026#34;sunlight_sensor_reading\u0026#34;: { \u0026#34;illuminance\u0026#34;: 9582.5 } } And create a topic:\n%copy%\n$ fluvio topic create weather-events Then produce to it:\n%copy%\n// delete the new lines so the entire JSON is interpreted as one record $ tr -d \u0026#39;\\n\u0026#39; \u0026lt; weather_event.json | fluvio produce weather-events --smartmodule weather-event-cleaner Let\u0026rsquo;s review the resulting records.\n%copy first-line%\n$ fluvio consume weather-events -B -d Consuming records from \u0026#39;weather-events\u0026#39; starting from the beginning of log { \u0026#34;device_id\u0026#34;: 3407, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:54.308Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;absolute_pressure\u0026#34;: 30.01, \u0026#34;humidity\u0026#34;: 52, \u0026#34;illuminance\u0026#34;: 9582.5 } That\u0026rsquo;s a lot of storage space saved per record! And we still have all the data we need to feed our model.\nExpanding Our SmartModule\u0026rsquo;s Functionality Mapping our input data down is a useful function, but there is a lot more we can ask the SPU to help us with before committing our records.\nSuppose now that the barometers we shipped with our weather-buttons were a bit unreliable. About 1 out of every 50 events our weather buttons send contains only the default values for the barometer data:\n{ \u0026#34;...\u0026#34;: \u0026#34;// --snipped\u0026#34;, \u0026#34;thermometer_reading\u0026#34;: { \u0026#34;temperature\u0026#34;: 23.92, \u0026#34;humidity\u0026#34;: 54 }, \u0026#34;barometer_reading\u0026#34;: { \u0026#34;absolute_pressure\u0026#34;: 0.0, \u0026#34;temperature\u0026#34;: 0.0, \u0026#34;humidity\u0026#34;: 0, \u0026#34;computed_altitude\u0026#34;: 0 }, \u0026#34;sunlight_sensor_reading\u0026#34;: { \u0026#34;illuminance\u0026#34;: 9502.6 } } We\u0026rsquo;ll have to remove events such as these or they will skew our results. Luckily, if we remove these invalid events from our data pool, we will still have the density of data needed to feed to our model.\nWe could ask the SPU to perform this filtering on write (for the producer) or on read (for the consumer). Handling this filtering on write will save a few more bytes on disk. Additionally it will save us some compute as our dozens of Consumers won\u0026rsquo;t have to filter the same records. So we\u0026rsquo;ll modify our weather-event-cleaner SmartModule to filter out these records in addition to performing the JSON mapping.\nTo facilitate the filtering, we\u0026rsquo;ll first have to trade in the #[smartmodule(map)] attribute for #[smartmodule(filter_map)]. Then we\u0026rsquo;ll have to modify the return type from Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; to Result\u0026lt;Option\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt;. The additional Option\u0026lt;T\u0026gt; wrapping the tuple allows us to return Ok(None) when we want to drop the record:\n%copy%\n// src/lib.rs // \u0026lt;--snipped--\u0026gt; #[smartmodule(filter_map)] pub fn filter_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Option\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { let key = record.key.clone(); let input_event: InputWeatherEvent = serde_json::from_slice(record.value.as_ref())?; // filter out the invalid data if input_event.barometer_reading.absolute_pressure == 0.0 { return Ok(None); } let output_event = OutputWeatherEvent::from(input_event); let value = serde_json::to_vec_pretty(\u0026amp;output_event)?; Ok(Some((key, value.into()))) // remember to adjust the return value for new signature } Again we can build our SmartModule:\n%copy%\n$ smdk build And replace our old SmartModule on the cluster:\n%copy%\n$ smdk load To test that the new SmartModule still performs the mapping but also omits our invalid events, we\u0026rsquo;ll add another test file, weather_event2.json:\n%copy%\n{ \u0026#34;device_id\u0026#34;: 3407, \u0026#34;device_version\u0026#34;: \u0026#34;1.8.0\u0026#34;, \u0026#34;resident_email\u0026#34;: \u0026#34;johndoe@test.com\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;weather-button data collector\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:56.317Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Houston\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;TX\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;zip\u0026#34;: 77002, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;thermometer_reading\u0026#34;: { \u0026#34;temperature\u0026#34;: 23.92, \u0026#34;humidity\u0026#34;: 54 }, \u0026#34;barometer_reading\u0026#34;: { \u0026#34;absolute_pressure\u0026#34;: 0.0, \u0026#34;temperature\u0026#34;: 0.0, \u0026#34;humidity\u0026#34;: 0, \u0026#34;computed_altitude\u0026#34;: 0 }, \u0026#34;sunlight_sensor_reading\u0026#34;: { \u0026#34;illuminance\u0026#34;: 9502.6 } } Then produce both files with the new smartmodule.\n%copy%\n// again remove the new lines but add one to demarcate $ (tr -d \u0026#39;\\n\u0026#39; \u0026lt; weather_event.json; echo \u0026#39;\u0026#39;; tr -d \u0026#39;\\n\u0026#39; \u0026lt; weather_event2.json) | fluvio produce weather-events --smartmodule weather-event-cleaner And verify the results:\n%copy first-line%\n$ fluvio consume weather-events -B -d Consuming records from \u0026#39;weather-events\u0026#39; starting 1 from the end of log { \u0026#34;device_id\u0026#34;: 3407, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:55.312Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;absolute_pressure\u0026#34;: 30.01, \u0026#34;humidity\u0026#34;: 52, \u0026#34;illuminance\u0026#34;: 9582.5 } { \u0026#34;device_id\u0026#34;: 3407, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-04-12T15:07:55.312Z\u0026#34;, \u0026#34;observation_location\u0026#34;: { \u0026#34;latitude\u0026#34;: 29.7432, \u0026#34;longitude\u0026#34;: -95.4011, \u0026#34;altitude\u0026#34;: 80 }, \u0026#34;temperature\u0026#34;: 23.88, \u0026#34;absolute_pressure\u0026#34;: 30.01, \u0026#34;humidity\u0026#34;: 52, \u0026#34;illuminance\u0026#34;: 9582.5 } Here we can see the invalid weather event was filtered out, while the valid event was again mapped and committed.\nNow that the SPU is applying our new FilterMap SmartModule for the producers, we\u0026rsquo;re maintaining the integrity of our results and avoiding any added latency on read.\nConclusion We\u0026rsquo;ve seen how applying Map and FilterMap type SmartModules while producing data rather than while consuming can help us to save storage \u0026amp; compute, validate our data once, and offload latency from Consumer processes. Be sure to brainstorm whether applying these or any other types of SmartModules while producing can be helpful to your Fluvio use case.\n","description":"Learn how to save resources and validate incoming data by applying SmartModules before save","keywords":null,"summary":"SmartModules have long provided Fluvio users a way to manipulate data in real-time. Until recently, the Stream Processing Unit (SPU) performed these transformations just after reading messages from storage, before delivering them to Consumers. Now the SPU can apply SmartModule transformations just after receiving them from Producers, before writing to disk.\nThis post will demonstrate how the SPU can apply multiple types of SmartModules for Producers in order to save storage space, save compute, validate data, and offload latency from the consumption flow to the production flow.","title":"Optimize Your Fluvio Cluster with Pre-Commit SmartModules ","url":"http://localhost:1315/blog/2023/04/producer-smartmodule/"},{"body":" Prev Next Page: / Download ","description":"WebAssembly or WASM is a safe, portable, low-level code format designed for efficient execution and compact representation.","keywords":null,"summary":" Prev Next Page: / Download ","title":"WebAssembly Specification","url":"http://localhost:1315/resources/webassembly-specification/"},{"body":"What the data processing tools in the markets won’t tell you about data pipelines As an engineering leader in an ambitious growing organization, there is a nine out of ten chance of you getting sucked into using data processing infrastructure and tools that will need 3x to 5x more time, developer capacity, and budget.\nMaybe you are already on that slippery slope subscribing to the ease of managed cloud solutions or the temptation of open source tools. It would take 180 days or less for your weekends to be consumed by ‘incidents,’ and the rest of the ride is going to be bumpy until the crash landing! If you have been doing it for a while, you know this reality!\nI have been in similar circumstances through the past 16 years of my career in building data intensive products. Whether it is the low entry barrier for open source big data tools, the latest fads claiming the death of entire categories, or managed services which are ‘easy to get started to use’\u0026hellip; until they come to harvest your organs with their bills!\nWhat’s up with the tooling bloat? Nearly 1500 companies are selling memes!\nIt is a pain to exist in this market. But hey, there is sunk cost fallacy, survivorship bias, fear of crashing out, and more. Yet there is a sliver of hope of a brighter future!\nWhat you must do if you are trying to build profitable future-proof data solutions I have seen way too many failed data projects because of over promising, over complicating, over engineering, under communicating and not empathizing with the customer. You likely have as well.\nAt the end of the day the data is important, but it is an input to better decisions and success!\nThat is what we are looking to actualize for our businesses in the age of entropy overload!\nLess is indeed more! This is the hardest problem for me as a product leader with a hands-on data engineering and AI/ML background. An area that I deliberately strive for 1% improvement each day!\nThe thing is that the real-time data game is lost before it begins if you are looking to keep on routing data through multiple data landings and manage a ludicrous amount of transformation logic all of which results in more copies of the same data.\nMore infrastructure to manage. More applications to manage. More costs to your business. And a dangling carrot of profits which keeps you on the hamster wheel as long as you can survive.\nNow if you want to play that game, I would be in the stands rooting for you and hoping you win. Or, I can be real and tell you that 9 out of 10 times you are going to be on the losing side!\nAt least, that has been my experience. Not everyone is Netflix, Uber, LinkedIn and there are several reasons for that. Capital, talent pool, expectation vs reality of actual digital competence of infrastructure and the ecosystem.\nYou need to be real about your budget and capacity, and your build vs. buy decisions. It sounds obvious, yet this is where we suck big time.\nHow we are solving the problem at InfinyOn At InfinyOn we are building simple primitives to build data flows which are currently helping small engineering teams at a few companies to orchestrate and build products based on machine generated data from sensors and server applications.\nNow our website says that we are a real-time event streaming platform, which we are. However, real-time is like a slang these days!\nHere is what we are not. We are not a Kafka reseller, a Flink reseller, a database, or yet another big data analytics solution! There are many real time databases out there which are awesome, but the real-time game is played as the data flows and not in database queries. This is a pattern that is pretty sparse in the entire software ecosystem.\nThe problem that our small team has found to be the hardest is the one of data orchestration and transformation.\nNow you might be thinking, what is the big deal? There are so many ETL, ELT companies out there, and you are correct!\nJust look at the amount customer complaints about their connectors, and the inflated costs, are and how suboptimal the workflow is…\nWe are not building yet another tool which tries to serve everything up on a platter, sets the expectation of serving a Michelin star restaurant quality dish and serves up fast food quality bloatware.\nWe have built the foundations of an end-to-end data streaming tool that combines a message queue, a transformation engine, and Serverless data flow runtime from 0 to 1.\nNo infrastructure to set up, no upfront cost, just build clients and configurations to connect your data sources, add your business logic transformations. And Voila, the data collection, transformation and delivery setup is done.\nWe empower companies who value their product and data, with a data orchestration and transformation layer that is green, future proof, cost effective as well as robust, blazing fast, and easy to maintain.\nHow we can elevate your data flows? We are opening up limited spots to double our design partners in Q2, 2023 to power up your technology roadmap with simplicity. We will provide you with a ton of credits on InfinyOn Cloud to get started, and collaborate with your engineering team to build out your data collection, data transformation, and data orchestration layer. You will have your data pipeline built out and you will ship intelligent features in your product.\nOur raving customer says that in 99% of their use cases, InfinyOn empowers them to build robust data orchestration in a single sprint with a team of 1 or 2 engineers! What are their use cases? They need to collect and shape machine generated data for building their product which includes telemetry, monitoring, analytics, automation, and machine learning features. Our platform allows them to build simple clients that orchestrates the shaped data to the different features and applications so they can focus on serving their customers.\nA huge financial services customer benchmarked our product against the currently available messaging and data orchestration solutions in the market and found us to be 3x lower in latency, 5x faster in throughput, 7x lower CPU consumption, and 50x lower memory consumption. If you read between the lines of this benchmark, what they are saying is that the platform is ridiculously efficient and cost effective.\nLet me balance out the positive feedback by tempering the expectations a bit. We believe that connectors of data sources are a ubiquitous pattern, yet there is no one size fits all approach to connectors. It is the reason for the subpar experience with endless lists of prebuilt connectors.\nWe have built development kits to empower and enable you to build robust connectors and smart modules. We are here to collaborate with you to build connectors and transformation smart modules which you will be able to reuse and repurpose for all your data collection, and transformation use cases.\nThere are several things that we can build together, but I don’t believe in ‘if you build it they will come, or ‘fake it till you make it.’\nWe want to teach you how to fish rather than serving you stale canned fish!\nWe may have a fit, if you are a technology or delivery leader at a technology company that is working with data understands the pains and gains of data and infrastructure with a limited budget to build. It would be a perfect fit if you are working on remote monitoring, telemetry type use cases and innovating in manufacturing, robotics, connected devices, gaming, avionics, spacetech etc.\nIf the above description resonates, I would invite you to apply to our data flow design sprint to accelerate your product development and build for the future.\nIf you would like to be a part of the next exclusive cohort of data flow design sprint, register here: Let\u0026#39;s Go! Technical fine-print We are building the simplest and most performant data capture and transformation primitives to process data as it flows from the edge. Of course that means serverless, real-time, event streaming and all the buzz words that you can think of!\nIf you take away one thing and one thing only from our self description, then I hope it is this:\nInfinyOn is a platform for developing simple yet ridiculously efficient data flows to build delightful software.\nThe product is a low latency data orchestration and transformation runtime which is written from scratch using Rust and the first principles of software engineering! It’s like a kernel with endless possibilities. It’s compatible with Web Assembly giving us an astronomical level of flexibility and usability. This is just the tip of the iceberg!\nThe primitives that we use in our platform are producers, consumers, topics, connectors, and smart modules aligned with the messaging paradigm.\nFluvio is the core of the platform, the equivalent of a kernel that pacakges the primitives into a runtime InfinyOn Cloud s a fully managed operating system that simplifies the deployment of the Fluvio kernel and runtime. Topics, Producers, and Consumers are standard messaging system concepts. TopicsConnectors are used to connect to data sources and destinations using connection protocol, authentication, and access patterns. You can build connectors using the Connector Development Kit to connect to any protocol to orchestrate ingress and egress of data. Smart modules are data transformation operators which filter, group, transform, count, map and more. You can build smart modules using the Smart Module Development Kit to transform the data in topics. InfinyOn Cloud Hub enables reusing connectors and smart modules to apply common patterns of data collection and data transformation to shape data for diverse use cases. Our current design partners and customers can’t rave enough about InfinyOn Cloud, which is the best part of my job as the Head of Product!\nThank you for reading through this piece.\nI am looking to start collaborating with a cohort of 3 companies in Q2 and I want to identify the design partners by the end of April and get to building together.\nIf you are a technology or delivery leader building data-intensive applications for the future, I would invite you to apply to the upcoming data flow design sprint.\nIf you would like to be a part of the next exclusive cohort of data flow design sprint, register here: Let\u0026#39;s Go! If you would like to learn more to make an informed decision, book a meeting to learn more. Here is my calendar: Setup a 1:1 call with me. Connect with us: Please, be sure to join our Discord server if you want to talk to us or have any questions.\nSubscribe to our YouTube channel\nFollow us on Twitter\nFollow us on LinkedIn\nTry InfinyOn Cloud a fully managed Fluvio service\nHave a happy coding, and stay tuned!\nFurther reading Handling JSON data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly The InfinyOn Continuous Intelligence Platform ","description":"How to build ‘future proof,’ ‘green,’ data flows and drastically improve the the odds of success of your data application.","keywords":null,"summary":"What the data processing tools in the markets won’t tell you about data pipelines As an engineering leader in an ambitious growing organization, there is a nine out of ten chance of you getting sucked into using data processing infrastructure and tools that will need 3x to 5x more time, developer capacity, and budget.\nMaybe you are already on that slippery slope subscribing to the ease of managed cloud solutions or the temptation of open source tools.","title":"Why 87% of all data projects are doomed to fail, and how you can improve the odds of success","url":"http://localhost:1315/blog/2023/03/failing-data-projects/"},{"body":"Goal It\u0026rsquo;s been 2 months for me at InfinyOn. In this post, I will share about the journey of joining InfinyOn as the Head of Product. This post will serve as a foundation for describing the product management system at InfinyOn as we build it out.\nContext Close to the end of 2022, I was in an interview with the Chief Data officer at a large pharma company for a Director of Data Products role. The CDO posed a question about the hops, skips, and jumps in my career timeline.\nShe asked, “Why are you unable to find a fit to stay beyond a couple of years in your previous data product management roles?”\nMy response to the question was, “this is my way of trading off stability for learning the patterns in different market contexts.”\nAt that point, we both knew that even though that was a truthful and accurate representation of my rationale, it was certainly incomplete. While I did not end up continuing with the opportunity in the pharma company, I was exploring the problem space further.\nWhy did I change 6 roles in the last 7 years? What are the various dimensions and attributions of the changes?\nThe exploration and discovery of the longevity of my career in data product management led to a few salient dimensions of the problem:\nProfessional Development Fit People and Leadership Expertise Organization Design Effectiveness Data Maturity Technology Stack Complexity Market Forces and Economics In this iteration, I am going to prioritize and focus the conversation on data maturity, technology stack, and the market forces.\nPatterns of data product opportunities In November 2021, I was in my fifth product role with a well-funded eCommerce data play.\nIt was my fifth role in 5 years! How did I get here?\nAll of my past product roles have been about building out functional data platforms to enable data applications, including AI/ML, to be built at scale. Almost all the companies I worked with made significant investments to make it happen.\nWhen I started out with data about five years into my career in 2011, the electronic health record database I was thrown into the deep end with had over 4200 tables and was deployed on-prem. That data intensive role in analytics and insights was also transformative for me in multiple other areas, including:\non-prem deployment to SaaS waterfall to agile small data to big data. Kinda messed up writing out that last point, especially since Jordan Tigani the CEO of MotherDuck, just published \u0026ldquo;BIG DATA IS DEAD\u0026rdquo; a week back.\nFast forward to 2016, I moved from data engineering and data science into my first data product management role as the 7th employee in a seed stage AI startup. I had tried and failed at building an augmented reality ecommerce play between 2014 and 2016, and this was my third attempt at an early stage startup. Turns out it\u0026rsquo;s really hard to package speech, vision, and natural language algorithms into production software. In 2018, another AI startup gained us for the talent mainly and some of the algorithmic intellectual property. The CEO parted ways, and I moved with my family to Canada.\nBetween 2018 and 2021, I had worked with a series A stage life sciences AI play, A large100-year-old American Automaker in their AI Center of Excellence, A HR Tech SaaS play and then in the eCommerce analytics company.\nIt’s not typical to spend a lot of time in architecture deep dives in product management interviews, however, my distinct memory of the experience with the eCommerce company is that the presentation was focused on where the company wanted to go in terms of its tech stack and not the current state.\nThe team was charged up. There was a palpable excitement about the future and a 9 figure runway. The customer portfolio and revenue were growing at a healthy scale and everyone was hoping for the next s-curve of growth.\nBut something was not quite right! There was a nagging anxiety among the exec leaders about the health of the business and even the most high energy town-halls would have this perceivable faking of confident motivational speech like the worst cuts of “The Wolf of Wall Street.”\nPatterns of data product nightmares It took roughly 45 to 60 days to calibrate the gaps between the expectation and the reality. We had a ton of data. The state of the data and the platform were a hot mess. Amazon was loving our architectural patterns and data pipelines since they made over $1.5 for every $1 we made! Every time we onboarded an ideal customer, we needed to calculate how much it would cost us to serve them, which is an excellent muscle to develop.\nThe customers needed the data at a relevant and specific frequency. Beyond a handful of instant alerts, the pattern for checking the data was not real time. However, the customers were expecting fresh data and not looking to wait for hours to see the results of their search.\nImagine you are a consumer product company managing your portfolio on ecommerce marketplaces and you come to a software to add a list of 1000s of products to see the trends of the product portfolio performance in the marketplace with up to two years of historical trends. Now imagine a platform that has to serve 10s of thousands of such cohorts of products.\nWhile this may not seem super complex and folks may think, what’s the big deal with that? It’s the upstream data pipeline that collects data for over a billion products on eCommerce marketplaces like Amazon at varying frequency depending on popularity. As an example of frequency, the data on Amazon updates as frequently as hourly for top selling products. This is where things get interesting. The eCommerce players want to see the view of a slice of the market spanning related categories, subcategories, brand clusters and they would want the data to be updated at least daily.\nData pipelines at scale are harder than most people realize!\nThe workflows involved:\nMaintaining an updated queue of products to track and update based on popularity in the marketplace Collect the data with a reasonable frequency Ensuring that the collected data is captured and streamed into the data storage, which included buckets, hot data stores, document stores, data warehouse and cache Resolving basic data quality issues, albeit reactively with some after the fact monitoring and alerting Running the legacy pipeline with the monolithic DAGs to support existing customers while building out the distributed parallel processing to process batches and deliver better bang for the buck The data was consumed by large-scale machine learning use cases, analytical aggregations and serving them up to the customers What followed for the next year in my role were epochs of deliberate re-platforming work, paying off tech debt, 3 re-orgs and still the light at the end of the tunnel was not in sight!\nImagine running out of a 9 figure runway in 3 years and still lacking the ability to deliver quality data to customers on time.\nThe desperate reality I have been on the hamster wheel. People problems have kept growing. There were a lot of expectations. Delivering significant outcomes on the re-platforming work and optimizing hosting costs was not enough.\nWe had tried it all: file formats, partitioning, distributed batch processing. We’d be in engineering meetings and data science teams would plead for easier access to data, feature stores, software engineering teams would like stream friendly data delivery without having to figure out distributed computation!\nData Science has gone from being the sexiest job of 21st century to recovering data scientists moving to data engineering or MLops.\nBig data is dead.\nModern data stack is dead.\nWe are all in zombie land, trying to solve data pipeline problems and build data products. And sunk cost fallacy overshadowed the return on investments!\nThis is the data opportunity in 2023. It\u0026rsquo;s difficult, but it is what we are solving for.\nAlternative solutions Based on my perception and interpretation of the reality of the ecosystem, I started having more dialogues with my circle of trustworthy architects with a goal of starting a venture that simplifies this Frankenstein modern data stack that is the beloved of the emperor who has no clothes!\nI simply wanted to build a proof of concept of getting the data from web sources and APIs to get it into a form that one could make simple inferences and decisions on it. The pattern in my mind was to treat the entity that is central to the use case similar to a company stock and do precisely three things. Count relevant facts over specific time-steps, measure the deltas of the relevant facts and the correlations between related facts and simply plot these. And, I wanted to process these as efficiently as possible in terms of cost, complexity, and latency.\nThis is how one of my trusted architect buddies referred me to Fluvio.io.\nA seed was sown.\nAs a data product lead, I was familiar with a significant amount of tooling, and I was familiar with the technical leaps in web assembly, but I knew little about Fluvio or rust.\nI consider myself to be disillusioned towards new tooling.\nSince picking up data engineering in 2011, I have had my fair share of tooling exposure and the forced refactoring of distributed data projects due to what I call “cotton candy syndrome.”\nThere are JVM based tools that we have all used for administrative overhead, workflow orchestration, to ultimately compile SQL queries into distributed batch jobs dependent on the highly efficient JVM and serializer deserializers.\nAll the use cases that I have worked on since the past 12 years are temporal, which is most business problems. And we all know how amazing the whole software ecosystem is at handling dates. Remember Y2K?\nIn my experience, it is like fixing the hole in the bucket with straw which needs to be cut with an ax which needs to be sharpened using a stone which needs to be moistened by water which needs to be fetched with the bucket which is leaking!\nIn the end, I did not reach a conclusion, and the problem remained in my head.\nAn exciting opportunity A few months later, a data practitioner was talking about web assembly and data tools on Linked and I simply shared Fluvio. That led to a conversation with Grant Swanson from InfinyOn the managed cloud platform that runs Fluvio and I was now triggered to learn more.\nThe findings were indeed compelling. At a high level there are two types of problems in the data world one where the expected temporal latency is miniscule i.e. the true real time use cases, and the other where the speed and volume of data creation calls for efficiency in processing the data just to keep up.\nWe all have hopefully learnt that monolithic systems and synchronous micro services are equally hard to scale in terms of infrastructure, talent capacity and cold hard cash needed to operate a data play at scale.\nAll DATA CONSUMERS ultimately want fresh and high-quality data fast and cheap to make decisions.\nAnd the current systems simply have not delivered.\nYet another layer of abstraction to aggregate the existing tools and repackaging them will not solve it for the businesses. At least that is what the empirical evidence suggests.\nThe core performance issues are truly foundational. Not that these abstraction layers are useless, they have a significant role in improving the data maturity of many organizations. But ‌the foundational changes will be a function of innovation that will simplify the data capture, movement, and transformation and minimize it.\nIn the early days of distributed parallel computing with HDFS and Map Reduce, we were referencing the foundations of storage costs, size of storage capacity, read write speed, computational needs, workflow orchestration, cluster management.\nIn the epochs of artificial intelligence and machine learning, we have become more aware of the hidden cost of machine learning, the scale of computational needs, the need for bespoke hardware, as well as tooling integration. We have also become more aware of Garbage In Garbage Out, No free lunch theorem and similar mental models to appreciate the complexity at play.\nThe basic necessity of data informed decision making that is valuable to a business remains the same - Fresh high quality data with basic transformations over time delivered seamlessly to the data consumers.\nThis is no longer an internal platform play since analytics is a ubiquitous functionality provided with most of the software ecosystem\nThe problem space is diverse and in need of simple first-principle solutions!\nThis impressed me with InfinyOn. Connectors for source and sink, smart modules for data operations, and running with a ridiculous efficiency. This is what the data world really needs. This is how real time streams ought to be done.\nWe have a challenging yet exciting road ahead. But here I am, having finally found a purpose at the root of a problem space that would reform how data is processed. Excited to be building with an amazing team at InfinyOn a product that will make real time streaming how it\u0026rsquo;s supposed to be.\nIf you are working with a challenging data stack and impactful data problems, I would love to connect and understand your current challenges and areas of risk in your data stack and explore how we could help you simplify your data pipeline and make it more efficient and effective:\nSetup a 1:1 call with me.\nConnect with us: Please, be sure to join our Discord server if you want to talk to us or have any questions.\nSubscribe to our YouTube channel\nFollow us on Twitter\nFollow us on LinkedIn\nTry InfinyOn Cloud a fully managed Fluvio service\nHave a happy coding, and stay tuned!\nFurther reading Handling JSON data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly The InfinyOn Continuous Intelligence Platform ","description":"Why I am excited about the future of InfinyOn and what made my want to take on building the product management system and practice here","keywords":null,"summary":"Goal It\u0026rsquo;s been 2 months for me at InfinyOn. In this post, I will share about the journey of joining InfinyOn as the Head of Product. This post will serve as a foundation for describing the product management system at InfinyOn as we build it out.\nContext Close to the end of 2022, I was in an interview with the Chief Data officer at a large pharma company for a Director of Data Products role.","title":"Why I am betting on InfinyOn","url":"http://localhost:1315/blog/2023/03/betting-on-infinyon/"},{"body":"The modern data stack today is way too complicated. Learn how to build time-sensitive data applications in just a few minutes using InfinyOn Cloud. (16 minutes)\n","description":"Step-by-step tutorial on building a time-sensitive data app.","keywords":null,"summary":"The modern data stack today is way too complicated. Learn how to build time-sensitive data applications in just a few minutes using InfinyOn Cloud. (16 minutes)","title":"InfinyOn MQTT to Postgres in 10 mins","url":"http://localhost:1315/resources/mqtt-to-postgres/"},{"body":"The modern data stack today is way too complicated. Learn how to build time-sensitive data applications in just a few minutes using InfinyOn Cloud. (16 minutes)\n","description":"Step-by-step tutorial on building a time-sensitive data app.","keywords":null,"summary":"The modern data stack today is way too complicated. Learn how to build time-sensitive data applications in just a few minutes using InfinyOn Cloud. (16 minutes)","title":"InfinyOn MQTT to Postgres in 10 mins","url":"http://localhost:1315/videos/mqtt-to-postgres/"},{"body":"View our most webinar with James Stuart-Bruges, Head of Product for Klarian, Ben Cleary, CTO for Klarian, and Alexander Mikalev, Sr. Solutions Architect for InfinyOn. (37 minutes)\n","description":"This webinar will showcase a joint solution from Klarian and InfinyOn.","keywords":null,"summary":"View our most webinar with James Stuart-Bruges, Head of Product for Klarian, Ben Cleary, CTO for Klarian, and Alexander Mikalev, Sr. Solutions Architect for InfinyOn. (37 minutes)","title":"Real-time Pipeline Monitoring for the Energy Sector","url":"http://localhost:1315/resources/real-time-pipeline-monitoring/"},{"body":"View our most webinar with James Stuart-Bruges, Head of Product for Klarian, Ben Cleary, CTO for Klarian, and Alexander Mikalev, Sr. Solutions Architect for InfinyOn. (37 minutes)\n","description":"This webinar will showcase a joint solution from Klarian and InfinyOn.","keywords":null,"summary":"View our most webinar with James Stuart-Bruges, Head of Product for Klarian, Ben Cleary, CTO for Klarian, and Alexander Mikalev, Sr. Solutions Architect for InfinyOn. (37 minutes)","title":"Real-time Pipeline Monitoring for the Energy Sector","url":"http://localhost:1315/videos/real-time-pipeline-monitoring/"},{"body":"","description":"Learn how to deploy AI and machine learning on the live data stream.","keywords":null,"summary":"","title":"Real-time Pipeline Monitoring for the Energy Sector","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/5351212469941392217/"},{"body":"","description":"Learn how to set up their InfinyOn cloud environment, install the CLI, create a topic, and configure connectors.","keywords":null,"summary":"","title":"Best practices for building real-time data pipelines with transformations","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/413196898453812054/"},{"body":" Prev Next Page: / Download ","description":"Enhance your Rust programming skills and increase your efficiency with this comprehensive Rust Language Cheat Sheet.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Rust Language Cheat Sheet","url":"http://localhost:1315/resources/rust-language-cheat-sheet/"},{"body":"View our most recent lightining talk with InfinyOn CTO, Sehyo Chang at DuckCon 2023. (8 minutes)\n","description":"Next generation stream processing with DuckDB integration.","keywords":null,"summary":"View our most recent lightining talk with InfinyOn CTO, Sehyo Chang at DuckCon 2023. (8 minutes)","title":"DuckCon 2023 (Lightning Talks) - Integrating DuckDB with Fluvio","url":"http://localhost:1315/resources/duckcon-2023-duckdb-fluvio/"},{"body":"View our most recent lightining talk with InfinyOn CTO, Sehyo Chang at DuckCon 2023. (8 minutes)\n","description":"Next generation stream processing with DuckDB integration.","keywords":null,"summary":"View our most recent lightining talk with InfinyOn CTO, Sehyo Chang at DuckCon 2023. (8 minutes)","title":"DuckCon 2023 (Lightning Talks) - Integrating DuckDB with Fluvio","url":"http://localhost:1315/videos/duckcon-2023-duckdb-fluvio/"},{"body":"DuckDB is an open-source SQL OLAP database that\u0026rsquo;s lightweight, fast, and user-friendly, a perfect match for Fluvio data streaming. Integrating these two technologies is a step toward Real-Time OLAP. This blog will show how to use Fluvio SmartModules and DuckDb SQL to transform data records and generate powerful materialized views.\nPrerequisite This blog assumes that you have already installed Fluvio and have a running cluster. You have 2 options:\nSign-up for InfinyOn Cloud You\u0026rsquo;ll need an InfinyOn Cloud account to download jolt from SmartModule Hub Install Fluvio on your local machine -\u0026gt; This demo also uses the development version of the Fluvio toolchain.\nLet\u0026rsquo;s get started.\nData Source In this example, we will consume live data from transit vehicles in Helsinki, Finland. The city publishes real-time metrics such as speed, acceleration, route, etc., and makes this data publicly available via MQTT. We will read this data and calculate the average speed per vehicle.\nUse Fluvio CDK to setup up an MQTT Connector We will use Fluvio\u0026rsquo;s Connector Developer Kit (CDK) to setup MQTT connector.\n-\u0026gt; CDK is currently in development, we will need to build and install development version.\nBuilding CDK This section assumes that you have Rust installed. Please follow setup instructions to install Rust and Cargo.\nClone the Fluvio repository:\n%copy first-line%\n$ git clone https://github.com/infinyon/fluvio.git Build CDK:\n%copy first-line%\n$ cd fluvio; make build-cdk RELEASE=true Check that the binary has been generated\n%copy first-line%\n$ ls target/release/cdk target/release/cdk For convenience, you may want to copy the binary to your PATH.\nBuilding MQTT Connector using CDK Now, we can deploy the MQTT connector using CDK. First, we clone the new MQTT connector repository.\n-\u0026gt; The following instructions are different from the MQTT connector in the documentation.\nClone MQTT connector repository:\n%copy first-line%\n$ cd ..; git clone https://github.com/infinyon/mqtt-connector.git Use CDk to build mqtt-connector:\n%copy first-line%\n$ cd mqtt-connector; ../fluvio/target/release/cdk build Deploying MQTT Connector to connect to Helsinki MQTT Broker Create following configuration file h1.yaml:\n%copy%\nmeta: version: latest name: h2 type: mqtt-source topic: veh1 create-topic: true mqtt: url: \u0026#34;mqtt://mqtt.hsl.fi\u0026#34; topic: \u0026#34;/hfp/v2/journey/ongoing/vp/+/+/+/#\u0026#34; client_id: \u0026#34;fluvio-mqtt\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json Then run following command to start MQTT connector:\n%copy first-line%\n$ ../fluvio/target/release/cdk deploy start --config h1.yaml This will start MQTT connector and connect to Helsinki MQTT broker. It will subscribe to the topic /hfp/v2/journey/ongoing/vp/+/+/+/# and publish the data to Fluvio topic veh1.\nYou can verify that the connector is running by running following command:\n%copy first-line%\n$ fluvio consume veh1 Consuming records from \u0026#39;veh1\u0026#39; {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/bus/0012/02244/1098/1/Rastila(M)/19:03/1453126/5/60;25/20/07/96\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;VP\u0026#34;:{\u0026#34;desi\u0026#34;:\u0026#34;98\u0026#34;,\u0026#34;dir\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;oper\u0026#34;:12,\u0026#34;veh\u0026#34;:2244,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T17:00:15.231Z\u0026#34;,\u0026#34;tsi\u0026#34;:1675357215,\u0026#34;spd\u0026#34;:0.0,\u0026#34;hdg\u0026#34;:244,\u0026#34;lat\u0026#34;:60.209415,\u0026#34;long\u0026#34;:25.076423,\u0026#34;acc\u0026#34;:0.0,\u0026#34;dl\u0026#34;:179,\u0026#34;odo\u0026#34;:90,\u0026#34;drst\u0026#34;:0,\u0026#34;oday\u0026#34;:\u0026#34;2023-02-02\u0026#34;,\u0026#34;jrn\u0026#34;:304,\u0026#34;line\u0026#34;:145,\u0026#34;start\u0026#34;:\u0026#34;19:03\u0026#34;,\u0026#34;loc\u0026#34;:\u0026#34;GPS\u0026#34;,\u0026#34;stop\u0026#34;:1453126,\u0026#34;route\u0026#34;:\u0026#34;1098\u0026#34;,\u0026#34;occu\u0026#34;:0}}} ..... If you are using InfinyOn Cloud, checkout the Dashboard:\nThis is lots of data.\nBuilding DuckDB Fluvio extension Fluvio-duck is a Fluvio extension that allows DuckDB to query data from Fluvio. It\u0026rsquo;s still in development and not yet published to crates.io. So we will need to build it from source.\nPre-requisite Building Fluvio-Duck requires full build of DuckDB which requires CMake.\nInstall on Mac:\n%copy first-line%\n$ brew install cmake For other platforms, please follow CMake installation instructions here.\nDownloading and building Fluvio-Duck Clone the repository:\n%copy first-line%\n$ cd .. \u0026amp;\u0026amp; git clone --recurse-submodules https://github.com/infinyon/fluvio-duck.git ~\u0026gt; There is a linking issue on Linux.\nAs a workaround please add the following to CMakeLists.txt:\n%copy%\ntarget_link_libraries(${LIB_NAME} \u0026#34;${CMAKE_BINARY_DIR}/src/libduckdb_static.a\u0026#34;) Then build the extension:\n%copy first-line%\n$ cd fluvio-duck \u0026amp;\u0026amp; make release This will take a while\u0026hellip; good time for a coffee break.\nOnce it is done, both DuckDB and Fluvio-Duck will be stored in the .build/release directory.\nQuerying data from Fluvio using DuckDB Now we are ready to query data from Fluvio using DuckDB. The Fluvio-Duckdb extension is using development version of DuckDB. So we will need to start DuckDB build from prev steps.\nRun DuckDb:\n%copy first-line%\n$ ./build/release/duckdb --unsigned Loading extension:\n%copy first-line%\nD load \u0026#39;./build/release/extension/fluvio-duck/fluvioduck.duckdb_extension\u0026#39;; Now we can use the extension to query data from Fluvio.\nGetting topics and partitions Fluvio object such as topics and partitions can be queried directly from DuckDB.\nRetrieve topics:\n%copy first-line%\nD select * from fluvio_topics(); ┌─────────┬────────────┐ │ name │ partitions │ │ varchar │ int32 │ ├─────────┼────────────┤ │ veh1 │ 1 │ └─────────┴────────────┘ Topic h1 was created by the MQTT connector. If there are other topics they are also displayed.\n%copy first-line%\nD select * from fluvio_partitions(); ┌─────────┬───────────┬───────┐ │ topic │ partition │ LEO │ │ varchar │ varchar │ int32 │ ├─────────┼───────────┼───────┤ │ veh1 │ 0 │ 28994 │ └─────────┴───────────┴───────┘ Querying Fluvio topics With SQL, you can query Fluvio topics and materialize as SQL table.\nThe SQL command has the following format:\nD select \u0026lt;param\u0026gt; from fluvio_consume(\u0026#39;\u0026lt;topic_name\u0026gt; \u0026lt;options\u0026gt;\u0026#39;); Feed Fluvio CLI commands as function parameters and queries as SQL parameters.\nFor example, run the following command to get last 5 events from topic h1:\n%copy first-line%\nD select * from fluvio_consume(\u0026#39;veh1 --tail 5\u0026#39;); ┌────────┬──────────────────────┬─────────────────────────────────────────────────────────────────────────────────────┐ │ offset │ timestamp │ value │ │ int32 │ timestamp_ms │ varchar │ ├────────┼──────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤ │ 100914 │ 2023-02-02 17:10:5… │ {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/bus/0022/00921/2549/2/Tapiola (M)/18:44… │ │ 100915 │ 2023-02-02 17:10:5… │ {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/bus/0022/00967/2582/2/Espoon keskus/18:… │ │ 100916 │ 2023-02-02 17:10:5… │ {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/bus/0022/01143/2510/2/Herttoniemi(M)/18… │ │ 100917 │ 2023-02-02 17:10:5… │ {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/train/0090/06055/3001R/1/Riihimäki/16:3… │ │ 100918 │ 2023-02-02 17:10:5… │ {\u0026#34;mqtt_topic\u0026#34;:\u0026#34;/hfp/v2/journey/ongoing/vp/train/0090/01081/3001I/1/Lentoas. - Hel… │ └────────┴──────────────────────┴─────────────────────────────────────────────────────────────────────────────────────┘ You can ask for Fluvio help by using the --help option:\n%copy first-line%\nD select * from fluvio_consume(\u0026#39;--help\u0026#39;); .... help command output Using SmartModules to transform MQTT data Fluvio connector converts the MQTT data to JSON format. Next, we can use Fluvio SmartModules to transform the data and make it suitable for queries and analytics.\nIn this case, we only want the following fields from the data lat, long, veh. While at it, we also want to rename fields for readability: veh to vehicle. The jolt SmartModule published in the SmartModule Hub can be used for this purpose.\nTo download the jolt SmartModule, you\u0026rsquo;ll need an InfinyOn Cloud account.\nDownload the SmartModule using fluvio CLI:\n%copy first-line%\n$ fluvio hub sm download infinyon/jolt@0.3.0 downloading infinyon/jolt@0.3.0 to infinyon-jolt-0.3.0.ipkg ... downloading complete ... checking package trying connection to fluvio router.infinyon.cloud:9003 ... cluster smartmodule install complete Transformation file The Jolt transformation step is defined in the following YAML file: jolt.yaml\n%copy%\ntransforms: - uses: infinyon/jolt@0.3.0 with: spec: - operation: shift spec: payload: VP: lat: \u0026#34;lat\u0026#34; long: \u0026#34;long\u0026#34; veh: \u0026#34;vehicle\u0026#34; route: \u0026#34;route\u0026#34; spd: \u0026#34;speed\u0026#34; tst: \u0026#34;tst\u0026#34; Querying with transformation To get last 10 events from topic veh1 and transform the data using jolt.yaml file:\n%copy first-line%\nD select * from fluvio_consume(\u0026#39;veh1 --tail 5 --transforms-file=jolt.yaml\u0026#39;); ┌────────┬──────────────────────┬─────────────────────────────────────────────────────────────────────────────────────┐ │ offset │ timestamp │ value │ │ int32 │ timestamp_ms │ varchar │ ├────────┼──────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤ │ 728564 │ 1969-12-31 23:59:5… │ {\u0026#34;lat\u0026#34;:60.172147,\u0026#34;long\u0026#34;:24.947603,\u0026#34;route\u0026#34;:\u0026#34;1055\u0026#34;,\u0026#34;speed\u0026#34;:5.96,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T1… │ │ 728565 │ 1969-12-31 23:59:5… │ {\u0026#34;lat\u0026#34;:60.20546,\u0026#34;long\u0026#34;:24.878425,\u0026#34;route\u0026#34;:\u0026#34;1500\u0026#34;,\u0026#34;speed\u0026#34;:4.99,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T17… │ │ 728566 │ 1969-12-31 23:59:5… │ {\u0026#34;lat\u0026#34;:60.178948,\u0026#34;long\u0026#34;:24.828018,\u0026#34;route\u0026#34;:\u0026#34;2550\u0026#34;,\u0026#34;speed\u0026#34;:9.72,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T1… │ │ 728567 │ 1969-12-31 23:59:5… │ {\u0026#34;lat\u0026#34;:60.209576,\u0026#34;long\u0026#34;:25.076889,\u0026#34;route\u0026#34;:\u0026#34;1082\u0026#34;,\u0026#34;speed\u0026#34;:0.0,\u0026#34;tst\u0026#34;:\u0026#34;2023-02-02T17… │ .... Mapping JSON columns to SQL columns In the previous example, the JSON data is returned as a single column. That still make harder to analyze using SQL or DuckDB. You can map the JSON columns to SQL columns using the -c option. The -c option takes a column name and a JSON path. The JSON path is a dot separated path to the JSON column. For example, to map the lat column to d column, you can use -c lat:d=\u0026quot;lat\u0026quot;.\nFollowing example show how to create materialized view with mapped columns:\n%copy%\nD create view transit as select * from fluvio_consume(\u0026#39; veh1 --tail 5 --transforms-file=jolt.yaml -c lat:d=\u0026#34;lat\u0026#34; -c long:d=\u0026#34;long\u0026#34; -c vehicle:i=\u0026#34;vehicle\u0026#34; -c route=\u0026#34;route\u0026#34; -c speed:d=\u0026#34;speed\u0026#34; -c time:t=\u0026#34;tst\u0026#34; \u0026#39;); Let\u0026rsquo;s run it:\n%copy first-line%\nD select * from transit; ┌─────────────┬─────────────┬─────────┬─────────┬────────┬─────────────────────────┐ │ lat │ long │ vehicle │ route │ speed │ time │ │ double │ double │ int32 │ varchar │ double │ timestamp_ms │ ├─────────────┼─────────────┼─────────┼─────────┼────────┼─────────────────────────┤ │ 60.208204 │ 24.974945 │ 693 │ 1055 │ 0.01 │ 2023-02-02 17:27:34.587 │ │ 60.208783 │ 24.947053 │ 19 │ 1069 │ 3.66 │ 2023-02-02 17:27:34.564 │ │ 60.741549 │ 24.782922 │ 6079 │ 3001R │ 0.0 │ 2023-02-02 17:27:34.558 │ │ 60.178087 │ 24.950206 │ 424 │ 1006 │ 2.83 │ 2023-02-02 17:27:34.57 │ │ 60.221636 │ 24.896216 │ 1416 │ 1052 │ 11.21 │ 2023-02-02 17:27:34.553 │ ├─────────────┴─────────────┴─────────┴─────────┴────────┴─────────────────────────┤ │ 5 rows 6 columns │ └──────────────────────────────────────────────────────────────────────────────────┘ Now fields are mapped into SQL readable columns, you can use SQL commands to perform analysis on the data. For example, let\u0026rsquo;s get the average speed of the vehicles by route:\n%copy first-line%\nD select route, avg(speed) from transit group by route; ┌─────────┬──────────────────────┐ │ route │ avg(speed) │ │ varchar │ double │ ├─────────┼──────────────────────┤ │ 31M2 │ 17.02 │ │ 31M1 │ 12.065000000000001 │ │ 9641 │ 12.66 │ │ 1071 │ 7.045 │ │ 1506 │ 4.605 │ ├─────────┴──────────────────────┤ │ 5 rows 2 columns │ └────────────────────────────────┘ Converting fluvio topic data to Parquet Previous examples show how to consume data from fluvio topics and perform SQL analysis on the data. You can also convert the data to Parquet format and perform analysis using Parquet tools. To convert the data to Parquet format, you can use the COPY command:\nFirst install Parquet extensions into DuckDB:\n%copy first-line%\nD INSTALL parquet; Load \u0026#39;parquet\u0026#39;; The comman structure to copy data into a parquet format is the following:\nD COPY (SELECT * FROM \u0026lt;fluvio_topic\u0026gt;) TO \u0026#39;\u0026lt;parquet_file\u0026gt;\u0026#39; (FORMAT \u0026#39;parquet\u0026#39;); To convert the data from transit materialized view to helsinki.parquet file, you can run the following command:\n%copy first-line%\nD COPY (SELECT * FROM transit) TO \u0026#39;helsinki.parquet\u0026#39; (FORMAT \u0026#39;parquet\u0026#39;); -\u0026gt; Current version of fluvio-duck extension is not yet optimized for performance. It is recommended to use the COPY command for small data sets.\nTo read back from the parquet file, use:\n%copy first-line%\nD select * from read_parquet(\u0026#39;helsinki.parquet\u0026#39;) ; ┌─────────────┬─────────────┬─────────┬─────────┬────────┬─────────────────────────┐ │ lat │ long │ vehicle │ route │ speed │ time │ │ double │ double │ int32 │ varchar │ double │ timestamp_ms │ ├─────────────┼─────────────┼─────────┼─────────┼────────┼─────────────────────────┤ │ 60.208204 │ 24.974945 │ 693 │ 1055 │ 0.01 │ 2023-02-02 17:27:34.587 │ │ 60.208783 │ 24.947053 │ 19 │ 1069 │ 3.66 │ 2023-02-02 17:27:34.564 │ │ 60.741549 │ 24.782922 │ 6079 │ 3001R │ 0.0 │ 2023-02-02 17:27:34.558 │ │ 60.178087 │ 24.950206 │ 424 │ 1006 │ 2.83 │ 2023-02-02 17:27:34.57 │ │ 60.221636 │ 24.896216 │ 1416 │ 1052 │ 11.21 │ 2023-02-02 17:27:34.553 │ ├─────────────┴─────────────┴─────────┴─────────┴────────┴─────────────────────────┤ │ 5 rows 6 columns │ └──────────────────────────────────────────────────────────────────────────────────┘ Conclusion In this blog post, we showed that it is possible to use DuckDB to perform SQL analysis on data from Fluvio topics. This is just beginning of the integration between Fluvio and DuckDB. Join our community on Discord to give us feedback on the integration of Fluvio and DuckDB. Let us know if there are other use cases you\u0026rsquo;ll find valuable.\nFurther reading Handling XML data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly ","description":"Learn how to use how to utilize Fluvio SmartModules and DuckDb SQL to convert data records and create effective materialized views.","keywords":null,"summary":"DuckDB is an open-source SQL OLAP database that\u0026rsquo;s lightweight, fast, and user-friendly, a perfect match for Fluvio data streaming. Integrating these two technologies is a step toward Real-Time OLAP. This blog will show how to use Fluvio SmartModules and DuckDb SQL to transform data records and generate powerful materialized views.\nPrerequisite This blog assumes that you have already installed Fluvio and have a running cluster. You have 2 options:\nSign-up for InfinyOn Cloud You\u0026rsquo;ll need an InfinyOn Cloud account to download jolt from SmartModule Hub Install Fluvio on your local machine -\u0026gt; This demo also uses the development version of the Fluvio toolchain.","title":"Using DuckDB with Fluvio","url":"http://localhost:1315/blog/2023/02/duckdb-fluvio/"},{"body":"For the past few years, organizations have been adopting real-time streaming services but continue using batch processing for machine learning ML tools and analytics. Using databases and ETL tools as a bridge between real-time and ML adds unnecessary complexity and lengthens the time to resolution. This blog aims to demonstrate that ML tools can interact with real-time streams using Python without needing ETL. You will create an account in InfinyOn Cloud, set up a Jupyter Notebook environment, and write a small script that joins the two. Let\u0026rsquo;s begin the journey to real-time analytics.\nA running example:\nPrerequisite This blog assumes the following:\nAn active InfinyOn Cloud Account - follow this tutorial to setup an account and provision a fluivo cluster. Familiarity with Google Colab or Jupyter Notebooks. An email account with OAuth 2.0 support. Let\u0026rsquo;s get started.\nProvision Data Streaming topics in InfinyOn Cloud Install fluvio CLI to manage your cluster in InfinyOn Cloud:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\nLogin into your InfinyOn cloud account:\n%copy first-line%\n$ fluvio cloud login --use-oauth2 Create a new topic for our streams:\n%copy first-line%\n$ fluvio topic create hello-python Copy below data and save into a data.json file:\n%copy%\n{\u0026#34;c\u0026#34;:27.55,\u0026#34;d\u0026#34;:0.41,\u0026#34;dp\u0026#34;:1.5107,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709682} {\u0026#34;c\u0026#34;:27.55,\u0026#34;d\u0026#34;:0.41,\u0026#34;dp\u0026#34;:1.5107,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709682} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} {\u0026#34;c\u0026#34;:27.56,\u0026#34;d\u0026#34;:0.42,\u0026#34;dp\u0026#34;:1.5475,\u0026#34;h\u0026#34;:27.74,\u0026#34;l\u0026#34;:26.15,\u0026#34;o\u0026#34;:26.3,\u0026#34;pc\u0026#34;:27.14,\u0026#34;t\u0026#34;:1668709710} Populate the topic with the records above. For simplicity, we are pre-populate the data stream, but this data could be populated in real-time by an event driven client or connector.\n%copy first-line%\n$ fluvio produce hello-python -f data.json Create output topic using CLI:\n%copy first-line%\n$ fluvio topic create hello-python-out Run in Google Colab Open Notebook in Google Colab, and follow the instructions:\nAfter running list(lazy) let\u0026rsquo;s check the resulting stream:\n%copy first-line%\n$ fluvio consume hello-python-out -Bd Consuming records from \u0026#39;hello-python-out\u0026#39; starting from the beginning of log {\u0026#34;c\u0026#34;: 27.55, \u0026#34;d\u0026#34;: 0.41, \u0026#34;dp\u0026#34;: 1.5107, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709682, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.55, \u0026#34;d\u0026#34;: 0.41, \u0026#34;dp\u0026#34;: 1.5107, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709682, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} {\u0026#34;c\u0026#34;: 27.56, \u0026#34;d\u0026#34;: 0.42, \u0026#34;dp\u0026#34;: 1.5475, \u0026#34;h\u0026#34;: 27.74, \u0026#34;l\u0026#34;: 26.15, \u0026#34;o\u0026#34;: 26.3, \u0026#34;pc\u0026#34;: 27.14, \u0026#34;t\u0026#34;: 1668709710, \u0026#34;median\u0026#34;: 26.72} Congratulations, you are all set! In the next section, we\u0026rsquo;ll go over the setups required to run Jupyter locally.\nRun on Local Machine There are a couple of prerequisites to run Jupyter on your local machine:\nInstall Conda Use Conda to create a new environment:\n%copy%\nconda create -n fluvio_env python=3.9 Activate the environment:\n%copy%\nconda activate fluvio_env Install jupyter and pandas:\n%copy%\npip install jupyter pip install pandas conda install -y matplotlib Start jupyter notebook:\n%copy%\njupyter notebook Create a new project:\nNext, run the same steps as in Google Colab example above:\n%copy%\n!pip install fluvio==0.14.2 Login to InfinyOn cloud, and login using OAuth2 process with Google:\n%copy%\nfrom fluvio import cloud cloud.login() Import dependencies:\n%copy%\nimport json import itertools from fluvio import Fluvio, Offset Connect to hello-python topic in InfinyOn cloud, and create consumer:\n%copy%\nTOPIC_NAME = \u0026#34;hello-python\u0026#34; PARTITION = 0 fluvio = Fluvio.connect() records=[] consumer = fluvio.partition_consumer(TOPIC_NAME, PARTITION) Read the first eight records from the beginning of the data stream:\n%copy%\nrecords = (json.loads(record.value_string()) for record in itertools.islice(consumer.stream(Offset.beginning()), 8)) This line runs instantly because it creates a generator. Generaters are interepreted from right to left:\nCreate a stream consumer Take a slice of 8 records using itertools.islice Turn each record into json by record.value_string() string and json.loads Let\u0026rsquo;s turn the eight records into pandas dataframe using json_normalize:\n%copy%\nimport pandas df = pandas.json_normalize(records) df.plot() Now you can apply any pandas data transformation or action to the streamed data. For example add column with median:\n%copy%\ndf[\u0026#39;median\u0026#39;] = df.median(numeric_only=True, axis=1) And if you set offset to read from the end of the stream via Offset.end(), the notebook cell will be locked until you start populating data into the stream.\nThe next step is to write back data into the stream, and we can do it using lazy using python generators as well. Create output producer:\n%copy%\nOUTPUT_TOPIC=\u0026#34;hello-python-out\u0026#34; producer = fluvio.topic_producer(OUTPUT_TOPIC) Create a lazy producer generator:\n%copy%\nlazy = (producer.send_string(json.dumps(i)) for i in df.to_dict(\u0026#34;records\u0026#34;)) It runs instantly and it only returns iterable. Evaluate generator:\n%copy%\nlist(lazy) Conclusion In this blog post, we highlighted the significance of streams in contemporary development and emphasized that working with data streams in Python can be straightforward. The example showed how to read, process, and write streams via Python generators to make the process fast and efficient. The code used in the demonstration is available on Google Colab at this link.\nMachine learning developers can now bridge their Jupyter notebook experiments with real-time streaming capabilities with ease. So, be sure to join our Discord server if you want to talk to us or have any questions. Until next time!\nFurther reading Handling XML data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly How to Write to Apache Kafka from a Fluvio topic ","description":"Learn how to integrate Jupyter notebooks with real-time data streams using Python and InfinyOn Cloud.","keywords":null,"summary":"For the past few years, organizations have been adopting real-time streaming services but continue using batch processing for machine learning ML tools and analytics. Using databases and ETL tools as a bridge between real-time and ML adds unnecessary complexity and lengthens the time to resolution. This blog aims to demonstrate that ML tools can interact with real-time streams using Python without needing ETL. You will create an account in InfinyOn Cloud, set up a Jupyter Notebook environment, and write a small script that joins the two.","title":"How to process streaming data using Google Colab or Jupyter Notebook","url":"http://localhost:1315/blog/2023/01/jupyter-with-real-time-data/"},{"body":" Prev Next Page: / Download ","description":"Learn about the latest and most impactful data streaming use cases in our 2023 Guide to stay ahead in the field of data management.","keywords":null,"summary":" Prev Next Page: / Download ","title":"2023 Guide to Data Streaming Use Cases","url":"http://localhost:1315/resources/2023-guide-to-data-streaming-use-cases/"},{"body":"When building any kind of real-time IoT application, trying to figure out how to send data from the sensor to the cloud (or vice versa) is a big part of the equation. Ben Cleary, Head of Technology from Klarian, is focused on leading the software development and data teams as they continue to innovate on their Digipipe platform. The product allows users to monitor their operations and physical pipelines used to transport products including oil, gas, or hydrogen. Klarian’s mission is to optimize pipeline infrastructure for a more sustainable planet.\nKlarian has built a real-time IoT application that captures data from multiple sources, including their edge controller and sensor hardware, systems, or processes. Beyond the sensor data, they use external APIs to gather important information such as commodity and energy pricing. Third-party data integrations from existing SCADA, Field Ops, and incident management systems are ingested, processed, and stored. Klarian customers get tremendous value from Klarian\u0026rsquo;s analysis of this data to find meaningful insights, which generate actions needed to make better decisions.\nData ingestion initially started as a complicated architecture for Klarian. Some of the major pain points for the Klarian team included the following:\nWhy Klarian Chose InfinyOn Cloud over Kafka or RabbitMQ InfinyOn Cloud is a unified platform for event stream processing and real-time data transformation. Klarian decided to move away from Kafka and Rabbitmq to InfinyOn Cloud to reduce the cost and complexity of operating and managing real-time data pipelines. Other factors that contributed to their decision were time to market, ease of management, and improved data quality.\nTime to Market Klarian rolled out InfinyOn Cloud as an intelligent data collection pipeline fronting the Karian IoT application. The cloud platform is a versatile multi-dimensional product for data ingestion, real-time data visualization, STL workflow, and Streaming AI. Check out the following diagram for additional information:\n“InfinyOn Cloud is the heart of our data analytics platform” said Rupert Young, Klarian\u0026rsquo;s Senior Delivery Manager\nSimplified Architecture with Ease of Management Klarian has a small entrepreneurial team and did not want to deal with DevOps overhead associated with Kafka or RabbitMQ. Instead, they chose InfinyOn Cloud, a fully managed service, where they built end-to-end data pipelines with real-time data transformations in a fraction of the time compared to the other solutions. In their new deployment, Klarian Edge Controller, on the left, streams data into InfinyOn Cloud via the MQTT connector. Then, the platform performs a series of transformations and sends the result to Postgres via SQL sink. See the diagram below for details.\nCost Reduction As a bonus, Ben and his team were pleased to realize that The Cost of Running Fluvio vs. Apache Kafka, also offered them significant cost advantages. InfinyOn Cloud is a managed service for Fluvio, an open-source product built from the ground up using the Rust programming language. The The Java vs Rust comparison provides a detailed analysis of the benefits of using Rust for industry-leading high-performance services.\nDistributed Intelligence for Superior Data Quality An IoT service\u0026rsquo;s performance, security, resilience, and scalability are highly correlated with the quality of the data delivery service. No amount of strategy and new technology will help if the data isn\u0026rsquo;t accurate, secure, and actionable. SmartModule Hub, a recently launched service in InfinyOn Cloud, is an industry-first app store where users can share and reuse data transformations to accelerate the roll-out of intelligent data pipelines. InfinyOn SmartModules, powered by WebAssembly, allows users to program unique functions, apply intelligence and enrich data for superior data quality.\nOne of the exciting aspects of the SmartModule Hub is helping organizations take control of the explosion of data from IoT services and beyond. When SmartModules are distributed between edge and core, they form a hierarchical cooperative intelligent system that can optimize data transfers, improve predictive maintenance, and minimize operational overhead.\nAdditional IoT Use-cases InfinyOn Cloud can be used for numerous IoT use cases, including predictive maintenance, supply chain automation, and real-time inventory management. If you are building a real-time IoT application and want to meet the InfinyOn team we would be delighted to spend some time with you to discuss your use-case.\nFeel free to schedule time here. We also welcome you to become a member of our Discord channel, follow us on LinkedIn or Twitter and try InfinyOn Cloud.\n","description":"Why Klarian Chose InfinyOn Cloud over Kafka or RabbitMQ.","keywords":null,"summary":"When building any kind of real-time IoT application, trying to figure out how to send data from the sensor to the cloud (or vice versa) is a big part of the equation. Ben Cleary, Head of Technology from Klarian, is focused on leading the software development and data teams as they continue to innovate on their Digipipe platform. The product allows users to monitor their operations and physical pipelines used to transport products including oil, gas, or hydrogen.","title":"Building Real-time IoT Apps.","url":"http://localhost:1315/blog/2022/02/building-real-time-iot-applications/"},{"body":"This webinar will provide a step-by-step process on how to build real-time data pipelines that include inline transformations to structure and format IoT sensor data. Details include how to:\n• Start your cloud environment\n• Create a topic to store IoT data\n• Get data into InfinyOn Cloud with an inbound connector\n• Transform IoT data in real-time using a SmartModule\n• Create an outbound connector to a SQL database\n","description":"Gain valuable insights and practical knowledge on how to effectively analyze real-time data streams in IoT applications by watching our webinar.","keywords":null,"summary":"This webinar will provide a step-by-step process on how to build real-time data pipelines that include inline transformations to structure and format IoT sensor data. Details include how to:\n• Start your cloud environment\n• Create a topic to store IoT data\n• Get data into InfinyOn Cloud with an inbound connector\n• Transform IoT data in real-time using a SmartModule\n• Create an outbound connector to a SQL database","title":"IoT Streaming Analytics with Real-time Data","url":"http://localhost:1315/resources/iot-streaming-analytics/"},{"body":"This webinar will provide a step-by-step process on how to build real-time data pipelines that include inline transformations to structure and format IoT sensor data. Details include how to:\n• Start your cloud environment\n• Create a topic to store IoT data\n• Get data into InfinyOn Cloud with an inbound connector\n• Transform IoT data in real-time using a SmartModule\n• Create an outbound connector to a SQL database\n","description":"Gain valuable insights and practical knowledge on how to effectively analyze real-time data streams in IoT applications by watching our webinar.","keywords":null,"summary":"This webinar will provide a step-by-step process on how to build real-time data pipelines that include inline transformations to structure and format IoT sensor data. Details include how to:\n• Start your cloud environment\n• Create a topic to store IoT data\n• Get data into InfinyOn Cloud with an inbound connector\n• Transform IoT data in real-time using a SmartModule\n• Create an outbound connector to a SQL database","title":"IoT Streaming Analytics with Real-time Data","url":"http://localhost:1315/videos/iot-streaming-analytics/"},{"body":"","description":"Learn how to build real-time data pipelines for IoT streaming analytics.","keywords":null,"summary":"","title":"IoT Streaming Analytics with Real-time Data","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/1655064493983483404/"},{"body":"Ben Cleary, the Head of Technology for Klarian explains the problems he was trying to solve when building out their Digipipe Platform that allows users to monitor their oil and gas pipelines in real time.\nKlarian captures data from multiple sources (sensors or systems), then processes and analyzes it to find meaningful insights, generating actions to give users the information needed to make better decisions.\nInfinyOn Cloud, a fully managed Fluvio service, was selected by Klarian for event stream processing and real-time data transformation. Data ingestion, real-time data visualization, and streaming AI are a few of the ways Klarian uses InfinyOn Cloud to power its DigiPipe platform.\nMoving away from the traditional ETL workflows, Klarian has transitioned to Stream, Transform, and Load (STL) workflows to simplify its middle-tier data architecture.\n","description":"Klarian Selects InfinyOn Cloud for MQTT to Postgres Use-Case for Real-time Pipeline Monitoring.","keywords":null,"summary":"Ben Cleary, the Head of Technology for Klarian explains the problems he was trying to solve when building out their Digipipe Platform that allows users to monitor their oil and gas pipelines in real time.\nKlarian captures data from multiple sources (sensors or systems), then processes and analyzes it to find meaningful insights, generating actions to give users the information needed to make better decisions.\nInfinyOn Cloud, a fully managed Fluvio service, was selected by Klarian for event stream processing and real-time data transformation.","title":"MQTT to Postgres Use Case from Klarian","url":"http://localhost:1315/resources/mqtt-to-postgres-use-case/"},{"body":"Ben Cleary, the Head of Technology for Klarian explains the problems he was trying to solve when building out their Digipipe Platform that allows users to monitor their oil and gas pipelines in real time.\nKlarian captures data from multiple sources (sensors or systems), then processes and analyzes it to find meaningful insights, generating actions to give users the information needed to make better decisions.\nInfinyOn Cloud, a fully managed Fluvio service, was selected by Klarian for event stream processing and real-time data transformation. Data ingestion, real-time data visualization, and streaming AI are a few of the ways Klarian uses InfinyOn Cloud to power its DigiPipe platform.\nMoving away from the traditional ETL workflows, Klarian has transitioned to Stream, Transform, and Load (STL) workflows to simplify its middle-tier data architecture.\n","description":"Klarian Selects InfinyOn Cloud for MQTT to Postgres Use-Case for Real-time Pipeline Monitoring.","keywords":null,"summary":"Ben Cleary, the Head of Technology for Klarian explains the problems he was trying to solve when building out their Digipipe Platform that allows users to monitor their oil and gas pipelines in real time.\nKlarian captures data from multiple sources (sensors or systems), then processes and analyzes it to find meaningful insights, generating actions to give users the information needed to make better decisions.\nInfinyOn Cloud, a fully managed Fluvio service, was selected by Klarian for event stream processing and real-time data transformation.","title":"MQTT to Postgres Use Case from Klarian","url":"http://localhost:1315/videos/mqtt-to-postgres-use-case/"},{"body":"SANTA CLARA, CA \u0026ndash; Klarian, an intelligent pipeline monitoring company, announces a strategic technology agreement with InfinyOn. The agreement with InfinyOn, a real-time event streaming company, will drive innovation into the energy sector with real-time data and actionable insights. Klarian has developed Digipipe, an end-to-end pipeline monitoring solution with a performance dashboard that integrates with InfinyOn, to stream, transform and load events in real-time.\nPipelines transport some of our most valuable natural resources. Pipeline failures can have devastating environmental, social, and financial consequences. Operators need real-time insights into operational data to reduce risk, optimise production, and comply with regulation and environmental goals. Data is collected from Klarian sensors and events are continuously streaming to the Digipipe Cloud Platform with single digit millisecond latency. Operations teams no longer have to rely on old data generated through traditional batch processing.\nEmpowering users, developers, and data analysts with the ability to collect their own supplementary data alongside the primary Klarian sensor data using Digipipe is a key objective. The Klarian team evaluated several potential solutions to enable Digipipe to operate in real-time with services such as System Control and Data Acquisition (SCADA), IoT platforms, and serverless functions. The same problems occurred. Proprietary protocols, vendor platform lock-in, and complicated tooling that is expensive and difficult to manage.\nTraditional tooling for event streaming such as Kafka, AirFlow, and Broadway was considered by Klarian to quickly develop its platform with real-time data. Building an event streaming architecture that ingests, transforms and distributes data results in a complicated mesh of data acquisition, data transformation, and data storage services.\nKlarian selected InfinyOn Cloud, a unified platform for real-time event streaming and data transformation, to power Digipipe. “We started working with more traditional systems and services but quickly identified that they were not a viable option. By deploying InfinyOn Cloud we were able to improve our engineering velocity. The days of blocking deployment or teams waiting for services to be deployed are gone for us.” said Ben Cleary, Head of Technology at Klarian.\n“Working with InfinyOn has enabled Klarian to realize substantial performance improvements compared to Kafka. During the initial proof of concept, InfinyOn was able to deliver a 3x reduction in latency, 5x improvement in throughput, 7x better CPU utilization and 50x better memory utilization” said Seyho Chang, CTO of InfinyOn.\nInfinyOn Cloud will be the backbone of the Klarian data analytics platform. Klarian plans to continue developing its Digipipe product with upcoming features such as the InfinyOn Smart Module Hub, an industry first data transformation App Store. Together, Klarian and InfinyOn will collect pipeline data from upstream to midstream and transform it into performance clarity for operators in real-time.\nAbout Klarian The Klarian mission is to optimise pipeline infrastructure for a more sustainable planet. Our intelligent pipeline monitoring solution, Digipipe, captures operations data to mitigate risks, optimise production, improve efficiencies, and comply with regulation and environmental goals. Proprietary algorithms reveal meaningful data, hidden insights, and actionable intelligence to give operations the information needed to manage assets safely, efficiently, and profitably. From upstream to downstream, Klarian transforms pipeline efficiency.\nAbout InfinyOn InfinyOn, a real-time event streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. Founded in 2019 by A.J. Hunyady and Sehyo Chang, InfinyOn aims to accelerate organizations\u0026rsquo; journey to the real-time economy. SmartModules enable enterprises to program their data pipelines as they flow between producers and consumers for real-time services. Its Cloud product lets enterprises quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\n","description":"Driving innovation into the energy sector with real-time data.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; Klarian, an intelligent pipeline monitoring company, announces a strategic technology agreement with InfinyOn. The agreement with InfinyOn, a real-time event streaming company, will drive innovation into the energy sector with real-time data and actionable insights. Klarian has developed Digipipe, an end-to-end pipeline monitoring solution with a performance dashboard that integrates with InfinyOn, to stream, transform and load events in real-time.\nPipelines transport some of our most valuable natural resources.","title":"Klarian and InfinyOn Agree to Jointly Deliver Real-time Pipeline Monitoring","url":"http://localhost:1315/press-releases/infinyon-klarian-announcement/"},{"body":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time event streaming company, announces the v0.10 beta release of the InfinyOn Cloud platform. Building an infrastructure for real-time event streaming is challenging, expensive to scale, and difficult to operate. InfinyOn Cloud provides a modern approach that reduces the cost and complexity of operating and managing real-time data pipelines.\nEnterprises can bring real-time intelligence to data pipelines for Edge and Cloud use cases with InfinyOn Cloud. “We were able to rapidly build an event driven architecture with end to end data pipelines that sourced MQTT data, with inline transformations and sink it to a postgres database” said Ben Cleary, CTO of Klarian. “InfinyOn Cloud has enabled Klarian to build highly tuned data models, delivering actionable insights within the energy sector.”\nInfinyOn SmartModules, powered by WebAssembly, allows users to program unique functions, apply intelligence and enrich data. “WebAssembly allows small data processing modules to be moved anywhere at a fraction of the cost when compared with VMs or containers\u0026rsquo;\u0026rsquo; said Alex Williams, Founder and Publisher for The New Stack. InfinyOn users can apply filters, mask private fields, remove invalid records, and use machine learning at the edge or in the Cloud in a single platform.\nShift from ETL to STL InfinyOn introduces a new concept of Stream, Transform and Load (STL), a fundamental paradigm shift from Extract, Transform and Load (ETL). Traditional ETL pipelines are batch driven and often consist of a fragmented architecture that is difficult to manage and includes lots of software or microservices to make data usable.\n“Implementing the novel concept of STL, users can quickly build programmable data pipelines for real-time actionable insights.” said Grant Swanson, InfinyOn VP of Marketing. “Eliminating ETL tools creates substantial cost savings and a simplified architecture for our users.”\nIndustry First Data Transformation App Store - SmartModule Hub SmartModule Hub is an industry-first app store where users can share and reuse data transformations to quickly build sophisticated data pipelines.\n“SmartModule Hub is a centralized catalog that empowers engineers to build and share stream transformations across the organization.” said Infinyon CTO, Sehyo Chang. “Data Engineering, Data Science and Machine Learning Ops teams will benefit by leveraging existing SmarModules instead of writing custom transformations. The combination of STL and Smartmodule Hub accelerates development and significantly reduces time to market.”\nIoT Edge and Cloud Use Cases Gartner predicts that ”By 2025, more than 50% of enterprise-managed data will be created and processed outside the data center or cloud.” IoT big data statistics show that with the increased adoption of IoT devices, the amount of data generated will reach 73.1 zettabytes (ZB) by 2025. I\u0026amp;O leaders must plan for the growth of IoT and related data at the edge, protect their enterprise from edge growing pains in security and connectivity, and prepare for changes in edge computing use cases.\nThe InfinyOn Real-time Data Platform bridges the edge to the cloud. Distributing intelligence with SmartModules between edge and cloud plays a critical role in developing valuable insights for business decisions across all sectors and industries. InfinyOn Cloud can be used for numerous use-cases including predictive maintenance, supply chain automation and real-time inventory management while minimizing data transfers.\nAbout InfinyOn InfinyOn, a real-time event streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. Founded in 2019 by A.J. Hunyady and Sehyo Chang, InfinyOn aims to accelerate organizations\u0026rsquo; journey to the real-time economy. SmartModules enable enterprises to program their data pipelines as they flow between producers and consumers for real-time services. Its Cloud product lets enterprises quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\n","description":"Unified Platform for Real-time Event Streaming and Data Transformation.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time event streaming company, announces the v0.10 beta release of the InfinyOn Cloud platform. Building an infrastructure for real-time event streaming is challenging, expensive to scale, and difficult to operate. InfinyOn Cloud provides a modern approach that reduces the cost and complexity of operating and managing real-time data pipelines.\nEnterprises can bring real-time intelligence to data pipelines for Edge and Cloud use cases with InfinyOn Cloud.","title":"InfinyOn Announces the Release of InfinyOn Cloud a Real-time Data Platform","url":"http://localhost:1315/press-releases/infinyon-cloud-announcement/"},{"body":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time event streaming company, raised a $5M round of seed funding, led by Gradient Ventures and Fly Ventures, with participation from Bessemer Venture Partners, TSVC, and others.\nReal-time event streaming adoption is gaining momentum as an integral component of delivering exceptional customer experiences and enhanced operations. With products built on modern cloud-native architecture, InfinyOn seeks to become a trusted partner for organizations seeking to kick off and expand their data-in-motion journey.\n\u0026ldquo;We live in an event-driven world, where users communicate with businesses on multiple channels in real-time. Car vendors roll out services to detect accidents then call drivers and passengers to ensure everyone’s safety. E-commerce vendors track packages to inform customers of their location, medical devices track blood sugar levels and make recommendations to users,” said A.J. Hunyady, co-founder and CEO of InfinyOn. “Organizations need to react to situations in real-time so they can give their users the best experience.”\nInfinyOn built a real-time event streaming platform with in-line computation capabilities powered by WebAssembly that enables organizations to deploy and operate large-scale multi-channel data pipelines from edge to Cloud. InfinyOn gained traction across various industries, and now ranks as one of the top vendors on G2’s list for Event Stream Processing Software products. InfinyOn leverages Fluvio Open Source, a programmable platform for data-in-motion built in Rust, an up-and-coming programming language known for strong safety, small footprint, and low memory. With well over 1,000 Github stars, Fluvio is gaining popularity within the developer and open source communities.\n\u0026ldquo;Legacy data platforms built in the Java era generate large binaries, require lots of memory, and are challenging to operate from edge to core. These also lack in-line processing capabilities for real-time decisions,\u0026rdquo; said Sehyo Chang co-founder and CTO of InfinyOn. “We simplify data architectures by removing the need for ETL tools, provide a more cost-effective platform with up to 80x reduction in memory and deliver maximum security with a memory safe solution.”\n\u0026ldquo;We integrate healthcare data coming from different sources: IoT, patient inputs, and physicians. With InfinyOn we can do it rapidly and efficiently with a modern tool. It is written in Rust which makes it easier for the team to integrate compared with Kafka.\u0026rdquo; said Nammu CEO, Chris Brucker.\nInfinyOn makes it easy to extract, shape, and transform data from multiple sources and compute an outcome in real-time. While still in Beta, early adopters see compelling advantages in a proof of concept with InfinyOn compared to alternative solutions. Aside from ease of use and the speed of development, vendors see significant reductions in the total cost of ownership compared to other vendors.\n\u0026ldquo;The ability to create data pipelines that collect events from our vending machines and notify suppliers in real-time is game-changing,\u0026rdquo; said Cory Hewitt, CEO of Gimme. “Our customers want more ways to use the data in their database in making informed decisions about product inventory and product ordering at the warehouse level. More data requests equals more server demand. So we\u0026rsquo;re investing in powerful, scalable tools, like InfinyOn to queue and organize all these new data requests.”\nAbout InfinyOn InfinyOn, a real-time event streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. Founded in 2019 by A.J. Hunyady and Sehyo Chang, InfinyOn aims to accelerate organizations\u0026rsquo; journey to the real-time economy. SmartModules enable enterprises to program their data pipelines as they flow between producers and consumers for real-time services. Its Cloud product lets enterprises quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\nAbout Fly Ventures Fly Ventures is a specialized venture capital firm dedicated to backing technical founders solving hard problems from day-zero to seed. Headquartered in Berlin, Germany, Fly has been on the zero to one journey with portfolio companies who have raised in excess of $1Bn in follow-on funding from some of the world’s best VCs. For more information, visit www.fly.vc.\nAbout Gradient Ventures Gradient Ventures, Google’s AI-focused venture fund, helps founders build transformational companies. The fund focuses on helping founders navigate the challenges in developing new technology products, using the latest best practices in recruiting, marketing, design, and engineering so that great ideas can come to life. Gradient was founded in 2017 and is based in Palo Alto, California. For more information, visit www.gradient.com.\n","description":"Led by Gradient Ventures and Fly Ventures with participation from Bessemer Venture Partners and TSVC.  ","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time event streaming company, raised a $5M round of seed funding, led by Gradient Ventures and Fly Ventures, with participation from Bessemer Venture Partners, TSVC, and others.\nReal-time event streaming adoption is gaining momentum as an integral component of delivering exceptional customer experiences and enhanced operations. With products built on modern cloud-native architecture, InfinyOn seeks to become a trusted partner for organizations seeking to kick off and expand their data-in-motion journey.","title":"InfinyOn Raises $5M to deliver a Unified Platform for Event Streaming and Real-time Data Transformation","url":"http://localhost:1315/press-releases/funding-announcement/"},{"body":"This cost-savings case study from a company revolutionizing the decentralized application space is a must-read for any business looking to scale their data streaming capabilities. With $50 million in funding, thousands of users, and growing market momentum, this company was hitting the limit with their current Kafka architecture and struggling to scale. By turning to InfinyOn and conducting a proof of concept with Fluvio, they were able to achieve significant savings.\nWith a use case that required data streaming and SmartModules for extensive data transformations, they estimated they would need 22,000 partitions to run their network of users. However, with Fluvio, they were able to achieve the same results while using significantly less RAM. On average, Kafka requires ~1Gb of RAM per partition, while Fluvio only requires ~50Mb of RAM per partition. This resulted in a savings of over $41k per month for the company. By viewing this case study, you\u0026rsquo;ll see the real cost savings potential of using Fluvio over traditional Kafka architecture.\nPrev Next Page: / Download ","description":"Make an informed decision on your streaming infrastructure by comparing the costs of running Fluvio vs. Apache Kafka with our detailed analysis.","keywords":null,"summary":"This cost-savings case study from a company revolutionizing the decentralized application space is a must-read for any business looking to scale their data streaming capabilities. With $50 million in funding, thousands of users, and growing market momentum, this company was hitting the limit with their current Kafka architecture and struggling to scale. By turning to InfinyOn and conducting a proof of concept with Fluvio, they were able to achieve significant savings.","title":"The Cost of Running Fluvio vs. Apache Kafka","url":"http://localhost:1315/resources/cost-of-fluvio-vs-apache-kafka/"},{"body":"This blog shows the power of Fluvio for performing real-time data transformations and provides a step by step example of how to stream clean data to a Kafka topic. In this example we are taking source data from the Finnhub API with our HTTP source connector, aggregating stock prices, and caluclating unrealised gains or losses in real-time before we send it to Apache Kafka.\nStart Install minikube, helm, kubectr with the following instructions: https://www.fluvio.io/docs/get-started/linux/#installing-kubernetes-cluster.\nInstall Fluvio. Install Fluvio CLI:\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\nStart Fluvio Cluster:\n%copy first-line%\n$ fluvio cluster start Verify the cluster is running:\n%copy%\n$ fluvio topic create greetings echo \u0026#34;Hello, Fluvio\u0026#34; | fluvio produce greetings fluvio consume greetings -B -d Part one Recap of the Financal Services Demo • git clone https://github.com/infinyon/fluvio-demo-04-12-2022-finance-demo.git\n• register on finhub.io and obtain api token\n• update API token in quote-data-input.yaml\nCreate a HTTP connector Check if the fluvio topic is populated:\n%copy first-line%\n$ fluvio consume gme-stocks -B Start a local Apache Kafka dev Clone https://github.com/infinyon/kafka_webinar_16_August_2022 and change the value ADV_HOST in docker-compose-webinar.yml, where ADV_HOST is pinned to minikube network gateway 192.168.49.1:\ncheck minikube ip\n%copy%\n$ minikube ip 192.168.49.2 and amend ADV_HOST in docker-compose-webinar.yml\n%copy first-line%\n$ docker compose -f docker-compose-webinar.yml up -d Validate that Kafka is working\n%copy first-line%\n$ docker run --rm -it --net=host lensesio/fast-data-dev kafka-topics --zookeeper localhost:2181 --list Write to Kafka from Fluvio topic ADV_HOST and kafka_url in webinar-kafka-sink-connector.yml shall match to local IP (ifconfig| grep inet for linux)\n%copy%\nfluvio connector create -c ./webinar-kafka-sink-connector.yml fluvio connector logs -f my-kafka-sink1 Part two Apply a Smart Module to a fluvio topic before writing to Kafka Smart module calculates unrealised gains or losses. Runs an aggregate function on an assumed \u0026ldquo;purchased\u0026rdquo; stocks (warrant).\n%copy%\nfn update_profit(\u0026amp;mut self) { let mut profit = 0.0; for warrant in \u0026amp;self.warrants { profit += (self.current_price - (warrant.exercise_price + warrant.purchase_price))*warrant.count as f64; } where warrents.txt\n%copy%\n{\u0026#34;expiry_date\u0026#34;: \u0026#34;Tue, 11 Apr 2022 13:50:37 +0000\u0026#34;, \u0026#34;exercise_price\u0026#34;: 140.0, \u0026#34;purchase_price\u0026#34;: 12.0, \u0026#34;count\u0026#34;: 10} {\u0026#34;expiry_date\u0026#34;: \u0026#34;Tue, 12 Apr 2022 13:50:37 +0000\u0026#34;, \u0026#34;exercise_price\u0026#34;: 110.0, \u0026#34;purchase_price\u0026#34;: 10.0, \u0026#34;count\u0026#34;: 11} {\u0026#34;expiry_date\u0026#34;: \u0026#34;Tue, 12 Apr 2022 17:50:37 +0000\u0026#34;, \u0026#34;exercise_price\u0026#34;: 150.0, \u0026#34;purchase_price\u0026#34;: 11.0, \u0026#34;count\u0026#34;: 12} {\u0026#34;expiry_date\u0026#34;: \u0026#34;Tue, 13 Apr 2022 13:50:37 +0000\u0026#34;, \u0026#34;exercise_price\u0026#34;: 160.0, \u0026#34;purchase_price\u0026#34;: 13.0, \u0026#34;count\u0026#34;: 13} In the fluvio-demo-04-12-2022-finance-demo folder run\n%copy%\nmake sm-upload make produce-warrants make sm-consume Those commands will compile and upload a smart module. Produce warrants will generate purchase orders so current profit can be calculated.\nStart Kafka sink connector with SmartModule %copy%\nfluvio connector create -c ./webinar-kafka-sink-connector-with-sm.yml Rerun produce warrants: In fluvio-demo-04-12-2022-finance-demo run\n%copy% Sink connector reads fluvio topic from the end, and we are re-running make produce-warrants to make sure fluvio topic is populated, which is then appearing in kafka-aggregate-fluvio.\nWatch kafka topic via Web UI http://localhost:3030/kafka-topics-ui/#/cluster/fast-data-dev/topic/n/kafka-aggregate-fluvio/ or via command line:\n%copy%\ndocker run --rm -it --net=host landoop/fast-data-dev kafka-console-consumer --topic kafka-aggregate-fluvio --bootstrap-server \u0026#34;192.168.1.89:9092\u0026#34; Webinar recording: Enhance your kafka infrastructure with fluvio See webinar with live demo. Stay connected\nConnect with us: Please, be sure to join our Discord server if you want to talk to us or have any questions.\nSubscribe to our YouTube channel\nFollow us on Twitter\nFollow us on LinkedIn\nHave a happy coding, and stay tuned!\nAdditional Notes Running Kafka commands:\n%copy%\ndocker run --rm -it --net=host landoop/fast-data-dev kafka-topics --zookeeper localhost:2181 --list %copy%\ndocker run --rm -it --net=host landoop/fast-data-dev kafka-console-consumer --topic kafka-aggregate-fluvio --bootstrap-server \u0026#34;192.168.49.1:9092\u0026#34; Further reading Handling JSON data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly The InfinyOn Continuous Intelligence Platform ","description":"Aggregate and Transform data in real-time with Fluvio before writing to a Kafka topic","keywords":null,"summary":"This blog shows the power of Fluvio for performing real-time data transformations and provides a step by step example of how to stream clean data to a Kafka topic. In this example we are taking source data from the Finnhub API with our HTTP source connector, aggregating stock prices, and caluclating unrealised gains or losses in real-time before we send it to Apache Kafka.\nStart Install minikube, helm, kubectr with the following instructions: https://www.","title":"How to Write to Apache Kafka from a Fluvio topic","url":"http://localhost:1315/blog/2022/09/write-to-kafka-from-fluvio/"},{"body":"This demo covers how to install Fluvio opensource software, how to start a Fluvio cluster, how to start a lockal Kafka developement environment, and how to apply a SmartModule in order to stream clean data to a Kafka topic.\n","description":"This live demo shows the power of Fluvio SmartModules for performing data aggregations and streaming clean data to a Kafka topic.","keywords":null,"summary":"This demo covers how to install Fluvio opensource software, how to start a Fluvio cluster, how to start a lockal Kafka developement environment, and how to apply a SmartModule in order to stream clean data to a Kafka topic.","title":"How to Stream Clean Data to a Kafka Topic with Fluvio","url":"http://localhost:1315/resources/stream-clean-data-to-kafka-with-fluvio/"},{"body":"This demo covers how to install Fluvio opensource software, how to start a Fluvio cluster, how to start a lockal Kafka developement environment, and how to apply a SmartModule in order to stream clean data to a Kafka topic.\n","description":"This live demo shows the power of Fluvio SmartModules for performing data aggregations and streaming clean data to a Kafka topic.","keywords":null,"summary":"This demo covers how to install Fluvio opensource software, how to start a Fluvio cluster, how to start a lockal Kafka developement environment, and how to apply a SmartModule in order to stream clean data to a Kafka topic.","title":"How to Stream Clean Data to a Kafka Topic with Fluvio","url":"http://localhost:1315/videos/stream-clean-data-to-kafka-with-fluvio/"},{"body":"This webinar shows the power of Fluvio for real-time event streaming and data transformation. View the Kafka vs. Fluvio session to learn:\n• What are the major differences between the platforms\n• Why companies are leveraging Fluvio to compliment Kafka\n• How to stream clean data to a Kafka topic without microservices or ETL tools\n","description":"Learn how to perform real-time data transformations and write to a Kafka topic without the need for microservices or ETL tools.","keywords":null,"summary":"This webinar shows the power of Fluvio for real-time event streaming and data transformation. View the Kafka vs. Fluvio session to learn:\n• What are the major differences between the platforms\n• Why companies are leveraging Fluvio to compliment Kafka\n• How to stream clean data to a Kafka topic without microservices or ETL tools","title":"Enhance your Kafka Infrastructure with Fluvio","url":"http://localhost:1315/resources/enhance-your-kafka-infrastructure-with-fluvio/"},{"body":"This webinar shows the power of Fluvio for real-time event streaming and data transformation. View the Kafka vs. Fluvio session to learn:\n• What are the major differences between the platforms\n• Why companies are leveraging Fluvio to compliment Kafka\n• How to stream clean data to a Kafka topic without microservices or ETL tools\n","description":"Learn how to perform real-time data transformations and write to a Kafka topic without the need for microservices or ETL tools.","keywords":null,"summary":"This webinar shows the power of Fluvio for real-time event streaming and data transformation. View the Kafka vs. Fluvio session to learn:\n• What are the major differences between the platforms\n• Why companies are leveraging Fluvio to compliment Kafka\n• How to stream clean data to a Kafka topic without microservices or ETL tools","title":"Enhance your Kafka Infrastructure with Fluvio","url":"http://localhost:1315/videos/enhance-your-kafka-infrastructure-with-fluvio/"},{"body":"","description":"Learn why companies are leveraging Fluvio to compliment Kafka.","keywords":null,"summary":"","title":"Enhance your Kafka Infrastructure with Fluvio","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/7829882206451748624/"},{"body":"The JSON format remains one of the most popular text data formats for Data-in-Transition. You can encounter JSON data on every stack level of your application: from the database to UI, from IoT sensors data to the mobile app\u0026rsquo;s payload. And it is not a coincidence; the format has a good balance between being convenient for developers and decent payload density. In Rust ecosystem, the de-facto standard for dealing with JSON is Serde. Although it is the best choice for most cases, there can be alternative approaches that can work best for your application. One of these approaches we are going to cover in this article.\nThis blog is intended for Rust beginners.\nPre-conditions In order to properly follow this blog, you need to be familiar with Serde crate and JSON data format.\nScenario: Processing JSON in Rust. The format changes often. Imagine you are developing a new application that processes a stream of records in JSON format. You are not quite sure what records will look like in the future, but for now, you are currently working with this input:\n%copy%\n{ \u0026#34;id\u0026#34;: \u0026#34;0000000001\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34; } As part of your application\u0026rsquo;s pipeline, you need to transform the input into the following output:\n%copy%\n{ \u0026#34;key_id\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value_id\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;source_connector_12\u0026#34; } So the change is that we remove the id field, we rename key to key_id, value to value_id, and we add the new source field with fixed value source_connector_12.\nLet\u0026rsquo;s set up a Rust project and see what the code would look like with Serde crate:\n%copy first-line%\n$ cargo new json-to-json-serde --lib Ensure that Cargo.toml has the serde (with the derive feature) and serde_json crates:\n%copy%\nserde = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1\u0026#34; Sample code:\n%copy%\n// src/lib.rs use serde::Deserialize; use serde::Serialize; #[derive(Deserialize)] struct InputRecord { id: String, key: String, value: String, } #[derive(Serialize)] struct OutputRecord { key_id: String, value_id: String, source: String, } impl From\u0026lt;InputRecord\u0026gt; for OutputRecord { fn from(input: InputRecord) -\u0026gt; Self { let InputRecord { key, value, .. } = input; Self { key_id: key, value_id: value, source: \u0026#34;source_connector_12\u0026#34;.to_string(), } } } #[cfg(test)] mod tests { use super::*; use serde_json::json; #[test] fn it_works() { let input = json!({ \u0026#34;id\u0026#34;: \u0026#34;0000000001\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34; }); let input: InputRecord = serde_json::from_value(input).expect(\u0026#34;valid input\u0026#34;); let output = OutputRecord::from(input); assert_eq!( serde_json::to_value(\u0026amp;output).expect(\u0026#34;valid output\u0026#34;), json!({ \u0026#34;key_id\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value_id\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;source_connector_12\u0026#34; }) ) } } There can also be a solution with an untyped Value type, but it is less convenient for development, hence less popular, and is not that important for the sake of this scenario.\nWhat is the disadvantage of such a solution? As I said before, you are developing a new application that often implies changes in input and output formats. So, for every change you want to make to your transformation, you must re-compile your code. In real life, it is not just re-compile. The whole new process of CD and CI might need to be passed until the app gets the new flow. Therefore, as a developer of such an application, you might want to have the transformation flexible, meaning that you don\u0026rsquo;t need to change the code if the structure of the input JSON or output one changes in time.\nFluvio Jolt We, in Fluvio, decided to port to Rust a Java library Jolt that does exactly what we want here. Meet Fluvio Jolt: JSON to JSON transformation where the \u0026ldquo;specification\u0026rdquo; for the transform is itself a JSON document. Since the transformation is dynamic, flexible, and defined by another input, you don\u0026rsquo;t need to re-compile your application if the structure of your records changes. As we said, the \u0026ldquo;specification\u0026rdquo; for the transformation is a JSON document also. Let\u0026rsquo;s take a look at the specification needed for our case:\n%copy%\n[ { \u0026#34;operation\u0026#34;: \u0026#34;shift\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;key_id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value_id\u0026#34; } }, { \u0026#34;operation\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;source_connector_12\u0026#34; } } ] We can see there are two operations here. The first one is shift. We say which fields we want to move to and where. The second operation is default. We define a new value for the field if it is not present. The delete operation is also available, but these two operations cover all we need. Once we have the spec, we are ready to transform.\nLet\u0026rsquo;s set up another Rust project with Fluvio Jolt:\n%copy first-line%\n$ cargo new json-to-json-jolt --lib Add fluvio-jolt crate to Cargo.toml:\n%copy%\nfluvio-jolt = \u0026#34;0.1\u0026#34; serde_json = \u0026#34;1\u0026#34; The code will be similar to this (notice that we don\u0026rsquo;t need to define new types):\n%copy%\n// src/lib.rs #[cfg(test)] mod tests { use fluvio_jolt::TransformSpec; use serde_json::json; #[test] fn it_works() { let spec: TransformSpec = serde_json::from_str( r#\u0026#34; [ { \u0026#34;operation\u0026#34;: \u0026#34;shift\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;key_id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;value_id\u0026#34; } }, { \u0026#34;operation\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;source_connector_12\u0026#34; } } ] \u0026#34;#, ).expect(\u0026#34;parsed spec\u0026#34;); let input = json!({ \u0026#34;id\u0026#34;: \u0026#34;0000000001\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34; }); let output = fluvio_jolt::transform(input, \u0026amp;spec); assert_eq!( serde_json::to_value(\u0026amp;output).expect(\u0026#34;valid output\u0026#34;), json!({ \u0026#34;key_id\u0026#34;: \u0026#34;09B5C373-CB7F-4E45-9F38-50C1F8F69C5D\u0026#34;, \u0026#34;value_id\u0026#34;: \u0026#34;42883098-74A2-4353-A534-26F533665B67\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;source_connector_12\u0026#34; }) ) } } Codewise, we only need to parse a string containing all the transformation logic we want, and we are ready to process records.\nConclusion What does it give you? Now, you have an option to modify the specification in runtime or load it during the startup of your application. You can pass it along with your records or read it from another source. Usecases are limited only by your imagination.\nIf you find this library helpful, we encourage you to contribute! It is still in the early stage of development, and there are a lot of missing features that can and should be ported from Java\u0026rsquo;s Jolt.\nPlease, be sure to join our Discord server if you want to talk to us or have any questions.\nHave a happy coding, and stay tuned!\nFurther reading Handling JSON data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly The InfinyOn Continuous Intelligence Platform ","description":"How to use Fluvio Jolt to transform JSON data in a flexible way","keywords":null,"summary":"The JSON format remains one of the most popular text data formats for Data-in-Transition. You can encounter JSON data on every stack level of your application: from the database to UI, from IoT sensors data to the mobile app\u0026rsquo;s payload. And it is not a coincidence; the format has a good balance between being convenient for developers and decent payload density. In Rust ecosystem, the de-facto standard for dealing with JSON is Serde.","title":"Flexible JSON transformations in Rust","url":"http://localhost:1315/blog/2022/08/fluvio-jolt-intro/"},{"body":"//Sample form\nFirst Name: Last Name: Title: Company: Email: Phone: ","description":"This datasheet covers the availability, features, benefits, performance and more.","keywords":null,"summary":"//Sample form\nFirst Name: Last Name: Title: Company: Email: Phone: ","title":"Enterprise Datasheet for Fluvio OSS and InfinyOn Cloud","url":"http://localhost:1315/resources/enterprise-datasheet/"},{"body":"In a previous blog, we made an example on how we can work with XML data in Fluvio. Another very common data format in data streaming is JSON. In this blog, we will show an example on how to handle JSON data in SmartModules.\nThis blog is intended for Rust beginners.\nCheck out the full code in the fluvio-smartmodule-examples repository.\nPre-conditions In order to properly follow this blog, you need to have installed the Fluvio CLI and a have a Fluvio cluster up and running. You can accomplish both requirements using Infinyon Cloud following the next steps:\nDownload Fluvio CLI Sign-up for a free InfinyOn Cloud account. Login to InfinyOn Cloud via CLI: fluvio cloud login Scenario: Regional Weather in Hong Kong For this scenario, we are going to use one of the Hong Kong Observatory Open Data API to retrieve information about the latest 10-minute mean visibility on some of the places in Hong Kong.\nMore description about the endpoint that we will use, can be found in the latest 10-minute mean visibility section of the Hong Kong Observatory Open Data API documentation.\nFirst, let\u0026rsquo;s try the API with curl:\n%copy first-line%\n$ curl \u0026#34;https://data.weather.gov.hk/weatherAPI/opendata/opendata.php?dataType=LTMV\u0026amp;lang=en\u0026amp;rformat=json\u0026#34; {\u0026#34;fields\u0026#34;:[\u0026#34;Date time\u0026#34;,\u0026#34;Automatic Weather Station\u0026#34;,\u0026#34;10 minute mean visibility\u0026#34;],\u0026#34;data\u0026#34;:[[\u0026#34;202206082110\u0026#34;,\u0026#34;Central\u0026#34;,\u0026#34;13km\u0026#34;],[\u0026#34;202206082110\u0026#34;,\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;45km\u0026#34;],[\u0026#34;202206082110\u0026#34;,\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;17km\u0026#34;],[\u0026#34;202206082110\u0026#34;,\u0026#34;Waglan Island\u0026#34;,\u0026#34;N\\/A\u0026#34;]]} Note that the API is intended to be used with CSV format, but since we want to handle JSON data, we will use the JSON format parameter.\nWe want to use a SmartModule to transform this response into a JSON where we have two keys. datetime with the Date time field of the response, and stationsMeanVisibility with the mean visibility of each station. For example, for the previous response, we will get something like:\n{\u0026#34;datetime\u0026#34;:\u0026#34;202206082110\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;13km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;17km\u0026#34;}]} Using Connector as source We want to feed our topic automatically in Fluvio with the information of that call. Fortunately, in Fluvio we can use connectors as a source to import data from third party services into Fluvio topics. For this case, we can use the HTTP connector.\nIn order to create the connector we need a config file. For this example, we created a file called hk_mean_visibility.yml with this content:\n%copy%\n# hk_mean_visibility.yml --- apiVersion: v1 version: 0.2.1 name: hk-mean-visibility-connector type: http topic: hk-mean-visibility create_topic: true direction: source parameters: endpoint: https://data.weather.gov.hk/weatherAPI/opendata/opendata.php?dataType=LTMV\u0026amp;lang=en\u0026amp;rformat=json method: GET interval: 600 This configuration will create a http connector called hk-mean-visibility-connector that produces to topic hk-mean-visibility the response body from calling the 10-minute mean visibility endpoint from Hong Kong Observatory Open Data API each 600 seconds.\nWith that file, we can create a connector with the command:\n%copy first-line%\n$ fluvio connector create -c hk_mean_visibility.yml Once that is created, the connector will start producing records to the hk-mean-visibility topic:\n%copy first-line%\n$ fluvio consume hk-mean-visibility -B Consuming records from the beginning of topic \u0026#39;hk-mean-visibility\u0026#39; {\u0026#34;fields\u0026#34;:[\u0026#34;Date time\u0026#34;,\u0026#34;Automatic Weather Station\u0026#34;,\u0026#34;10 minute mean visibility\u0026#34;],\u0026#34;data\u0026#34;:[[\u0026#34;202206082130\u0026#34;,\u0026#34;Central\u0026#34;,\u0026#34;12km\u0026#34;],[\u0026#34;202206082130\u0026#34;,\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;45km\u0026#34;],[\u0026#34;202206082130\u0026#34;,\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;13km\u0026#34;],[\u0026#34;202206082130\u0026#34;,\u0026#34;Waglan Island\u0026#34;,\u0026#34;N\\/A\u0026#34;]]} Create a new project for SmartModule Since, we want to convert one record into a records with a different format, we should use a map SmartModule. In order to get started, we can use the cargo-generate tool to create a map template project. If you don\u0026rsquo;t already have it installed, you can get it with this command:\n%copy first-line%\n$ cargo install cargo-generate After you have cargo-generate installed, you can create a smartmodule project using map and no parameters template using the following command:\n%copy first-line%\n$ cargo generate --git=https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : smartmodule-json 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · map ✔ 🤷 Want to use SmartModule parameters? · false [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `smartmodule-json`... ✨ Done! New project created smartmodule-json Note that, we selected map as the SmartModule type and that we are not using SmartModule parameters.\nNow, it is time to edit our generated SmartModule template project to behave the way we want. First, we need to make sure that the dependencies to handle JSON in Rust are installed. In Rust, the defacto standard to handle JSON data is with the serde and serde_json crates. By default, our cargo generate template projects have both crates installed.\nMake sure, that Cargo.toml have the serde (with the derive feature) and serde_json crates.\n%copy%\nserde = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1\u0026#34; Once we are sure that we can use serde and serde_json crates. It is time to create our Rust structures to represent the data from our records. Let\u0026rsquo;s start with input data:\n{ \u0026#34;fields\u0026#34;: [ \u0026#34;Date time\u0026#34;, \u0026#34;Automatic Weather Station\u0026#34;, \u0026#34;10 minute mean visibility\u0026#34; ], \u0026#34;data\u0026#34;: [ [ \u0026#34;202206082130\u0026#34;, \u0026#34;Central\u0026#34;, \u0026#34;12km\u0026#34; ], [ \u0026#34;202206082130\u0026#34;, \u0026#34;Chek Lap Kok\u0026#34;, \u0026#34;45km\u0026#34; ], [ \u0026#34;202206082130\u0026#34;, \u0026#34;Sai Wan Ho\u0026#34;, \u0026#34;13km\u0026#34; ], [ \u0026#34;202206082130\u0026#34;, \u0026#34;Waglan Island\u0026#34;, \u0026#34;N/A\u0026#34; ] ] } We have two fields in the json: fields and data. fields is a list of strings and data is a list of a list of strings. In Rust, that can be translated into:\n%copy%\n// src/lib.rs struct InputHKMeanVisibility { fields: Vec\u0026lt;String\u0026gt;, data: Vec\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt; } Now let\u0026rsquo;s do our Rust struct for the output:\n{ \u0026#34;datetime\u0026#34;: \u0026#34;202206082110\u0026#34;, \u0026#34;stationsMeanVisibility\u0026#34;: [ { \u0026#34;stationName\u0026#34;: \u0026#34;Central\u0026#34;, \u0026#34;meanVisibility\u0026#34;: \u0026#34;13km\u0026#34; }, { \u0026#34;stationName\u0026#34;: \u0026#34;Chek Lap Kok\u0026#34;, \u0026#34;meanVisibility\u0026#34;: \u0026#34;45km\u0026#34; }, { \u0026#34;stationName\u0026#34;: \u0026#34;Sai Wan Ho\u0026#34;, \u0026#34;meanVisibility\u0026#34;: \u0026#34;17km\u0026#34; } ] } In our planned JSON output, we have two fields, datetime that is a string and stationsMeanVisibility that is a list of objects that have a stationName (string) and meanVisibility (string). That can be translated in Rust to:\n%copy%\n// src/lib.rs struct OutputHKMeanVisibility { datetime: String, stations_mean_visibility: Vec\u0026lt;StationMeanVisibility\u0026gt;, } struct StationMeanVisibility { station_name: String, mean_visibility: String, } Note that the idiomatic way to define struct fields in Rust is snake_case. We will add the changes later to create JSON with camelCase fields.\nIt is time to use serde. This is as easy as importing the Serialize and Deserialize derive macros and adding them to the structs that we just created.\n%copy%\n// src/lib.rs use serde::{Serialize, Deserialize}; #[derive(Deserialize)] struct InputHKMeanVisibility { fields: Vec\u0026lt;String\u0026gt;, data: Vec\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt; } #[derive(Serialize)] #[serde(rename_all = \u0026#34;camelCase\u0026#34;)] struct OutputHKMeanVisibility { datetime: String, stations_mean_visibility: Vec\u0026lt;StationMeanVisibility\u0026gt;, } #[derive(Serialize)] #[serde(rename_all = \u0026#34;camelCase\u0026#34;)] struct StationMeanVisibility { station_name: String, mean_visibility: String, } Note that we only need Deserialize for the input struct and Serialize for the output struct. Also note that we are using #[serde(rename_all = \u0026quot;camelCase\u0026quot;)] to under the hood serialize the fields the way we want.\nNow, let\u0026rsquo;s create a method to transform from InputHKMeanVisibility type to OutputHKMeanVisibility type. In Rust, this kind of operation is typically done using the From trait.\nLet\u0026rsquo;s implement the From trait for our structures.\n%copy%\n// src/lib.rs impl From\u0026lt;InputHKMeanVisibility\u0026gt; for OutputHKMeanVisibility { fn from(input: InputHKMeanVisibility) -\u0026gt; Self { let datetime = input.data[0][0].to_owned(); let stations_mean_visibility = input .data .into_iter() .map(|data| StationMeanVisibility { station_name: data[1].to_owned(), mean_visibility: data[2].to_owned(), }) .collect(); Self { datetime, stations_mean_visibility, } } } That\u0026rsquo;s all the implementation for the From trait, we take datetime from the first element of the first element of data field. And we construct the stations_mean_visibility from the second and third element of each one of the elements of the data field.\nNow, it is time to write our code logic for our SmartModule.\n%copy%\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; #[smartmodule(map)] pub fn mean_visibility_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { // Deserialize input from JSON record using serde_json let input = serde_json::from_slice::\u0026lt;InputHKMeanVisibility\u0026gt;(record.value.as_ref())?; // transform input into output struct using From trait let output = OutputHKMeanVisibility::from(input); // Serialize output into JSON using serde_json let serialized_output = serde_json::to_vec(\u0026amp;output)?; Ok((None, RecordData::from(serialized_output))) } Let\u0026rsquo;s describe our code. First, we need to be aware that our map SmartModule needs a function with the macro #[smartmodule(map)] and with the right parameters and output types. In that function, we write our SmartModule logic. In this case, our function is using the serde_json crate to deserialize the input from the record into our input struct, then we transform our input struct to our output struct using the From trait that we just implemented. Then, we serialize our output struct into JSON again using serde_json and finally, we return that result.\nOur code now, is ready. It should look like this:\n%copy%\n//src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; use serde::{Deserialize, Serialize}; #[smartmodule(map)] pub fn mean_visibility_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { // Deserialize input from JSON record using serde_json let input = serde_json::from_slice::\u0026lt;InputHKMeanVisibility\u0026gt;(record.value.as_ref())?; // transform input into output struct using From trait let output = OutputHKMeanVisibility::from(input); // Serialize output into JSON using serde_json let serialized_output = serde_json::to_vec(\u0026amp;output)?; Ok((None, RecordData::from(serialized_output))) } #[derive(Deserialize)] struct InputHKMeanVisibility { #[allow(dead_code)] fields: Vec\u0026lt;String\u0026gt;, data: Vec\u0026lt;Vec\u0026lt;String\u0026gt;\u0026gt;, } #[derive(Serialize)] #[serde(rename_all = \u0026#34;camelCase\u0026#34;)] struct OutputHKMeanVisibility { datetime: String, stations_mean_visibility: Vec\u0026lt;StationMeanVisibility\u0026gt;, } #[derive(Serialize)] #[serde(rename_all = \u0026#34;camelCase\u0026#34;)] struct StationMeanVisibility { station_name: String, mean_visibility: String, } impl From\u0026lt;InputHKMeanVisibility\u0026gt; for OutputHKMeanVisibility { fn from(input: InputHKMeanVisibility) -\u0026gt; Self { let datetime = input.data[0][0].to_owned(); let stations_mean_visibility = input .data .into_iter() .map(|data| StationMeanVisibility { station_name: data[1].to_owned(), mean_visibility: data[2].to_owned(), }) .collect(); Self { datetime, stations_mean_visibility, } } } Once we have our code ready, we can build the smartmodule. First, make sure that you have the wasm32-unknown-unknown target installed and then compile with:\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown $ cargo build --release Then, we can upload that SmartModule to fluvio with the name smartmodule-json.\n%copy first-line%\n$ fluvio sm create smartmodule-json --wasm-file target/wasm32-unknown-unknown/release/smartmodule_json.wasm Finally, we use the smartmodule-json SmartModule uploaded to consume the hk-mean-visibility topic:\n%copy first-line%\n$ fluvio consume hk-mean-visibility -B --map smartmodule-json Consuming records from the beginning of topic \u0026#39;hk-mean-visibility\u0026#39; {\u0026#34;datetime\u0026#34;:\u0026#34;202206082130\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;12km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;13km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082140\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;11km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;15km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082150\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;12km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;16km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082200\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;11km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;15km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082210\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;13km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;17km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082220\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;17km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;16km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082230\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;20km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;20km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} {\u0026#34;datetime\u0026#34;:\u0026#34;202206082240\u0026#34;,\u0026#34;stationsMeanVisibility\u0026#34;:[{\u0026#34;stationName\u0026#34;:\u0026#34;Central\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;16km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Chek Lap Kok\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;45km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Sai Wan Ho\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;25km\u0026#34;},{\u0026#34;stationName\u0026#34;:\u0026#34;Waglan Island\u0026#34;,\u0026#34;meanVisibility\u0026#34;:\u0026#34;N/A\u0026#34;}]} That\u0026rsquo;s all! Now you can see that our smartmodule is transforming our input JSON data into a JSON data with a different schema.\nConclusion That\u0026rsquo;s it for this post. As with the XML example, you again can see that Fluvio can store any kind of binary data. And it is just responsability of the SmartModule Developer to be able to decode/deserialize that data successfully in order to apply custom logic on top of that.\nPlease, be sure to join our Discord server if you want to talk to us or have any questions. Until next time!\nFurther reading Handling XML data in Fluvio SmartModules Transform streaming data in real-time with WebAssembly The InfinyOn Continuous Intelligence Platform ","description":"How to use SmartModules to process JSON data in Fluvio","keywords":null,"summary":"In a previous blog, we made an example on how we can work with XML data in Fluvio. Another very common data format in data streaming is JSON. In this blog, we will show an example on how to handle JSON data in SmartModules.\nThis blog is intended for Rust beginners.\nCheck out the full code in the fluvio-smartmodule-examples repository.\nPre-conditions In order to properly follow this blog, you need to have installed the Fluvio CLI and a have a Fluvio cluster up and running.","title":"Handling JSON data in Fluvio SmartModules","url":"http://localhost:1315/blog/2022/06/smartmodule-json/"},{"body":"In Fluvio, records are just raw bytes, therefore we can create them with any kind of data. In SmartModules, we only need to know how to handle those raw bytes and how to convert them in something that we can process. In this blog, we will create a SmartModule that handles XML data.\nCheck out the full code in the fluvio-smartmodule-examples repository.\nPre-conditions In order to properly follow this blog, you need to have installed the Fluvio CLI and a have a Fluvio cluster up and running. You can accomplish both requirements using Infinyon Cloud following the next steps:\nDownload Fluvio CLI Sign-up for a free InfinyOn Cloud account. Login to InfinyOn Cloud via CLI: fluvio cloud login Scenario: Bike Occupancy in London For this scenario, we are going to use one of the public APIs from Transport for London to retrieve information from the Bike Occupancy in some of the Bike points that they have.\nIn particular, we are going to use the get bike points occupancy API. As an example, we can retrieve bike point occupancy for bike points BikePoints_1 and BikePoints2 in XML format using curl (beutified):\n%copy first-line%\n$ curl https://api.tfl.gov.uk/Occupancy/BikePoints/BikePoints_1,BikePoints_2 -H Accept:text/xml \u0026lt;ArrayOfBikePointOccupancy xmlns:i=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://schemas.datacontract.org/2004/07/Tfl.Api.Presentation.Entities\u0026#34;\u0026gt; \u0026lt;BikePointOccupancy\u0026gt; \u0026lt;BikesCount\u0026gt;0\u0026lt;/BikesCount\u0026gt; \u0026lt;EBikesCount\u0026gt;0\u0026lt;/EBikesCount\u0026gt; \u0026lt;EmptyDocks\u0026gt;19\u0026lt;/EmptyDocks\u0026gt; \u0026lt;Id\u0026gt;BikePoints_1\u0026lt;/Id\u0026gt; \u0026lt;Name\u0026gt;River Street , Clerkenwell\u0026lt;/Name\u0026gt; \u0026lt;StandardBikesCount\u0026gt;11\u0026lt;/StandardBikesCount\u0026gt; \u0026lt;TotalDocks\u0026gt;19\u0026lt;/TotalDocks\u0026gt; \u0026lt;/BikePointOccupancy\u0026gt; \u0026lt;BikePointOccupancy\u0026gt; \u0026lt;BikesCount\u0026gt;31\u0026lt;/BikesCount\u0026gt; \u0026lt;EBikesCount\u0026gt;0\u0026lt;/EBikesCount\u0026gt; \u0026lt;EmptyDocks\u0026gt;6\u0026lt;/EmptyDocks\u0026gt; \u0026lt;Id\u0026gt;BikePoints_2\u0026lt;/Id\u0026gt; \u0026lt;Name\u0026gt;Phillimore Gardens, Kensington\u0026lt;/Name\u0026gt; \u0026lt;StandardBikesCount\u0026gt;11\u0026lt;/StandardBikesCount\u0026gt; \u0026lt;TotalDocks\u0026gt;37\u0026lt;/TotalDocks\u0026gt; \u0026lt;/BikePointOccupancy\u0026gt; \u0026lt;/ArrayOfBikePointOccupancy\u0026gt; We want to use a SmartModule to transform this response into one record per BikePoint and in JSON format. So, for that response, we want to get two records with the following format (beutified):\n{ \u0026#34;BikesCount\u0026#34;: 0, \u0026#34;EBikesCount\u0026#34;: 0, \u0026#34;EmptyDocks\u0026#34;: 19, \u0026#34;Id\u0026#34;: \u0026#34;BikePoints_1\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;River Street , Clerkenwell\u0026#34;, \u0026#34;StandardBikesCount\u0026#34;: 0, \u0026#34;TotalDocks\u0026#34;: 19 } { \u0026#34;BikesCount\u0026#34;: 31, \u0026#34;EBikesCount\u0026#34;: 0, \u0026#34;EmptyDocks\u0026#34;: 6, \u0026#34;Id\u0026#34;: \u0026#34;BikePoints_2\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Phillimore Gardens, Kensington\u0026#34;, \u0026#34;StandardBikesCount\u0026#34;: 0, \u0026#34;TotalDocks\u0026#34;: 37 } Using Connector as source We want to feed our topic automatically in Fluvio with the information of that call. Fortunately, in Fluvio we can use connectors as a source to import data from third party services into Fluvio topics. For this case, we can use the HTTP connector.\nIn order to create the connector we need a config file. For this example, we created a file called bikepoints.yml with this content:\n%copy%\n# bikepoints.yml --- apiVersion: v1 version: 0.2.1 name: bikepoints-connector type: http topic: bikepoints-occupancy-xml create_topic: true direction: source parameters: endpoint: https://api.tfl.gov.uk/Occupancy/BikePoints/BikePoints_1,BikePoints_2,BikePoints_3 header: Accept:text/xml method: GET interval: 300 This configuration will create a http connector called bikepoints-connector that produces to topic bikepoints-occupancy-xml the response body from calling the get bike points occupancy endpoint each 300 seconds. Note that for this example we are retrieving information from three bike points (BikePoints_1, BikePoints_2, BikePoints_3) and that we are using the header Accept: text/xml in order to receive a response with XML format.\nWith that file, we can create a connector with the command:\n%copy first-line%\n$ fluvio connector create -c bikepoints.yml Once that is created, the connector will start producing records to the bikepoints-occupancy-xml topic:\n%copy first-line%\n$ fluvio consume bikepoints-occupancy-xml -B Consuming records from the beginning of topic \u0026#39;bikepoints-occupancy-xml\u0026#39; \u0026lt;ArrayOfBikePointOccupancy xmlns:i=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://schemas.datacontract.org/2004/07/Tfl.Api.Presentation.Entities\u0026#34;\u0026gt;\u0026lt;BikePointOccupancy\u0026gt;\u0026lt;BikesCount\u0026gt;0\u0026lt;/BikesCount\u0026gt;\u0026lt;EBikesCount\u0026gt;0\u0026lt;/EBikesCount\u0026gt;\u0026lt;EmptyDocks\u0026gt;19\u0026lt;/EmptyDocks\u0026gt;\u0026lt;Id\u0026gt;BikePoints_1\u0026lt;/Id\u0026gt;\u0026lt;Name\u0026gt;River Street , Clerkenwell\u0026lt;/Name\u0026gt;\u0026lt;StandardBikesCount\u0026gt;0\u0026lt;/StandardBikesCount\u0026gt;\u0026lt;TotalDocks\u0026gt;19\u0026lt;/TotalDocks\u0026gt;\u0026lt;/BikePointOccupancy\u0026gt;\u0026lt;BikePointOccupancy\u0026gt;\u0026lt;BikesCount\u0026gt;28\u0026lt;/BikesCount\u0026gt;\u0026lt;EBikesCount\u0026gt;0\u0026lt;/EBikesCount\u0026gt;\u0026lt;EmptyDocks\u0026gt;9\u0026lt;/EmptyDocks\u0026gt;\u0026lt;Id\u0026gt;BikePoints_2\u0026lt;/Id\u0026gt;\u0026lt;Name\u0026gt;Phillimore Gardens, Kensington\u0026lt;/Name\u0026gt;\u0026lt;StandardBikesCount\u0026gt;0\u0026lt;/StandardBikesCount\u0026gt;\u0026lt;TotalDocks\u0026gt;37\u0026lt;/TotalDocks\u0026gt;\u0026lt;/BikePointOccupancy\u0026gt;\u0026lt;BikePointOccupancy\u0026gt;\u0026lt;BikesCount\u0026gt;19\u0026lt;/BikesCount\u0026gt;\u0026lt;EBikesCount\u0026gt;0\u0026lt;/EBikesCount\u0026gt;\u0026lt;EmptyDocks\u0026gt;13\u0026lt;/EmptyDocks\u0026gt;\u0026lt;Id\u0026gt;BikePoints_3\u0026lt;/Id\u0026gt;\u0026lt;Name\u0026gt;Christopher Street, Liverpool Street\u0026lt;/Name\u0026gt;\u0026lt;StandardBikesCount\u0026gt;0\u0026lt;/StandardBikesCount\u0026gt;\u0026lt;TotalDocks\u0026gt;32\u0026lt;/TotalDocks\u0026gt;\u0026lt;/BikePointOccupancy\u0026gt;\u0026lt;/ArrayOfBikePointOccupancy\u0026gt; Create a new project for SmartModule Since, we want to convert one record into multiple records, we should use an array-map SmartModule. In order to get started, we can use the cargo-generate tool to create an array-map template project. If you don\u0026rsquo;t already have it installed, you can get it with this command:\n%copy first-line%\n$ cargo install cargo-generate After you have cargo-generate installed, you can create a smartmodule project using array-map and no parameters template using the following command:\n%copy first-line%\n$ cargo generate --git=https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : smartmodule-xml 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · array-map ✔ 🤷 Want to use SmartModule parameters? · false [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `smartmodule-xml`... ✨ Done! New project created smartmodule-xml Note that, we selected array-map as the SmartModule type and that we are not using SmartModule parameters.\nLet\u0026rsquo;s navigate into our project directory and take a look at the sample code we were given:\n%copy first-line%\n$ cd smartmodule-xml \u0026amp;\u0026amp; cat src/lib.rs We should see the following code:\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; #[smartmodule(array_map, params)] pub fn array_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { // Deserialize a JSON array with any kind of values inside let array = serde_json::from_slice::\u0026lt;Vec\u0026lt;serde_json::Value\u0026gt;\u0026gt;(record.value.as_ref())?; // Convert each JSON value from the array back into a JSON string let strings: Vec\u0026lt;String\u0026gt; = array .into_iter() .map(|value| serde_json::to_string(\u0026amp;value)) .collect::\u0026lt;core::result::Result\u0026lt;_, _\u0026gt;\u0026gt;()?; // Create one record from each JSON string to send let kvs: Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; = strings .into_iter() .map(|s| (None, RecordData::from(s))) .collect(); Ok(kvs) } Now, we want to edit this smartmodule to behave the way we want. First, we need a library to deserialize the record value that is stored in XML format. For that, we can use the quick-xml crate. In order to use that library the way we need, we have to add that crate to our Cargo.toml file with the serialize feature enabled.\nPaste the following code into Cargo.toml:\n%copy%\nquick-xml = { version = \u0026#34;0.23.0\u0026#34;, features = [\u0026#34;serialize\u0026#34;] } Then, we need to define the structs that will store the information from the records. In particular, for the shape of the data that the Bike occupancy API has, we can copy this into src/lib.rs:\n%copy%\n// src/lib.rs use serde::{Serialize, Deserialize}; #[derive(Serialize, Deserialize)] #[serde(rename_all = \u0026#34;PascalCase\u0026#34;)] pub struct ArrayOfBikePointOccupancy { bike_point_occupancy: Vec\u0026lt;BikePointOccupancy\u0026gt;, } #[derive(Serialize, Deserialize)] #[serde(rename_all = \u0026#34;PascalCase\u0026#34;)] pub struct BikePointOccupancy { bikes_count: usize, e_bikes_count: usize, empty_docks: usize, id: String, name: String, standard_bikes_count: usize, total_docks: usize, } Once the structs are defined, we just need to deserialize the records values into those structs and then for each Bike Point information, we serialize them as JSON and create separated records. We can copy this into out src/lib.rs file to do that:\n%copy%\n// src/lib.rs #[smartmodule(array_map)] pub fn array_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { // Deserialize XML from record using quick_xml crate let array = quick_xml:: de ::from_slice::\u0026lt;ArrayOfBikePointOccupancy\u0026gt;(record.value.as_ref())?; // Create a Json string for each bike point occupancy let strings: Vec\u0026lt;String\u0026gt; = array .bike_point_occupancy .into_iter() .map(|value| serde_json::to_string(\u0026amp;value)) .collect::\u0026lt;core::result::Result\u0026lt;_, _\u0026gt;\u0026gt;()?; // Create one record from each JSON string to send let kvs: Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; = strings .into_iter() .map(|s| (None, RecordData::from(s))) .collect(); Ok(kvs) } Now, our src/lib.rs should look like this:\n%copy%\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; use serde::{Deserialize, Serialize}; #[smartmodule(array_map)] pub fn array_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { // Deserialize XML from record let array = quick_xml:: de ::from_slice::\u0026lt;ArrayOfBikePointOccupancy\u0026gt;(record.value.as_ref())?; // Create a Json string for each bike point occupancy let strings: Vec\u0026lt;String\u0026gt; = array .bike_point_occupancy .into_iter() .map(|value| serde_json::to_string(\u0026amp;value)) .collect::\u0026lt;core::result::Result\u0026lt;_, _\u0026gt;\u0026gt;()?; // Create one record from each JSON string to send let kvs: Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; = strings .into_iter() .map(|s| (None, RecordData::from(s))) .collect(); Ok(kvs) } #[derive(Serialize, Deserialize)] #[serde(rename_all = \u0026#34;PascalCase\u0026#34;)] pub struct ArrayOfBikePointOccupancy { bike_point_occupancy: Vec\u0026lt;BikePointOccupancy\u0026gt;, } #[derive(Serialize, Deserialize)] #[serde(rename_all = \u0026#34;PascalCase\u0026#34;)] pub struct BikePointOccupancy { bikes_count: usize, e_bikes_count: usize, empty_docks: usize, id: String, name: String, standard_bikes_count: usize, total_docks: usize, } Once we have our code ready, we can build the smartmodule. First, make sure that you have the wasm32-unknown-unknown target installed and then compile with:\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown $ cargo build --release Then, we can upload that SmartModule to fluvio with the name smartmodule-xml.\n%copy first-line%\n$ fluvio sm create smartmodule-xml --wasm-file target/wasm32-unknown-unknown/release/smartmodule_xml.wasm Finally, we use the smartmodule-xml SmartModule uploaded to consume the bikepoints-occupancy-xml topic:\n%copy first-line%\n$ fluvio consume bikepoints-occupancy-xml -B --array-map smartmodule-xml Consuming records from the beginning of topic \u0026#39;bikepoints-occupancy-xml\u0026#39; {\u0026#34;BikesCount\u0026#34;:0,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:19,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_1\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;River Street , Clerkenwell\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:19} {\u0026#34;BikesCount\u0026#34;:28,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:9,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_2\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;Phillimore Gardens, Kensington\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:37} {\u0026#34;BikesCount\u0026#34;:19,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:13,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_3\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;Christopher Street, Liverpool Street\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:32} {\u0026#34;BikesCount\u0026#34;:0,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:19,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_1\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;River Street , Clerkenwell\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:19} {\u0026#34;BikesCount\u0026#34;:27,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:10,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_2\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;Phillimore Gardens, Kensington\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:37} {\u0026#34;BikesCount\u0026#34;:20,\u0026#34;EBikesCount\u0026#34;:0,\u0026#34;EmptyDocks\u0026#34;:12,\u0026#34;Id\u0026#34;:\u0026#34;BikePoints_3\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;Christopher Street, Liverpool Street\u0026#34;,\u0026#34;StandardBikesCount\u0026#34;:0,\u0026#34;TotalDocks\u0026#34;:32} That\u0026rsquo;s all! Now you can see that our smartmodule is transforming our XML data into multiple records with JSON format.\nConclusion That\u0026rsquo;s it for this post. You can see that Fluvio can store any kind of binary data. And it is just responsability of the SmartModule Developer to be able to decode/deserialize that data successfully in order to apply custom logic on top of that.\nPlease, be sure to join our Discord server if you want to talk to us or have any questions. Until next time!\nFurther reading Streaming the Reddit API using Fluvio\u0026rsquo;s WASM ArrayMap The InfinyOn Continuous Intelligence Platform Future Trends in Real-Time Data ","description":"How to use SmartModules to process XML data in Fluvio","keywords":null,"summary":"In Fluvio, records are just raw bytes, therefore we can create them with any kind of data. In SmartModules, we only need to know how to handle those raw bytes and how to convert them in something that we can process. In this blog, we will create a SmartModule that handles XML data.\nCheck out the full code in the fluvio-smartmodule-examples repository.\nPre-conditions In order to properly follow this blog, you need to have installed the Fluvio CLI and a have a Fluvio cluster up and running.","title":"Handling XML data in Fluvio SmartModules","url":"http://localhost:1315/blog/2022/06/smartmodule-xml/"},{"body":"","description":"How to connect, transform, and stream data that powers real-time applications and experiences.","keywords":null,"summary":"","title":"How to power event-driven mobile applications with InfinyOn Cloud","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/1230486975204092688/"},{"body":"Are you looking to build a more efficient and cost-effective data stack for your organization? Don\u0026rsquo;t miss out on our speaking session at KubeCon featuring the CTO of InfinyOn. Discover how Fluvio, the open-source project, leverages WASM technology to create a distributed stream platform that removes the barrier between data and compute, resulting in a new type of data stack that can be deployed anywhere with hyper-efficiency.\nDuring this session, you will learn about WASM-based SmartModules and its benefits in solving various use cases. This is a must-watch for anyone interested in modern data stack technology and its potential to revolutionize data-driven organizations.\n","description":"Building a WASM powered distributed streaming platform.","keywords":null,"summary":"Are you looking to build a more efficient and cost-effective data stack for your organization? Don\u0026rsquo;t miss out on our speaking session at KubeCon featuring the CTO of InfinyOn. Discover how Fluvio, the open-source project, leverages WASM technology to create a distributed stream platform that removes the barrier between data and compute, resulting in a new type of data stack that can be deployed anywhere with hyper-efficiency.\nDuring this session, you will learn about WASM-based SmartModules and its benefits in solving various use cases.","title":"KubeCon Cloud Native WASM Day Tech Talk","url":"http://localhost:1315/resources/kubecon-cloud-native-wasm-day/"},{"body":"Are you looking to build a more efficient and cost-effective data stack for your organization? Don\u0026rsquo;t miss out on our speaking session at KubeCon featuring the CTO of InfinyOn. Discover how Fluvio, the open-source project, leverages WASM technology to create a distributed stream platform that removes the barrier between data and compute, resulting in a new type of data stack that can be deployed anywhere with hyper-efficiency.\nDuring this session, you will learn about WASM-based SmartModules and its benefits in solving various use cases. This is a must-watch for anyone interested in modern data stack technology and its potential to revolutionize data-driven organizations.\n","description":"Building a WASM powered distributed streaming platform.","keywords":null,"summary":"Are you looking to build a more efficient and cost-effective data stack for your organization? Don\u0026rsquo;t miss out on our speaking session at KubeCon featuring the CTO of InfinyOn. Discover how Fluvio, the open-source project, leverages WASM technology to create a distributed stream platform that removes the barrier between data and compute, resulting in a new type of data stack that can be deployed anywhere with hyper-efficiency.\nDuring this session, you will learn about WASM-based SmartModules and its benefits in solving various use cases.","title":"KubeCon Cloud Native WASM Day Tech Talk","url":"http://localhost:1315/videos/kubecon-cloud-native-wasm-day/"},{"body":"At InfinyOn, we believe enabling a real-time data infrastructure is becoming a growing priority in the enterprise. This is evident when looking at company performance and indicators from Confluent (ticker symbol: CFLT). Their most recent quarterly results had revenue growth of 71% including 211% growth in cloud and were supported by 65% growth in customers and 130%+ dollar based net retention. This highlights the multiple growth engines that are working in CFLT\u0026rsquo;s favor and the need for real-time applications.\nConfluent Cloud is a fully managed Apache Kafka service. Apache Kafka is the world\u0026rsquo;s most popular open source event stream processing software with more than 80% of Fortune 100 companies using Kafka. The success has been phenomenal, however, Kafka is an aging technology that is over 10 years old. It was built using the Java programming language that has its limits compared to newer programming languages like Rust or Go. See our Java vs. Rust comparison or Rust in 100 seconds for more information on the advantages and disadvantages.\nFluvio and InfinyOn Cloud InfinyOn is focused on building its Fluvio open source community and educating the market on some of these new capabilities such as the ability to do real-time event stream processing and data transformation in a single unified cluster. Similar to Confluent Cloud, InfinyOn Cloud is a fully managed Fluvio service but it is differentiated in how it helps to simplify data architectures and eliminates the need for ETL tools. Current architectures often look like this diagram where legacy streaming technologies are essentially what we call a simple data pipeline that only functions as a messaging bus. Extract Transform and Load or (ETL) tools are needed for transformation and require batch processing.\nInfrastructure teams are stiching together and managing multiple tools to perform event stream processing and data transformation.\nSmart Data Pipelines Smart data pipelines that leverage Wasm technology have an enrichment layer where transformation happens on source connectors, sink connectors, producers, consumers, or within the stream processing unit (SPU).\nThe advantages are numerous for teams with data governance and data quality initiatives. Data engineering teams can have distributed policy with centralized control and enjoy the cost savings of running their real-time data infrastructure on Fluvio or InfinyOn Cloud, the most memory efficient technology available today. Prospects and customers are finding the cost savings attractive compared to the memory intensive, Java-based Apache Kafka.\nAs the journey for enterprises continues the build out and design of data infrastructure that connects applications, systems and data layers to what Gartner coins a ENS or Enterprise Nervous System, organizations are doing more stream processing because of the need for continuous intelligence, faster decision making, and creating exceptional customer experiences. Companies are accumulating more streaming data every year from internal sources like corporate websites, IoT sensors, or transactional applications and from external sources such as business partners, data providers and social media platforms.\nConclusion It’s clear that real-time is gaining momentum in the enterprise. Our mission is to accelerate the world\u0026rsquo;s transition to the real-time economy. Real-time data can give businesses valuable insight into product performance, customer behavior, supply chain management, business planning and more. Learn more about potential InfinyOn Cloud use-cases including Change Data Capture (CDC), predictive maintenance, supply chain automation, real-time payments and secure transactions.\n","description":"Real-time data infrastructure is becoming a growing priority in the enterprise.","keywords":null,"summary":"At InfinyOn, we believe enabling a real-time data infrastructure is becoming a growing priority in the enterprise. This is evident when looking at company performance and indicators from Confluent (ticker symbol: CFLT). Their most recent quarterly results had revenue growth of 71% including 211% growth in cloud and were supported by 65% growth in customers and 130%+ dollar based net retention. This highlights the multiple growth engines that are working in CFLT\u0026rsquo;s favor and the need for real-time applications.","title":"Real-time Gaining Momentum in the Enterprise","url":"http://localhost:1315/blog/2022/02/real-time-gaining-momentum/"},{"body":"","description":"Learn how to leverage programmable stream processing to clean and transform data in real-time.","keywords":null,"summary":"","title":"Real-time Event Streaming and Data Transformation for Financial Services","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/4870730280061351695/"},{"body":"InfinyOn Cloud is built on Rust which is blazingly fast and memory-efficent. It has the simplicity of high-level languages (Go, Python), but the control of low-level languages (C, C++). With no runtime or garbage collector, it can power performance-critical services, run on embedded devices, and easily integrate with other languages. Rust has a rich type system and ownership model guarantee memory-safety and thread-safety — enabling you to eliminate many classes of bugs at compile-time.\nWatch this short video to learn more -\nPublished by Fireship.io YouTube channel","description":"Rust is a memory-safe compiled programming language for building high-performance systems.","keywords":null,"summary":"InfinyOn Cloud is built on Rust which is blazingly fast and memory-efficent. It has the simplicity of high-level languages (Go, Python), but the control of low-level languages (C, C++). With no runtime or garbage collector, it can power performance-critical services, run on embedded devices, and easily integrate with other languages. Rust has a rich type system and ownership model guarantee memory-safety and thread-safety — enabling you to eliminate many classes of bugs at compile-time.","title":"Rust in 100 Seconds","url":"http://localhost:1315/resources/rust-in-100-seconds/"},{"body":"InfinyOn Cloud is built on Rust which is blazingly fast and memory-efficent. It has the simplicity of high-level languages (Go, Python), but the control of low-level languages (C, C++). With no runtime or garbage collector, it can power performance-critical services, run on embedded devices, and easily integrate with other languages. Rust has a rich type system and ownership model guarantee memory-safety and thread-safety — enabling you to eliminate many classes of bugs at compile-time.\nWatch this short video to learn more -\nPublished by Fireship.io YouTube channel","description":"Rust is a memory-safe compiled programming language for building high-performance systems.","keywords":null,"summary":"InfinyOn Cloud is built on Rust which is blazingly fast and memory-efficent. It has the simplicity of high-level languages (Go, Python), but the control of low-level languages (C, C++). With no runtime or garbage collector, it can power performance-critical services, run on embedded devices, and easily integrate with other languages. Rust has a rich type system and ownership model guarantee memory-safety and thread-safety — enabling you to eliminate many classes of bugs at compile-time.","title":"Rust in 100 Seconds","url":"http://localhost:1315/videos/rust-in-100-seconds/"},{"body":" Prev Next Page: / Download ","description":"Simplify data architectures and eliminate the need for ETL tools.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Real-Time Event Streaming and Data Transformation for Financial Services","url":"http://localhost:1315/resources/real-time-data-trasformation/"},{"body":"Fluvio is an open-source data streaming platform with in-line computation capabilities. It\u0026rsquo;s distributed stream processing with redundancy and failover, prevents data loss and minimize downtime. Programmable stream processing is used to clean, transform, correlate, and derive insights from data in real-time. Fluvio’s performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.\nView this introduction to Fluvio session to learn -\n• Why developers are choosing Fluvio over Kafka, Pulsar and Flink\n• What features and functionality are unique in the market\n• How to quickly get started with Fluvio\n","description":"See Fluvio open-source software in action.","keywords":null,"summary":"Fluvio is an open-source data streaming platform with in-line computation capabilities. It\u0026rsquo;s distributed stream processing with redundancy and failover, prevents data loss and minimize downtime. Programmable stream processing is used to clean, transform, correlate, and derive insights from data in real-time. Fluvio’s performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.\nView this introduction to Fluvio session to learn -\n• Why developers are choosing Fluvio over Kafka, Pulsar and Flink","title":"Introduction to Fluvio, The Programmable Platform for Data in Motion","url":"http://localhost:1315/resources/introduction-to-fluvio/"},{"body":"Fluvio is an open-source data streaming platform with in-line computation capabilities. It\u0026rsquo;s distributed stream processing with redundancy and failover, prevents data loss and minimize downtime. Programmable stream processing is used to clean, transform, correlate, and derive insights from data in real-time. Fluvio’s performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.\nView this introduction to Fluvio session to learn -\n• Why developers are choosing Fluvio over Kafka, Pulsar and Flink\n• What features and functionality are unique in the market\n• How to quickly get started with Fluvio\n","description":"See Fluvio open-source software in action.","keywords":null,"summary":"Fluvio is an open-source data streaming platform with in-line computation capabilities. It\u0026rsquo;s distributed stream processing with redundancy and failover, prevents data loss and minimize downtime. Programmable stream processing is used to clean, transform, correlate, and derive insights from data in real-time. Fluvio’s performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.\nView this introduction to Fluvio session to learn -\n• Why developers are choosing Fluvio over Kafka, Pulsar and Flink","title":"Introduction to Fluvio, The Programmable Platform for Data in Motion","url":"http://localhost:1315/videos/introduction-to-fluvio/"},{"body":"","description":"Learn how Fluvio's performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.","keywords":null,"summary":"","title":"Introduction to Fluvio, The Programmable Platform for Data in Motion","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/2865428877366707468/"},{"body":"Despite the occasionally tedious process of adapting to the new normal, developments and trends in technology are still emerging more quickly than ever. To that point, real-time analytics continues to play a key role in how businesses are rebuilding their structures and operations to become more future-ready. By this point, nearly two-thirds of organizations agree that real-time data has become necessary for decision-making in business.\nObserving the progress of the industry in the past year, it’s safe to say that while some trends are staying put, there are others coming in to dominate the playing field as well. Here, we\u0026rsquo;ll provide an overview of specific real-time data trends that are going to shape the industry in the next few years.\nReal-Time Overtaking Big Data For a long time now, big data has been at the forefront of the most important business practices. This might be changing soon. It\u0026rsquo;s clear now that big data has become less relevant than \u0026ldquo;fast data.\u0026rdquo; To give a few examples of how and where this is becoming apparent, we see e-commerce shoppers, streaming media consumers, gamers, and digital traders alike demanding faster processing of insights and information. Furthermore, startups that are able to analyze and extract insights from fast data often garner high public and private market investor interest –– which goes to show why organizations want to use the most recent and relevant data, and avoid using that which may be extraneous or outdated.\nContinuous Intelligence Platforms Because today’s organizations are data-driven, time-to-value is measured in milliseconds rather than hours. Big names like Amazon, Netflix, Google, and Alibaba all stay ahead of the game through their use of real-time analytics, but any organization can also thrive by adopting a continuous intelligence platform like ours here at InfinyOn. This platform can detect, react, and respond to relevant events in milliseconds, which in turn will improve efficiency across teams. It can also help your organization make smarter decisions by detecting market trends and changes by your competitors in real-time.\nFast-Adapting Training Sphere We also expect to see computer science programs adapting quickly to the new trend towards real-time data, with schools and programs rising to meet the challenges of training analysts. Those adept at technical problem-solving and critical thinking are expected to perform well in this fast-paced field, and will gain the high-level education and training they need to have an edge as tech professionals. Worth noting in this regard is that data analysts are consistently included among the most coveted jobs in the U.S., which drives educational institutions and business leaders to keep their curricula and training methods up to date.\nAI Operating on Real-Time Data AI models and tools are not yet widely adopted by marketers and customer experience managers. Although they respond quickly to changes, there’s a chance that algorithms may perform badly when input data differs too much from the data they\u0026rsquo;ve been trained on. However, companies are now seeking to make use of these models in real time using live data. Tackling the challenges of using real-time streaming data is also going to become even more urgent in the coming year, as more and more businesses plan to adopt AI practices as part of their regular operations.\nCloud Solutions for Real-Time Analytics In previous years, companies have collected, transported, and stored as much data as possible in the cloud. This practice has only been further necessitated as remote work, e-commerce, streaming services, and many other factors have come along. More than just accommodating these kinds of changes though, the cloud also improves real-time analytics by increasing network bandwidth, reducing the time of the analysis process to mere minutes, and providing access to large amounts of memory, among other benefits. Moreover, the cloud is also widely used in the industry as a way to launch relatively cost-effective solutions.\nReal-Time is the Future Overall, we see that businesses will continue to leverage real-time technology to gain valuable insights — ones that could potentially help them switch from being reactive to proactive. To stay ahead of the curve, you\u0026rsquo;ll want to make use of services that can process data as fast as it comes in. Interested? Check out our products for more information.\nContent intended only for the use of infinyon.com By Vera Soto\n","description":"Businesses will continue to leverage real-time technology to gain valuable insights.","keywords":null,"summary":"Despite the occasionally tedious process of adapting to the new normal, developments and trends in technology are still emerging more quickly than ever. To that point, real-time analytics continues to play a key role in how businesses are rebuilding their structures and operations to become more future-ready. By this point, nearly two-thirds of organizations agree that real-time data has become necessary for decision-making in business.\nObserving the progress of the industry in the past year, it’s safe to say that while some trends are staying put, there are others coming in to dominate the playing field as well.","title":"Future Trends in Real-Time Data","url":"http://localhost:1315/blog/2022/02/future-trends-in-real-time-data/"},{"body":" Prev Next Page: / Download ","description":"Read this eBook to learn about the latest and future trends in real-time data.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Future Trends in Real-Time Data","url":"http://localhost:1315/resources/future-trends-in-real-time-data-ebook/"},{"body":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time data streaming company, announced the launch of a new Postgres source and sink connectors for it’s open source Fluvio and InfinyOn Cloud platform. Postgres is a free and open-source relational database management system emphasizing extensibility and SQL compliance.\n“There are many benefits that include using the same data stream to perform data replication, generate events, and send alerts in real-time such as “item Y has sold out”. Having the ability to get audit trails, parallel replication with multiple tables to different destinations and easily manage inline transformations without the need for other tools or technologies is powerful” said A.J. Hunyady, CEO of InfinyOn.\nThe Postgres connectors feature the ability to replicate one or more tables from your postgres database anywhere in the world, perform incremental changes in real-time and perform inline transformations such as masking social security numbers or anonymize addresses with SmartModules. Database replication is a common use-case for data-driven companies across all industries. Streaming events from Postgres can be ingested and dispatched to other systems or applications and the data transformation happens in real time.\n“We are currently hiring for our connector engineering team and will be launching new source and sink data connectors every month. Over 5,000 companies currently use PostgreSQL in their tech stacks, including Apple, Reddit, Spotify, Skype, Twitch, Uber, Netflix, and Instagram.” said InfinyOn CTO, Sehyo Chang. Postgres ranked 2nd on the Stack Overflow survey for most popular database technologies in 2021 with 70% of respondents saying they love it.\nAbout InfinyOn InfinyOn, a real-time data streaming company, has architected a programmable platform for data in motion that is built on Rust and enables continuous intelligence for connected apps. SmartModules enable enterprises to intelligently program their data pipelines as the data flows between producers and consumers in real-time. With Fluvio OSS or InfinyOn Cloud, enterprises can quickly correlate events, apply business intelligence, and derive value as they occur. Our mission is to accelerate the world\u0026rsquo;s transition to the real-time economy. To learn more, please visit infinyon.com.\n","description":"New connector for building high-performance data pipelines.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time data streaming company, announced the launch of a new Postgres source and sink connectors for it’s open source Fluvio and InfinyOn Cloud platform. Postgres is a free and open-source relational database management system emphasizing extensibility and SQL compliance.\n“There are many benefits that include using the same data stream to perform data replication, generate events, and send alerts in real-time such as “item Y has sold out”.","title":"InfinyOn Announces Postgres Connector for Fluvio \u0026 InfinyOn Cloud","url":"http://localhost:1315/press-releases/postgres-connector/"},{"body":"InfinyOn is powering the real-time economy with intelligent data streaming. In 2021, mobile-generated traffic reached 54% of all internet traffic, changing the way users interact with businesses and brands. This transition means that all the elements of business transactions, including accounting, business reporting, customer analytics, inventory management, invoicing, logistics and payments must be processed as it happens in real time.\nSupply chain management in the real-time economy means there are insights into inventory, orders, vehicles, people and equipment using real-time information. This visibility into real-time data is an advantage for both logistics providers and their customers. Data is being fed into machine learning models that are creating smarter AI based on the data.\nLogistics Supply chains of the retail and wholesale distribution, e-commerce, grocery, food and beverage, manufacturing and transport industries are transforming their businesses with real-time logistics solutions. This enables an immediate response to external events and provides visibility that is essential for these businesses to compete in a connected real-time economy. It specifies logistic activities that track and trace, in real-time, the movement of goods and packages from the manufacturers, suppliers, warehouses and hubs to the end customer.\nWhen supply chains are operating with an infrastructure that gives them instant access to vital information, it helps to identify risks and figure out corrective action before the risk leads to major losses. Mitigating risks early leads to enhanced reliability, higher productivity, more transparency, and high profits.\nReal-time visibility in the supply chain leverages GPS-tracking to allow businesses to plan, schedule, and monitor their logistics process at all points. It provides supply officers with access to trackable information such as order receipts, the status of raw materials, shipping details, regulatory information, and the exact location of the order. Better management of this complex process helps enterprises to gain a competitive edge, enhance productivity, increase customer satisfaction, and also reduce transportation costs.\nSmart Cities Example This Smart City, end to end use case, that tracks the Helsinki public transit system shows how to reduce complexity and time to market for continuous intelligence. The diagram below of the Helsinki Bus architecture shows the power of InfinyOn Cloud for data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses. In this example we are ingesting over 3k events per second. The business logic was completed in just a few hours and the parsing of events, aggregate and join on an inline data stream is unique to InfinyOn.\nReal-time Payments (RTP) According to Statistica, there were over 800 billion cashless transactions in 2021 with that number expected to grow to over 1 trillion transactions in 2022. Real-time payments (RTP) are payments that are initiated and settled nearly instantaneously and are replacing ACA, cash and check payments. Leaders in the real-time payment space have networks that provide always-on access, which means they are always online to process money transfers. Robust messaging is used to send and receive information about payments. Payment on demand when products or services are delivered is another common use-case for RTP.\nThere is considerable opportunity for both real-time payment solutions providers and financial institutions to work together to offer consumers quicker transactions. The value of real-time payments is instant access to funds. For consumers or companies that need the funds as soon as possible, instant access can bring a delightful customer experience.\nReal-time Inventory Management Real-time inventory management is a critical component of the real-time economy and running a successful business. Efficient inventory management can make or break your business for both supply and demand purposes. Real-time inventory management is the ability to automate the process of collecting transactional data on sales, shipments and movement of goods in order to optimize and proactively restock inventory.\nBenefits include the ability to predict inventory stocking, visibility into current stock at any time and ensure accurate financial planning for cost of goods sold. With InfinyOn, companies can convert data into a common inventory, and stream event data to business intelligence tools, databases or data lakes for analytics and storage.\nEnhancing Machine Learning Models Machine learning and AI are emerging technologies that are revolutionizing businesses, brands, and even entire industries. Machine learning models are oftentimes based on real-time calculations for the use cases they serve. Companies have a brief window of opportunity to make predictions that can have an immediate business impact. Real-time data pipelines can be used to enhance machine learning models.\nAccording to the Wikipedia definition, “Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.” Essentially the model is only as good as the data that it\u0026rsquo;s fed.\nStreaming data is an excellent way to feed machine learning models in order to make more accurate predictions. The machine learning model provides logic that assists the streaming data pipeline to uncover elements within the stream and potentially within historical data. Real-time scores based on the elements are generated and delivered to BI tools and applications so they can make their predictions or recommendations.\nCustomer Analytics and Customer 360 Customer analytics is one of the most common use-cases for the real-time economy. According to Harvard Business Review, 70% of enterprises have increased their spending on real-time customer analytics solutions over the past year. 60% use real-time customer analytics to improve customer experience across touchpoints and devices. 58% of enterprises are seeing a significant increase in customer retention and loyalty as a result of using real-time customer analytics.\nThe companies they surveyed were asked, “What’s the best way for businesses to differentiate themselves today? By delivering a unique, real-time customer experience across all touch points and one that is based on a solid, connected business strategy driven by data and analytics insights.”\nPlacing a continuous intelligence platform at the heart of a business enables the discovery of new business opportunities. By both centralizing data across the company, and simultaneously distributing it to every application or system, new insights can reveal unexpected ways to build new revenue sources. Improved customer experiences, streamlined operations, faster decision making, better collaboration, increased innovation, and better ability to compete are all benefits derived from operating in real-time.\nContinuous Intelligence Continuous intelligence is a design pattern in which real-time analytics are integrated into business operations, processing current and historical data to prescribe actions in response to business moments and other events. The InfinyOn continuous intelligence platform contains a modern event stream processing engine that performs real-time calculations for data in motion.\nWe\u0026rsquo;ve developed a programmable, scalable, cloud-native Continuous Intelligence Platform that processes data in real time. InfinyOn helps Enterprises detect, react, and respond to meaningful events in milliseconds vs. hours, days and weeks while integrating with your existing systems and tools. No infrastructure overhaul required. Just add your data sources, compose your intelligent pipelines in minutes and dispatch actionable events to all relevant stakeholders.\nConclusion Our mission is to accelerate the world\u0026rsquo;s transition to the real-time economy. Real-time data can give businesses valuable insight into product performance, customer behavior, supply chain management, business planning and more. Companies can no longer afford to spend months or years and millions of dollars on building out a real-time data infrastructure. To learn more about InfinyOn please visit www.infinyon.com or if you are interested in our open source Fluvio software go to www.fluvio.io.\n","description":"How InfinyOn is accelerating the real-time economy with InfinyOn Cloud and Fluvio.","keywords":null,"summary":"InfinyOn is powering the real-time economy with intelligent data streaming. In 2021, mobile-generated traffic reached 54% of all internet traffic, changing the way users interact with businesses and brands. This transition means that all the elements of business transactions, including accounting, business reporting, customer analytics, inventory management, invoicing, logistics and payments must be processed as it happens in real time.\nSupply chain management in the real-time economy means there are insights into inventory, orders, vehicles, people and equipment using real-time information.","title":"Accelerating the Real-Time Economy","url":"http://localhost:1315/blog/2022/02/real-time-economy-blog/"},{"body":" Prev Next Page: / Download ","description":"This eBook covers the benefits of Fluvio open source software compared to Kafka, Pulsar, Flink and Spark.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Fluvio - The Programmable Data Platform","url":"http://localhost:1315/resources/fluvio-progammable-data/"},{"body":" Prev Next Page: / Download ","description":"This whitepaper covers the core elements of the real-time economy.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Accelerating the Real-Time Economy","url":"http://localhost:1315/resources/real-time-economy/"},{"body":"","description":"Learn how to quickly correlate events, apply business intelligence, and derive value from real-time data.","keywords":null,"summary":"","title":"Why Real-time Data Matters","url":"http://localhost:1315/webinars/https/register.gotowebinar.com/register/5284910811222538255/"},{"body":"Data and Analytics leaders across all industries are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Real-time data offers a range of benefits for organizations of any size. According to Gartner, businesses must use stream processing to meet their needs for continuous intelligence and real-time analytics.\nTo take advantage of the growing availability of real-time streaming data, organizations are reengineering how they make decisions by implementing event stream processing. Click the video below to get insights into the top 5 reasons why real-time data matters.\n","description":"This webinar covers the top 5 reasons why real-time data matters.","keywords":null,"summary":"Data and Analytics leaders across all industries are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Real-time data offers a range of benefits for organizations of any size. According to Gartner, businesses must use stream processing to meet their needs for continuous intelligence and real-time analytics.\nTo take advantage of the growing availability of real-time streaming data, organizations are reengineering how they make decisions by implementing event stream processing.","title":"Why Real-Time Data Matters","url":"http://localhost:1315/resources/why-real-time-data-matters/"},{"body":"Data and Analytics leaders across all industries are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Real-time data offers a range of benefits for organizations of any size. According to Gartner, businesses must use stream processing to meet their needs for continuous intelligence and real-time analytics.\nTo take advantage of the growing availability of real-time streaming data, organizations are reengineering how they make decisions by implementing event stream processing. Click the video below to get insights into the top 5 reasons why real-time data matters.\n","description":"This webinar covers the top 5 reasons why real-time data matters.","keywords":null,"summary":"Data and Analytics leaders across all industries are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Real-time data offers a range of benefits for organizations of any size. According to Gartner, businesses must use stream processing to meet their needs for continuous intelligence and real-time analytics.\nTo take advantage of the growing availability of real-time streaming data, organizations are reengineering how they make decisions by implementing event stream processing.","title":"Why Real-Time Data Matters","url":"http://localhost:1315/videos/why-real-time-data-matters/"},{"body":" Prev Next Page: / Download ","description":"Streaming data to feed machine learning models in order to make more accurate predictions.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Enhancing ML Models with Real-time Data Pipelines","url":"http://localhost:1315/resources/enhancing-ml-models/"},{"body":"Since its creation, SmartModules have allowed users to write custom code to interact with their streaming data in real-time. This blog will explore a new way to impact a SmartModule behavior through in-line parameters. As a result, different consumers may apply the same SmartModule to a data stream and receive a different result based on its unique parameter. For example, in the bus demo video, we used SmartModule parameters to locate a bus number from the fleet. This blog will use parameters on a SmartModule Map to transform records based on user-defined arguments.\nCheck out the full code in the fluvio-smartmodule-examples repository.\nScenario: Web user events Let\u0026rsquo;s say that we have a webpage where we track the actions that our registered users can do. In particular, let\u0026rsquo;s say that our users can only do any of these events:\nregister: a new user registered. login: user logged in. logout: user logged out. home_page: user visited home page. play_demo: user played demo video. action_a: user performed action A. action_b: user performed action B. In our system, we may want to anonymize or hide particular fields of the events for specific use cases. For example, we may want to take an input like this:\n{\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878631,\u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878637, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878650, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878731, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878763, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} and turn it into a new stream that looks like this:\n{\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878631, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878637, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878650, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878731, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878763, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} or maybe we may want to just remove the account_id and the user_client fields and turn it into a new stream that looks like this:\n{\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878631} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878637} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878650} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878731} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878763} We\u0026rsquo;ll use SmartModule parameters to implement these features in the next section.\nCreate a new project We can use the amazing cargo-generate tool to help us get started quickly with a Map template project. If you don\u0026rsquo;t already have it installed, you can get it with this command:\n%copy first-line%\n$ cargo install cargo-generate After you have cargo-generate installed, you can create a smartmodule project using map and parameters template using the following command:\n%copy first-line%\n$ cargo generate --git=https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : smartmodule-with-params 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · map ✔ 🤷 Want to use SmartModule parameters? · true [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `smartmodule-with-params`... ✨ Done! New project created smartmodule-with-params Note that, we selected map as the SmartModule type and that we wanted to use SmartModule parameters.\nLet\u0026rsquo;s navigate into our project directory and take a look at the sample code we were given:\n%copy first-line%\n$ cd smartmodule-with-params \u0026amp;\u0026amp; cat src/lib.rs We should see the following code:\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; #[smartmodule(map, params)] pub fn map(record: \u0026amp;Record, _params: \u0026amp;SmartModuleOpt) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { let key = record.key.clone(); let string = std::str::from_utf8(record.value.as_ref())?; let int = string.parse::\u0026lt;i32\u0026gt;()?; let value = (int * 2).to_string(); Ok((key, value.into())) } #[derive(fluvio_smartmodule::SmartOpt, Default)] pub struct SmartModuleOpt; This template code is one of the smallest possible Maps. It takes each input record as an integer, then multiplies it by two.\nNote that in has a _params argument that is not being used and that the macro attribute of the map function includes the params keyword: #[smartmodule(map, params)]. This is needed in order to use SmartModule parameters.\nIn order to use SmartModule parameters we also need to define a struct that implements Default and that derives the SmartOpt derive macro. It is also mandatory that all fields of the custom structure defined implement the FromStr trait.\nFor our purposes, we may want to start by defining a data structure that represents the different types of events that appear in our stream. We can use the serde and serde_json crates to help us deserialize this data structure from JSON. If you\u0026rsquo;re following along with the template, you should already have serde and serde_json as dependencies, so let\u0026rsquo;s look at how to write the code we need. Since we\u0026rsquo;re talking about distinct event types, we can use a Rust enum to represent this data type.\nBelow is the full code for the example. Look at the UserEvent enum that represents the input data, and the UserEventOutput struct that represents the output data we generate.\nPaste the following code into src/lib.rs:\n%copy%\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; use serde::{Deserialize, Serialize}; #[derive(Deserialize)] #[serde(tag = \u0026#34;type\u0026#34;, rename_all = \u0026#34;snake_case\u0026#34;)] pub enum UserEvent { Login(UserEventMetadata), Logout(UserEventMetadata), Register(UserEventMetadata), ActionA(UserEventMetadata), ActionB(UserEventMetadata), HomePage(UserEventMetadata), PlayDemo(UserEventMetadata), } #[derive(Deserialize)] pub struct UserEventMetadata { pub account_id: String, pub timestamp: i64, pub user_client: String, } impl UserEventMetadata { fn convert(self, params: \u0026amp;SmartModuleOpt) -\u0026gt; UserEventMetadataOutput { let account_id = if params.show_account_id { Some(self.account_id) } else { None }; let timestamp = if params.show_timestamp { Some(self.timestamp) } else { None }; let user_client = if params.show_user_client { Some(self.user_client) } else { None }; UserEventMetadataOutput { account_id, timestamp, user_client, } } } #[derive(Serialize)] #[serde(tag = \u0026#34;type\u0026#34;, rename_all = \u0026#34;snake_case\u0026#34;)] pub enum UserEventOutput { Login(UserEventMetadataOutput), Logout(UserEventMetadataOutput), Register(UserEventMetadataOutput), ActionA(UserEventMetadataOutput), ActionB(UserEventMetadataOutput), HomePage(UserEventMetadataOutput), PlayDemo(UserEventMetadataOutput), } #[derive(Serialize)] pub struct UserEventMetadataOutput { #[serde(skip_serializing_if = \u0026#34;Option::is_none\u0026#34;)] pub account_id: Option\u0026lt;String\u0026gt;, #[serde(skip_serializing_if = \u0026#34;Option::is_none\u0026#34;)] pub timestamp: Option\u0026lt;i64\u0026gt;, #[serde(skip_serializing_if = \u0026#34;Option::is_none\u0026#34;)] pub user_client: Option\u0026lt;String\u0026gt;, } impl UserEvent { fn convert(self, params: \u0026amp;SmartModuleOpt) -\u0026gt; UserEventOutput { match self { UserEvent::Login(metadata) =\u0026gt; UserEventOutput::Login(metadata.convert(params)), UserEvent::Logout(metadata) =\u0026gt; UserEventOutput::Logout(metadata.convert(params)), UserEvent::Register(metadata) =\u0026gt; UserEventOutput::Register(metadata.convert(params)), UserEvent::ActionA(metadata) =\u0026gt; UserEventOutput::ActionA(metadata.convert(params)), UserEvent::ActionB(metadata) =\u0026gt; UserEventOutput::ActionB(metadata.convert(params)), UserEvent::HomePage(metadata) =\u0026gt; UserEventOutput::HomePage(metadata.convert(params)), UserEvent::PlayDemo(metadata) =\u0026gt; UserEventOutput::PlayDemo(metadata.convert(params)), } } } #[smartmodule(map, params)] pub fn map(record: \u0026amp;Record, params: \u0026amp;SmartModuleOpt) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { let event: UserEvent = serde_json::from_slice(record.value.as_ref())?; let output = event.convert(params); let value = serde_json::to_string(\u0026amp;output)?; Ok((record.key.clone(), value.into())) } #[derive(fluvio_smartmodule::SmartOpt)] pub struct SmartModuleOpt { show_account_id: bool, show_timestamp: bool, show_user_client: bool, } impl Default for SmartModuleOpt { fn default() -\u0026gt; Self { Self { show_account_id: true, show_timestamp: true, show_user_client: true, } } } Let\u0026rsquo;s quickly look at what\u0026rsquo;s happening with our data structures:\nSince we\u0026rsquo;re working with different event, each enum variant represents one event type We\u0026rsquo;re using #[serde(tag = \u0026quot;type\u0026quot;)] to add a \u0026ldquo;type\u0026rdquo; field to each event with the name of the variant We\u0026rsquo;re using #[serde(rename_all = \u0026quot;snake_case\u0026quot;)] to rename the variants from e.g. PlayDemo to play_demo We have a SmartModuleOpt struct that implements Default and has the derived macro SmartOpt All fields in SmartModuleOpt are booleans (boolean implements the FromStr trait) By default, all fields in SmartModuleOpt are true. This means that if we don\u0026rsquo;t pass any parameters all the fields will be displayed. We implemented a UserEvent::convert function that takes as input the \u0026amp;SmartModuleOpt and returns an UserEventOutput. Now, let\u0026rsquo;s look at what\u0026rsquo;s going on inside the map function itself:\nFirst, we read the input as a JSON UserEvent called event Then we transform our UserEvent into UserEventOutput using the UserEvent::convert function described above with paramaters passed to the smartmodule. We are now ready to compile. If you\u0026rsquo;ve never compiled for WASM before, you\u0026rsquo;ll need to install the proper rustup target. You should only need to do this once.\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown Let\u0026rsquo;s go ahead and compile it, using --release mode to get the smallest WASM binary possible:\n%copy first-line%\n$ cargo build --release Let\u0026rsquo;s get set up on Fluvio and see our new smartmodule with params in action!\nTesting the Parameters in smartmodule with Fluvio CLI In order to follow along, make sure you have Fluvio installed and are up and running with a Fluvio cluster. The first thing we\u0026rsquo;ll need to do is to create a new Fluvio topic for us to stream our events.\n%copy first-line%\n$ fluvio topic create user-events Next, we\u0026rsquo;ll want to produce some sample records to this topic, these will act as the input to our SmartModule.\n%copy first-line%\n$ fluvio produce user-events \u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878631,\u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} Ok! \u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878637, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} Ok! \u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878650, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} Ok! \u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878731, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} Ok! \u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: 1640878763, \u0026#34;user_client\u0026#34;: \u0026#34;safari\u0026#34;} Ok! At this point, we\u0026rsquo;re ready to get to work with our smartmodule. Let\u0026rsquo;s use our Map while we consume records from our topic using the following command:\n%copy first-line%\n$ fluvio consume user-events -B --map=./target/wasm32-unknown-unknown/release/smartmodule_with_params.wasm {\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878631,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878637,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878650,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878731,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878763,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} As you can see, the output remains unchanged. This is happening because we are calling the smartmodule without passing values to the parameters it is using. When this happens, it uses the default value, which as we already mentioned is to display everything.\nIn order to pass parameters to the smartmodule using the CLI, we need to use the -e key=value flag. Let\u0026rsquo;s try to hide the account_id field:\n%copy first-line%\n$ fluvio consume user-events --map target/wasm32-unknown-unknown/release/smartmodule_with_params.wasm -B -e show_account_id=false Consuming records from the beginning of topic \u0026#39;events\u0026#39; {\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878631,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878637,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878650,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878731,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878763,\u0026#34;user_client\u0026#34;:\u0026#34;safari\u0026#34;} Now, let\u0026rsquo;s try to hide both account_id and user_client fields:\n%copy first-line%\nfluvio consume user-events --map target/wasm32-unknown-unknown/release/smartmodule_with_params.wasm -B -e show_account_id=false -e show_user_client=false Consuming records from the beginning of topic \u0026#39;events\u0026#39; {\u0026#34;type\u0026#34;:\u0026#34;login\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878631} {\u0026#34;type\u0026#34;:\u0026#34;home_page\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878637} {\u0026#34;type\u0026#34;:\u0026#34;play_demo\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878650} {\u0026#34;type\u0026#34;:\u0026#34;action_a\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878731} {\u0026#34;type\u0026#34;:\u0026#34;action_b\u0026#34;,\u0026#34;timestamp\u0026#34;:1640878763} We can see that the output stream hides the fields that we don\u0026rsquo;t want to display if we pass them through the CLI. This is useful if we want to reuse a smartmodule for different similar purposes.\nConclusion That\u0026rsquo;s it for this post, be sure to join our Discord server if you want to talk to us or have any questions. Until next time!\nFurther reading The InfinyOn Continuous Intelligence Platform Using Fluvio FilterMap to apply focus to real-time data ","description":"How to use SmartModules parameters to define alternate behaviors in data stream processing.","keywords":null,"summary":"Since its creation, SmartModules have allowed users to write custom code to interact with their streaming data in real-time. This blog will explore a new way to impact a SmartModule behavior through in-line parameters. As a result, different consumers may apply the same SmartModule to a data stream and receive a different result based on its unique parameter. For example, in the bus demo video, we used SmartModule parameters to locate a bus number from the fleet.","title":"Fluvio SmartModules with user-defined parameters","url":"http://localhost:1315/blog/2021/12/smartmodule-params/"},{"body":" Prev Next Page: / Download ","description":"Drive business value by increasing revenue, decreasing costs or mitigating risk with the following use cases.","keywords":null,"summary":" Prev Next Page: / Download ","title":"InfinyOn Cloud Use Cases for Enterprises and Developers","url":"http://localhost:1315/resources/use-cases-ebook/"},{"body":" Prev Next Page: / Download ","description":"Extract, detect, and react to internal or external events in real-time.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Continuous Intelligence for Chief Data Officers","url":"http://localhost:1315/resources/cdo-article/"},{"body":"As more of our work moves online, there is a growing need for collaboration apps where businesses communicate with customers and employees in real-time. These new collaboration apps will accelerate decisions, improve customer experience, and open new revenue opportunities.\nIn the article “Reimagining the post-pandemic organization\u0026quot;, McKinsey astutely observes that antiquated data systems are holding many companies back, forcing them to rely on manual processes to make decisions. Modern businesses must be able to collaborate with customers, partners, and employees in real-time, and do so through a variety of communication channels, organization’s boundaries, time zones, and legal jurisdictions. Making this transition requires new infrastructure that swaps out manual processes with collaboration centric real-time workflows. The proper infrastructure is needed to link human actions with systems, and to facilitate the transition from legacy systems with new technology.\nWe’ve found that developers are struggling to build real-time collaboration apps that connect users with the business contexts they care about. A business context is derived from data, but this data is often fragmented and distributed by different teams across an organization. Real-time data handling is a way of bringing data to users as fast as possible. A real-time collaboration app makes data available to users at the time of communication, which accelerates the speed of decisions.\nThere are already a number of vertical collaboration systems like Zoom, Slack, and Teams that function primarily as communication platforms. Yet these solutions do not account for business context. Developers focusing on creating real-time collaboration apps must either build plugins for these communication platforms or build their own real-time collaboration infrastructure. Plugin frameworks often provide inadequate access to the business context, and real-time collaboration infrastructure requires significant investments of time and expertise. Furthermore, most organizations do not have the resources to maintain and operate such a complex platform.\nIn order to solve the problems described above, we believe there is an opportunity for a purpose-built platform for real-time collaboration. A platform that is easy to operate, requires no upfront investment and allows developers to join data, build business logic, and shorten the time required to roll-out real-time collaboration apps.\nBusiness Faces an Existential Challenge The pandemic has fundamentally changed the way businesses operate. Not only do employees need to stay connected while working from home, but customers demand solutions with immediate feedback and minimal physical interactions. Beyond this, partners need real-time notification for deliveries, supply chain management, or other coordinated activities.\nBecause of these challenges, it’s no longer viable to maintain antiquated tools and processes that require constant manual intervention, or to use legacy communication channels for daily business activities. The more manual interventions there are, the more unpredictable resolution time becomes. Traditional resolution times range from minutes to days. Modern organizations that use real-time connections and automated activities require predictable resolution time measuring just milliseconds to seconds.\nLet’s take a look at an example of how a real-time collaboration app could dramatically improve a transaction at a car dealership.\nA Car Dealership Example The typical method of dealing with transactions at a car dealership involves many inefficient manual processes and antiquated communication methods. Long delays, part shortages, insufficient loaners are common annoyances that erode customer confidence and impact the success of the business.\nImagine taking your car to a dealership for an oil change appointment. The process might go like this:\nFirst, the parking attendant greets you, places a number on your windshield and walks inside to inform the service advisor. A few minutes later, the service advisor walks-up and takes you to his office to check your service records. The advisor is the master coordinator, and he must call up the service department to ensure the parts are available, check the system to see if your service is covered by warranty, etc. In most cases, you’ll be promised a call later in the day. Next, you need to arrange your transportation with the vehicle loan office. There is a long line, but after waiting you find out they are out of cars, so you take an Uber instead. A few hours later, you get a call from the service advisor who informs you that the car must receive a mandatory service recall and the part is in backorder, scheduled to arrive in a couple of days. What was supposed to be a short maintenance appointment has turned into a multi-day ordeal.\nThese challenges are not unique to car dealerships. Yet you can see the problems afflicting businesses relying on manual processes and primitive communication channels to interact with employees, vendors, and customers.\nA Vision for a Modern Car Dealership Now imagine the same scenario at a modern car dealership:\nWhen you arrive, a camera reads your license plate, matches your car to the VIN number, and notifies the backend platform of your arrival.\nThe platform looks up your service records, warranty information, and mandatory recalls. It ensures that all parts are available, generates an invoice, checks the loaner location and sends you a welcome message. The message has the cost of service, the loaner location, and the estimated pick-up time.\nYou step out of the car, enter the loaner, and drive off. You are in and out of the car dealership in minutes.\nThis type of transformation from legacy business to modern business can be highly disruptive and many businesses may see it as an impossible undertaking. We believe proper planning and the right technology can make this transition possible for any organization. With Fluvio, the transition can be as painless as possible.\nIntroducing Fluvio A Data Platform for Real-Time Collaboration Fluvio is a high performance distributed platform that coordinates all real-time data exchanges in an organization. It deploys in minutes, operates hands-free, and covers a broad range of use cases. The platform is suitable for services such as:\nLog aggregation for servers Command center for IOT devices Middle tier for microservices Notification services for mobile devices Fully-featured collaborative apps, with chat, voice, video, geo-tracking, and more. The Fluvio data platform for real-time collaboration was built on these three pillars:\nData Streams Collaboration Ease of Use Data Streams Streaming Data is data that is generated and delivered continuously. A data stream often consists of individual messages that describe events that have occurred. A key advantage of streaming data, as opposed to having batches of data delivered periodically, is that the average time to take action on an event is much lower. Since streaming data is delivered immediately, events can be processed and acted upon immediately. Therefore, a critical prerequisite to building real-time collaborative applications is having a scalable, durable, low-latency data streaming platform. We built Fluvio to meet this need.\nFluvio data streaming platform is built to optimize the following: speed, scale, retention, and resilience.\nSpeed Fluvio takes advantage of all system cores and hardware IO interfaces to achieve high-speed and low latency. This is a game-changer for businesses like financial services, gaming and augmented reality companies, or service providers offering 5G services.\nScale As organizations grow and new services are deployed, the volume of real-time data grows in proportion. Growing organizations must be able to effectively scale their data streams and platforms. Fluvio is designed for parallelism and horizontal scale.\nAt the core Fluvio’s design are Streaming Processing Units (SPUs). A Fluvio cluster can handle hundreds of SPUs and each SPU can serve thousands of consumers. This makes the platform capable of handling any real-time data streaming needs, from a small one-person prototype to the most demanding multi-team, multi-app, globally distributed environments.\nFluvio allows you to scale-up your cluster gradually rather than provisioning for peak. As your demand for real-time streaming grows, you can simply add SPUs and Fluvio will do the rest.\nHorizontal scale ensures you don’t need new real-time infrastructure to deploy new services. You also won’t need to ask your IT department to work overtime when you roll out a marketing campaign that brings more clients to your service.\nRetention The term retention is often found in the context of storage systems, where it defines the time period the data should persist for. Organizations may have data retention policies that range from 5 to 10 years. In the context of real-time data systems, retention periods are more often measured in seconds to minutes.\nFluvio extends the retention period for real-time data streams from seconds to years. Each data stream is an immutable store, where the messages are written in the order they are received. A long lived immutable store sets the foundation for a series of new features. The data streams can become the authoritative data source (A.K.A. System of Record (SOR)) for all events generated by different systems organization-wide. These events can be played back anytime in the future. Playbacks can be used to:\nRecover a persistent store Rebuild an in-memory database Playback historical data for new features Perform retrospective analysis Run audit reports, and more. For example, suppose a dealership stored all appointments in a database that was corrupted during a power outage. The last backup was performed the previous week and 7 days of data had been lost. Since Fluvio was used, the database changes were also captured as events in a data stream. The events were played back and all appointments were recovered.\nResiliency Resiliency is the ability to continue operating in the event of a failure. A platform responsible for real-time data services must be highly resilient.\nFluvio data streams are replicated for durability and they can survive multiple points of failure. Failures may result from system crashes, network outages, or reasons yet to be determined Should a failure happen, Fluvio ensures that data streams remain available with minimal interruption. Once a failure is detected, the cluster redistributes workloads and temporarily removes the failed component from the quorum. Upon recovery, the cluster ensures the component is fully synchronized before it is allowed to re-join the quorum. For additional information on high availability, check out the Fluvio architecture.\nFluvio supports a wide range of deployment scenarios: single cloud, multi-cloud, cloud-to-data center, and more. If Fluvio is deployed in a single availability zone, it protects against server outages. When deployed across availability zones, Fluvio protects against zone outages, and when deployed in hybrid mode it protects against data center outages. It’s up to the users to define the deployment model most suitable for their environment.\nTo sum up: Fluvio is low-latency. Fluvio also allows you to scale up and manage data streams as your needs increase. Fluvio’s retention reliably stores incoming data streams for years. Fluvio’s resiliency provides protection against outages and crashes. Collaboration At the top-most level, a real-time collaboration app has 3 components: a collaboration interface, a real-time streaming platform, and one or more business logic services.\nThe collaboration interface allows users to communicate with each other and produce an action. The real-time streaming platform passes the action to one or more services, and returns a result. The services translate the request into a response, based on the organization’s business logic.\nCollaboration Interface The design of the collaboration interface depends on the organization\u0026rsquo;s needs. Some organizations prefer to integrate with purpose-built communication tools such as Slack or Teams. Others prefer to build custom interfaces where they can control the workflow end-to-end. Fluvio supports both of these collaboration styles.\nIn terms of support for purpose built tools, Fluvio gives access to a programmable interface that allows the creation of a custom connector. Connectors can be configured to produce actions, listen for responses, then publish the result. For organizations that prefer to roll-out their own collaboration interface, Fluvio’s programmable interface can serve as a single point of contact for the application to send and receive events.\nto give developers a framework for their first collaboration app.\nReal Time Streaming Platform A powerful use-case for a real-time data streaming platform is to connect services that typically operate independently, enabling cross-service capabilities. Fluvio offers language-native programmable interfaces that each service can utilize to send or receive messages, to and from real-time data streams.\nIn the dealership example there are multiple services that need to communicate with each other :\nAdvisors: coordinate customer communication. Services: look-up maintenance schedules and assign mechanics. Billing: charges customer credit cards and generates receipts. Fluvio handles multi-service communication through data streams called topics. When a service completes work, it generates an event that is published to a topic. Fluvio receives the event and notifies all other interested services. Consuming services receive notification, complete their work and generate an event on their own, and so on. Let’s look at an example:\nWhen a customer arrives at the dealership, advisory service looks-up the car and publishes an event on the advisory topic. Services receives the event, schedules mechanics to perform the work, and generates an event on the services topic. Billing receives the event, charges the customer credit card and sends an event with the amount paid on billing topic. Business Logic Services When organizations roll out their own real-time collaborative apps, they implicitly build services to implement the business logic.\nFluvio programmable interfaces are built for polyglot applications, where developers have the freedom to use their preferred programming languages to implement a service. Fluvio develops and maintains language native APIs for Rust, Node and Swift. Upcoming releases will roll-out additional native language APIs for Java, Python, Go, and common languages.\nTo sum up: Fluvio supports both purpose-built and custom collaboration tools, letting users of both choose how to receive and manage events. Fluvio’s streaming platform lets independent services communicate through data streams/topics. Fluvio supports the creation of business logic services in common programming languages. Ease of Use Fluvio ensures ease of use by making its core components easy and intuitive to utilize. Fluvio is an open-source distributed system with two core components: the cluster running streaming services and the clients that communicate with the cluster. The clusters are available in two flavors: Infinyon Cloud and Fluvio Open Source. They are managed by Fluvio clients through the CLI or programmatic APIs.\nBeyond the core components, Fluvio prioritizes operational efficiency through declarative configuration management and reconciliation. The declarative approach lets developers simply declare the goal of their configuration and then the configuration management tool will do the rest. Meanwhile reconciliation ensures any cluster changes are still in line with the stated goal of the system.\nClusters Infinyon Cloud Infinyon Cloud({\u0026lt;ref \u0026ldquo;/cloud\u0026rdquo;\u0026gt;}) eliminates the need to install and operate a cluster. The installation, upgrading, and all other maintenance tasks are managed by our team.\nFluvio Open Source Fluvio Open Source can be installed in private data centers, public clouds, or on personal machines. Fluvio CLI has built-in pre-check tests and installation scripts for most common environments. To install Fluvio, first download the Fluvio CLI from Github, and then run the installer.\nClients Fluvio CLI Fluvio CLI covers a broad range of operations: cluster installation, data stream provisioning, produce/consume operations, and more. Checkout the CLI Guide for a detailed list of commands.\nFluvio CLI has built-in multi-cluster support, where the CLI can switch from one environment to another with one command. This feature is particularly useful when testing client software against different data sets. Developers can build the code against a local data set, then switch to a cloud instance to run the code against production data\nProgrammatic APIs Fluvio has programmatic APIs and role-based access for all operations. The operations are divided in two groups: admin and user. Operations such as cluster installation and topic management are reserved for administrators, whereas producer/consumer are open to users.\nAPI interfaces are currently available in Node, Rust and Swift, while other languages will be implemented in upcoming releases. Check out Fluvio docs for the API references.\nOperational Efficiency Fluvio uses declarative configuration management to simplify manageability and improve operational efficiency. With declarative configuration, the operator specifies the intended outcome, rather than how to accomplish it. For example, a topic with 3 partitions is a desired outcome. Given this outcome, Fluvio automatically distributes the topics across the SPUs to achieve optimal performance and reliability.\nAnytime there are changes in the cluster, Fluvio uses reconciliation to ensure all outcomes can still be met. This allows the system to self-heal and recover from any conditions automatically without human intervention. For example an SPU loses power and it disconnects from the cluster in the middle of receiving a batch of records. A few minutes later, the SPU comes back online and the reconciliation loop kicks-in. During reconciliation, the SPU is brought back in sync with the cluster before it resumes its operations.\nTo sum up: Fluvio clusters can be either cloud-based and automatically managed by the Fluvio team, or open-source. Fluvio clients allow management of the clusters with either the Fluvio CLI or a programmatic API. Fluvio ensures operational efficiency with declarative configuration management and reconciliation, making cluster management both intuitive and reliable. Conclusion Modern business is happening in real-time. Customers demand immediate results, and business leaders require operational insight approaching clairvoyance. The transition from collaboration to real-time collaboration requires a foundation that can provide the data for the business context in real-time. This transition doesn’t need to be difficult.\nFluvio is the first purpose-built platform for real-time collaboration. With Fluvio, the focus is on business data. We bring context first, then apply collaboration features to enable real-time decisions. Our product is in early infancy and we look forward to working with you, the community, in providing us feedback as we iterate through the next wave of collaborative features.\nJoin us, create an account in our InfinyOn Cloud, check out our open source project in Github or connect with us in Discord.\nWe look forward to talking to you, see you soon.\n","description":"Fluvio, a real-time data streaming platform for connected apps","keywords":null,"summary":"As more of our work moves online, there is a growing need for collaboration apps where businesses communicate with customers and employees in real-time. These new collaboration apps will accelerate decisions, improve customer experience, and open new revenue opportunities.\nIn the article “Reimagining the post-pandemic organization\u0026quot;, McKinsey astutely observes that antiquated data systems are holding many companies back, forcing them to rely on manual processes to make decisions. Modern businesses must be able to collaborate with customers, partners, and employees in real-time, and do so through a variety of communication channels, organization’s boundaries, time zones, and legal jurisdictions.","title":"Why We Built Fluvio","url":"http://localhost:1315/blog/2021/12/why-we-built-fluvio/"},{"body":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.\n","description":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.","keywords":null,"summary":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.","title":"Real-time Data Streaming | Helsinki Bus Demo","url":"http://localhost:1315/resources/helsinki-bus-demo/"},{"body":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.\n","description":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.","keywords":null,"summary":"This demo shows the power of InfinyOn Cloud for real-time data streaming. We combine 2 real-time data sources from the Helsinki public transit system and predict arrival times for buses.","title":"Real-time Data Streaming | Helsinki Bus Demo","url":"http://localhost:1315/videos/helsinki-bus-demo/"},{"body":" Prev Next Page: / Download ","description":"Why Fluvio was built on Rust for blazingly fast and memory-efficient performance and security.","keywords":null,"summary":" Prev Next Page: / Download ","title":"Java vs. Rust Comparison","url":"http://localhost:1315/resources/java-vs-rust/"},{"body":"SANTA CLARA, CA \u0026ndash; With experts predicting that global supply chain disruptions will get worse before they get better, organizations are relying on their data teams to enable faster decision making and rapid response-to-market changes. Businesses need the insights to detect and react to challenges in real time so they can improve production planning, better manage inventory, and track the movement of goods more efficiently. They also need to ensure network reliability and seamless support for customers dealing with the frustrations of stock shortages and delivery delays.\nBut many organizations are struggling to meet the moment because they lack the infrastructure to keep pace with the demands of managing complex, ever-changing data.\nData ages quickly. Supply chain leaders across all industries are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Yet, most enterprises still rely on a message broker (or log aggregator) to ingest their data into databases, data lakes, or log managers, then perform batch processing to gain analytical insights. The database/batch/bolt-on processing approach is just too slow for many of the time critical decisions facing today’s organizations.\nA common process that causes supply chains to fall behind their data is “Periodic Lookup,” in which data is streamed to a data store as quickly as possible to be then queried for insights at recurring intervals. But how often should you query? Every minute? Every hour? Also, as data accumulates, how much time does it take for the query to return?\nBatch processing is far too slow for organizations that are striving to meet customer expectations for frictionless experiences. The attention span of your average customer is measured in low digit seconds, and even a minute of network downtime can be disastrous. Gartner analyst Andrew Lerner estimates that each minute of downtime costs organizations about $5,600. Time and delays processing queries grows exponentially as the volume of data increases.\nIn today’s data-driven organizations, where time-to-value is measured in milliseconds rather than hours, Continuous Delivery is the best approach. With Continuous Delivery, insights are harvested in real time, while being streamed from users and services in the network. Continuous Delivery enables faster business decisions and better brand experiences.\nA Forrester Research report recently mentioned in Forbes reveals that 90% of enterprise leaders understand the importance of real-time insights, and 84% think that being able to execute real-time course corrections is vital to the long-term success of their organizations. We see this with giants like Amazon, Netflix, Google, and Alibaba, which are well-known for maintaining their competitive advantage by reacting in real time.\nReal-time data opens the door for CDOs to drive innovation and turn data into valuable insights that result in actionable business decisions. But the vast majority of companies struggle to roll out real-time services. Why? The root cause is a lack of intelligent infrastructure. Real-time services and insights require a deep stack of intelligent tools and services that need significant time, skill, and cost to build, deploy and operate.\nOne solution is to implement a cloud-native continuous intelligence platform. This type of platform is scalable, easily integrates with existing systems and tools, and enables organizations to detect, react, and respond to meaningful events in milliseconds, rather than hours, days or weeks – and it doesn’t require an infrastructure overhaul! Data leaders can link their data sources, compose intelligent pipelines in minutes and dispatch actionable events to all relevant stakeholders.\nContinuous intelligence helps balance organizational data, deliver real-time event correlation and accelerate business actions by improving collaboration and efficiency across teams. It helps data leaders stay on top of market and competitor trends, and facilitates smarter, faster decisions by alerting stakeholders of unusual events and delays in the supply chain with real-time data to support reactions.\nWith a continuous intelligence platform, company performance metrics can be delivered in real time and customer churn can be predicted. AI chatbots can provide 24-7 customer support and push out relevant “make-good” offers to mitigate frustrations. Organizations can adjust supply chains in seconds to react to demand fluctuations; forecast logistics challenges; produce audit decisions; identify root cause; remediate faulty operations; and, ultimately, speed the way products and services move across the globe to consumers.\nNow is the time for organizations to lead with their data, and cloud-based, continuous intelligence makes it possible to take action.\nAbout InfinyOn InfinyOn, a real-time data streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. SmartModules enable enterprises to intelligently program their data pipelines as they flow between producers and consumers for real-time services. With InfinyOn Cloud, enterprises can quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\n","description":"Real-Time intelligence matters more than ever.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; With experts predicting that global supply chain disruptions will get worse before they get better, organizations are relying on their data teams to enable faster decision making and rapid response-to-market changes. Businesses need the insights to detect and react to challenges in real time so they can improve production planning, better manage inventory, and track the movement of goods more efficiently. They also need to ensure network reliability and seamless support for customers dealing with the frustrations of stock shortages and delivery delays.","title":"Using Real-Time Intelligence to Drive More Resilient Supply Chains","url":"http://localhost:1315/press-releases/real-time-intelligence-for-more-resilient-supply-chains/"},{"body":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time data streaming company, announced the launch of SmartModules™ on it’s open source Fluvio and InfinyOn Cloud platform. SmartModules give architects, developers and data engineers the ability to perform inline computations on the data passing through the platform. Smart Connectors are a new feature released with SmartModules that enable customers to write custom logic to filter out irrelevant data before it gets sent over the network and persisted in a topic, saving on bandwidth and storage.\n\u0026ldquo;Smart Connectors give enterprises the ability to filter and shape their data which can lead to significant cost reduction for real-time applications,” said Sehyo Chang, CTO of Infinyon. “When using a Smart Connector for data import and export, oftentimes operators are only interested in receiving a subset of the available data from a third-party application. This will be beneficial to enterprises because they can help save streaming costs and ensure data integrity is achieved.\u0026rdquo;\nA common use-case for filtering data with stream processing is when applications are streaming transactions that contain personal data such as credit card or bank account information. “Financial Services companies who are building event based architectures have the need to scale out and make it very easy for their employees to produce and consume data. A fully robust event based architecture with thousands of business events flowing through the system at any one time is the goal for many of our customers. With InfinyOn Cloud, companies can dive deeper into customer data, regardless of its format or type, while still protecting consumers’ privacy,” said A.J. Hunyady, CEO of InfinyOn.\nThe data shaping feature in our SmartModule also allows users to take the extracted data structures and transform them into an application-specific format.\nAbout InfinyOn InfinyOn, a real-time data streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. SmartModules enable enterprises to intelligently program their data pipelines as they flow between producers and consumers for real-time services. With InfinyOn Cloud, enterprises can quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\n","description":"New capabilities help enterprises to reduce streaming costs and ensure data integrity.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; InfinyOn, a real-time data streaming company, announced the launch of SmartModules™ on it’s open source Fluvio and InfinyOn Cloud platform. SmartModules give architects, developers and data engineers the ability to perform inline computations on the data passing through the platform. Smart Connectors are a new feature released with SmartModules that enable customers to write custom logic to filter out irrelevant data before it gets sent over the network and persisted in a topic, saving on bandwidth and storage.","title":"InfinyOn Launches SmartModules for Real-time Streaming","url":"http://localhost:1315/press-releases/smartmodules-real-time-streaming/"},{"body":"In a data-driven organization, one technique that\u0026rsquo;s used to promote data-in-motion and enable real-time reaction is called Event Sourcing, where the data in a system is modeled as a stream of actions, rather than a sitting store of \u0026ldquo;current state\u0026rdquo;. Event Sourcing allows us to reconstruct the system\u0026rsquo;s state at any point in time, but to do this, we need to keep track of all the events flowing through the system, which can amount to terabytes or petabytes in large organizations. Sometimes, however, we want to develop specialized microservices that just need access to a small subset of the organization\u0026rsquo;s data. In these cases, it doesn\u0026rsquo;t make sense to feed the massive firehose of events to the microservice - it might get overwhelmed, or incur large costs for moving bulk data that might not be needed!\nIn scenarios like this, we can use tools like Fluvio\u0026rsquo;s FilterMap to create a special-purpose stream for our microservice, capturing only those events that are relevant, and pre-processing them to be most easily consumed and used by the microservice. In this blog, we\u0026rsquo;re going to explore a miniature version of this use-case, where we create a focused stream of event messages needed to develop an SMS Notifications microservice for an Online Grocery application.\nYou can check out the full code in the fluvio-smartmodule-examples repository.\nScenario: SMS Notifications for Online Groceries Let\u0026rsquo;s say that our microservice is in charge of tracking the status of online grocery orders. Customers create accounts, put items into their cart, and checkout in the online store - and these events are all captured in a Fluvio topic as records. Suppose we would like to add a feature where customers can receive SMS notifications as their grocery order is being processed. We might want to provide notifications when the courier starts collecting the groceries, whether they discover any items are out of stock, or when the order is ready to pick up.\nIn this system, there would be multiple types of events flowing through the topic, but only some of them are relevant to SMS notifications. Suppose we had the following types of events:\ncreate_account: A customer created an account add_to_cart: The customer added an item to their cart add_billing: The customer added a credit card or payment method to their account checkout: The customer confirmed the items in their cart and checked out order_begun: The courier at the grocery store has started collecting groceries item_status: The courier physically collected an item, or discovered it was out of stock order_ready: The order has been collected and is ready for curbside pickup For the SMS Notifications system, we are only interested in the last three event types, order_begun, item_status, and order_ready - the other events happen while the user is navigating the site, so there is no need to notify them! When building our notification service, we would like to have a stream of just the events that are relevant to our subsystem, and formatted in a way that makes it easy to fill our use-case.\nUltimately, we\u0026rsquo;d like to be able to take an input stream of records like this (where each line is one record):\n{\u0026#34;type\u0026#34;:\u0026#34;account_created\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;bill9876\u0026#34;,\u0026#34;preferred_name\u0026#34;:\u0026#34;Bill\u0026#34;,\u0026#34;phone_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_to_cart\u0026#34;,\u0026#34;item_id\u0026#34;:\u0026#34;1001\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Milk\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_to_cart\u0026#34;,\u0026#34;item_id\u0026#34;:\u0026#34;1002\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Eggs\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_billing\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;card_holder\u0026#34;:\u0026#34;Bill Billardson\u0026#34;,\u0026#34;card_number\u0026#34;:\u0026#34;1234-5678-2468-1357\u0026#34;,\u0026#34;expiration\u0026#34;:\u0026#34;01/99\u0026#34;,\u0026#34;security_code\u0026#34;:\u0026#34;999\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;checkout\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;order_begun\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Billy\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;item_status\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Milk\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;Collected\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;item_status\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Eggs\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;Out of stock, refunded\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;order_ready\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} and turn it into a new stream that looks like this:\n{\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, your groceries are being collected!\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, we have an update on your Milk: Collected\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, we have an update on your Eggs: Out of stock, refunded\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, your groceries have been collected and are ready to pick up!\u0026#34;} Let\u0026rsquo;s look at how we can use a FilterMap to select our three event types from our input stream, and prepare the event data to be most easily consumed and used by our SMS notification system.\nCreate a new project We can use the amazing cargo-generate tool to help us get started quickly with a FilterMap template project. If you don\u0026rsquo;t already have it installed, you can get it with this command:\n%copy first-line%\n$ cargo install cargo-generate After you have cargo-generate installed, you can create a FilterMap project template using the following command:\n%copy first-line%\n$ cargo generate --git=https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : filter-map 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · filter-map [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `filter-map`... ✨ Done! New project created filter-map Let\u0026rsquo;s navigate into our project directory and take a look at the sample code we were given:\n%copy first-line%\n$ cd filter-map \u0026amp;\u0026amp; cat src/lib.rs We should see the following code:\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; #[smartmodule(filter_map)] pub fn filter_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Option\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { let key = record.key.clone(); let string = String::from_utf8_lossy(record.value.as_ref()).to_string(); let int: i32 = string.parse()?; if int % 2 == 0 { let output = int / 2; Ok(Some((key.clone(), RecordData::from(output.to_string())))) } else { Ok(None) } } This template code is one of the smallest possible FilterMaps. It takes each input record as an integer, then filters it out if it\u0026rsquo;s odd, or divides it in half if it\u0026rsquo;s even.\nThe important thing to notice about a FilterMap is that it returns an Option of a record. If we decide to return None, then the input record gets filtered out and will not appear in the output stream. If we return Some record, then the record we return will continue downstream.\nFor our purposes, we\u0026rsquo;ll want to define a data structure that represents the different types of events that appear in our stream. We can use the serde and serde_json crates to help us deserialize this data structure from JSON, then examine it to see whether we should keep it or not. If you\u0026rsquo;re following along with the template, you should already have serde and serde_json as dependencies, so let\u0026rsquo;s look at how to write the code we need.\nSince we\u0026rsquo;re talking about distinct event types, we can use a Rust enum to represent this data type. Below is the full code for the example. Look at the GroceryEvent enum that represents the input data, and the SmsMessage struct that represents the output data we generate.\nPaste the following code into src/lib.rs:\n%copy%\nuse serde::{Serialize, Deserialize}; use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; /// Events that may take place in an online grocery service #[derive(Debug, Serialize, Deserialize)] #[serde(tag = \u0026#34;type\u0026#34;, rename_all = \u0026#34;snake_case\u0026#34;)] enum GroceryEvent { AccountCreated { account_id: String, username: String, preferred_name: String, phone_number: String, }, AddToCart { item_id: String, item_name: String }, AddBilling { account_id: String, card_holder: String, card_number: String, expiration: String, security_code: String, }, Checkout { account_id: String }, OrderBegun { account_id: String, sms_number: String, sms_name: String, }, ItemStatus { account_id: String, item_name: String, status: String, sms_number: String, sms_name: String, }, OrderReady { account_id: String, sms_number: String, sms_name: String, }, } #[derive(Debug, Serialize, Deserialize)] struct SmsMessage { number: String, message: String, } #[smartmodule(filter_map)] fn filter_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Option\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { let event: GroceryEvent = match serde_json::from_slice(record.value.as_ref()) { Ok(event) =\u0026gt; event, Err(_) =\u0026gt; return Ok(None), // Skip if we fail to parse JSON }; let sms_message = match event { GroceryEvent::OrderBegun { sms_name, sms_number, .. } =\u0026gt; SmsMessage { number: sms_number, message: format!(\u0026#34;Hello {}, your groceries are being collected!\u0026#34;, sms_name), }, GroceryEvent::ItemStatus { sms_name, sms_number, item_name, status, .. } =\u0026gt; SmsMessage { number: sms_number, message: format!( \u0026#34;Hello {}, we have an update on your {}: {}\u0026#34;, sms_name, item_name, status ), }, GroceryEvent::OrderReady { sms_name, sms_number, .. } =\u0026gt; SmsMessage { number: sms_number, message: format!( \u0026#34;Hello {}, your groceries have been collected and are ready to pick up!\u0026#34;, sms_name ), }, _ =\u0026gt; return Ok(None), }; let message_json = serde_json::to_string(\u0026amp;sms_message)?; Ok(Some((record.key.clone(), message_json.into()))) } Let\u0026rsquo;s quickly look at what\u0026rsquo;s happening with our data structures:\nSince we\u0026rsquo;re working with different event types, each enum variant represents one event type and its data We\u0026rsquo;re using #[serde(tag = \u0026quot;type\u0026quot;)] to add a \u0026ldquo;type\u0026rdquo; field to each event with the name of the variant We\u0026rsquo;re using #[serde(rename_all = \u0026quot;snake_case\u0026quot;)] to rename the variants from e.g. AccountCreated to account_created Now, let\u0026rsquo;s look at what\u0026rsquo;s going on inside the filter_map function itself:\nFirst, we read the input as a JSON GroceryEvent called event Then, we use Rust\u0026rsquo;s match statement to choose what to do based on the type of event We have cases for OrderBegun, ItemStatus, and OrderReady, which are the events we are interested in For all other events, we return Ok(None), which filters them out of the stream In each match case, we transform the input event into an SmsMessage, which we use as our output Finally, we serialize our SmsMessage into JSON and return it Let\u0026rsquo;s get set up on Fluvio and see our new FilterMap in action!\nTesting the FilterMap on Fluvio In order to follow along, make sure you have Fluvio installed and are up and running with a Fluvio cluster. The first thing we\u0026rsquo;ll need to do is to create a new Fluvio topic for us to stream our events.\n%copy first-line%\n$ fluvio topic create groceries Next, we\u0026rsquo;ll want to produce some sample records to this topic, these will act as the input to our FilterMap. Create a new file called groceries.txt with the following contents:\n%copy%\n{\u0026#34;type\u0026#34;:\u0026#34;account_created\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;bill9876\u0026#34;,\u0026#34;preferred_name\u0026#34;:\u0026#34;Bill\u0026#34;,\u0026#34;phone_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_to_cart\u0026#34;,\u0026#34;item_id\u0026#34;:\u0026#34;1001\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Milk\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_to_cart\u0026#34;,\u0026#34;item_id\u0026#34;:\u0026#34;1002\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Eggs\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;add_billing\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;card_holder\u0026#34;:\u0026#34;Bill Billardson\u0026#34;,\u0026#34;card_number\u0026#34;:\u0026#34;1234-5678-2468-1357\u0026#34;,\u0026#34;expiration\u0026#34;:\u0026#34;01/99\u0026#34;,\u0026#34;security_code\u0026#34;:\u0026#34;999\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;checkout\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;order_begun\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Billy\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;item_status\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Milk\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;Collected\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;item_status\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;item_name\u0026#34;:\u0026#34;Eggs\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;Out of stock, refunded\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} {\u0026#34;type\u0026#34;:\u0026#34;order_ready\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;sms_number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;sms_name\u0026#34;:\u0026#34;Bill\u0026#34;} Then, produce these events line-by-line to Fluvio using the following command:\n%copy first-line%\n$ fluvio produce groceries -f ./groceries.txt At this point, we\u0026rsquo;re ready to get to work with FilterMap.\nIf you\u0026rsquo;ve never compiled for WASM before, you\u0026rsquo;ll need to install the proper rustup target. You should only need to do this once.\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown Let\u0026rsquo;s go ahead and compile it, using --release mode to get the smallest WASM binary possible:\n%copy first-line%\n$ cargo build --release Now, we can use our FilterMap while we consume records from our topic using the following command:\n%copy first-line%\n$ fluvio consume groceries -B --filter-map=./target/wasm32-unknown-unknown/release/filter_map.wasm Consuming records from the beginning of topic \u0026#39;groceries\u0026#39; {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, your groceries are being collected!\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, we have an update on your Milk: Collected\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, we have an update on your Eggs: Out of stock, refunded\u0026#34;} {\u0026#34;number\u0026#34;:\u0026#34;1-800-234-5678\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Hello Bill, your groceries have been collected and are ready to pick up!\u0026#34;} We can see that the output stream only contains the three event types we care about, and that they have been formatted to be quickly and easily used by our SMS notification system. Overall, using FilterMap in this scenario has provided us with several advantages:\nEvents containing sensitive or irrelevant information are being filtered in advance This can help to avoid accidental disclosure by the SMS system, e.g. by a bug - if it never has access to sensitive information, it can\u0026rsquo;t leak it! Events are preprocessed to be minimal and directly useful to our downstream application. A simple SMS notification system just needs the number and message to send. Conclusion That\u0026rsquo;s it for this post, be sure to join our Discord server if you want to talk to us or have any questions. Until next time!\nFurther reading The InfinyOn Continuous Intelligence Platform Aggregate streaming data in real-time with WebAssembly ","description":"How to narrow-in on an event stream to build a focused real-time application.","keywords":null,"summary":"In a data-driven organization, one technique that\u0026rsquo;s used to promote data-in-motion and enable real-time reaction is called Event Sourcing, where the data in a system is modeled as a stream of actions, rather than a sitting store of \u0026ldquo;current state\u0026rdquo;. Event Sourcing allows us to reconstruct the system\u0026rsquo;s state at any point in time, but to do this, we need to keep track of all the events flowing through the system, which can amount to terabytes or petabytes in large organizations.","title":"Using Fluvio FilterMap to apply focus to real-time data","url":"http://localhost:1315/blog/2021/11/filter-map/"},{"body":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. In our latest release, we introduced SmartModules ArrayMap, a new kind of programmable API that allows you to break apart large records into smaller records. The key thing to know about the ArrayMap pattern is that it converts one input into zero or many outputs.\nOne of the primary motivations for ArrayMap is to control the granularity of your data stream. Sometimes, your stream’s records represent more than one data point, and you need to manipulate just a sub-piece of that record. ArrayMap takes each composite data record and breaks the large object into smaller pieces that can be worked on or analyzed individually. For example, you may want to analyze addresses in customer records, items purchased in a transaction, withdrawals in a bank account, etc.\nIn this post, I will show a practical use-case for SmartModules ArrayMap: breaking apart paginated API responses into a stream of content. As an example, I\u0026rsquo;ll be using real data from Reddit\u0026rsquo;s API, so feel free to follow along! If you decide to do so, you\u0026rsquo;ll need to get set up with Fluvio either via a free InfinyOn Cloud account or by setting up your own open-source Fluvio cluster. In addition, here\u0026rsquo;s a list of tools that we\u0026rsquo;ll be using throughout the blog:\nThe Fluvio CLI Rust, Cargo, and Rustup jq curl (installed by default on some systems) cargo-generate You can also check out the finished code on GitHub!\nGetting some data The first thing we need to do is get some sample data so we know what we\u0026rsquo;re working with. We can use the following curl command to do just that:\n%copy first-line%\n$ curl -H \u0026#34;User-agent:ExampleBot\u0026#34; \u0026#34;https://www.reddit.com/r/rust.json?count=10\u0026#34; | tee reddit.json -\u0026gt; Note: When writing a real application, you\u0026rsquo;ll want to choose your own special User-agent\nThis command will print a big body of JSON to the screen and also save it in a file called reddit.json, so we can use it later. Let\u0026rsquo;s pretty-print it now so we can take a good look at it. Make sure you have jq installed, then run:\n%copy first-line%\n$ jq \u0026lt; reddit.json { \u0026#34;kind\u0026#34;: \u0026#34;Listing\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;after\u0026#34;: \u0026#34;t3_qc1h1j\u0026#34;, \u0026#34;dist\u0026#34;: 27, \u0026#34;modhash\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;geo_filter\u0026#34;: null, \u0026#34;children\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;selftext\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;ups\u0026#34;: x, \u0026#34;upvote_ratio\u0026#34;: x.yz ... many more fields }, ... }, { ... }, { ... }, ], \u0026#34;before\u0026#34;: \u0026#34;t3_qahjqp\u0026#34; } } I\u0026rsquo;ve collapsed most of the fields in the children objects, there is a lot of information in there. The important thing to see is that the children field contains the \u0026ldquo;page\u0026rdquo; of 10 requests that we asked for in the request (recall the ?count=10 parameter).\nWhat we want to do is create a Fluvio Topic that contains requests like this as its Records, then create a SmartModules ArrayMap to create an output stream of the individual child elements (the posts themselves). When we\u0026rsquo;re done, each element in our stream should look like this:\n{ \u0026#34;id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;selftext\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;ups\u0026#34;: x, \u0026#34;upvote_ratio\u0026#34;: x.yz } Writing a SmartModules ArrayMap To get started with our ArrayMap, we can use cargo-generate to get up and running with a template of a ArrayMap SmartModule. If you don\u0026rsquo;t already have it, install cargo-generate with the following command:\n%copy first-line%\n$ cargo install cargo-generate Then, use it to create a new project from the ArrayMap template:\n%copy first-line%\n$ cargo generate --git=\u0026#34;https://github.com/infinyon/fluvio-smartmodule-template\u0026#34; ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : reddit-array-map 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · array-map [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `reddit-array-map`... ✨ Done! New project created reddit-array-map Let\u0026rsquo;s move our reddit.json into the project directory and change our directory so we can work with the project.\n%copy first-line%\n$ mv ./reddit.json ./reddit-array-map/ %copy first-line%\n$ cd reddit-array-map While we\u0026rsquo;re setting up, make sure you have the wasm32-unknown-unknown target installed by running this rustup command:\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown Now let\u0026rsquo;s take a look at the code that was generated for us:\n%copy first-line%\n$ cat src/lib.rs use fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; #[smartmodule(array_map)] pub fn array_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { // Deserialize a JSON array with any kind of values inside let array = serde_json::from_slice::\u0026lt;Vec\u0026lt;serde_json::Value\u0026gt;\u0026gt;(record.value.as_ref())?; // Convert each JSON value from the array back into a JSON string let strings: Vec\u0026lt;String\u0026gt; = array .into_iter() .map(|value| serde_json::to_string(\u0026amp;value)) .collect::\u0026lt;core::result::Result\u0026lt;_, _\u0026gt;\u0026gt;()?; // Create one record from each JSON string to send let records: Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; = strings .into_iter() .map(|s| (None, RecordData::from(s))) .collect(); Ok(records) } The starter code for SmartModules ArrayMap is built to take JSON arrays as input, and return a stream of the contents of those arrays as output. This is pretty similar to what we want to accomplish with the Reddit posts, but the array we want to retrieve is nested inside a larger response object.\nTo help us work with Reddit\u0026rsquo;s response data, let\u0026rsquo;s write some structs that mirror the JSON response structure and derive serde\u0026rsquo;s Serialize and Deserialize traits for them. If you\u0026rsquo;re following along and used the cargo-generate template, then serde and serde_json are already in your project dependencies.\nNow let\u0026rsquo;s add our structs and update our function. We need to make sure the field names correspond exactly to the fields in the JSON response. Any fields that we don\u0026rsquo;t name will simply be ignored. You can overwrite the existing code in src/lib.rs file with the following:\n%copy%\nuse fluvio_smartmodule::{smartmodule, Record, RecordData, Result}; use serde::{Serialize, Deserialize}; #[derive(Debug, Serialize, Deserialize)] struct RedditListing { data: RedditPage, } #[derive(Debug, Serialize, Deserialize)] struct RedditPage { children: Vec\u0026lt;RedditPost\u0026gt;, } #[derive(Debug, Serialize, Deserialize)] struct RedditPost { data: RedditPostData, } #[derive(Debug, Serialize, Deserialize)] struct RedditPostData { id: String, title: String, url: String, selftext: String, ups: i32, upvote_ratio: f32, } #[smartmodule(array_map)] pub fn array_map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;Vec\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt;\u0026gt; { // Step 1: Deserialize a RedditListing from JSON let listing = serde_json::from_slice::\u0026lt;RedditListing\u0026gt;(record.value.as_ref())?; // Step 2: Create a list of RedditPostData converted back into JSON strings let posts: Vec\u0026lt;(String, String)\u0026gt; = listing .data .children .into_iter() .map(|post: RedditPost| { // Convert each post into (ID, Post JSON) serde_json::to_string(\u0026amp;post.data).map(|json| (post.data.id, json)) }) .collect::\u0026lt;core::result::Result\u0026lt;_, _\u0026gt;\u0026gt;()?; // Step 3: Convert each Post into a Record whose key is the Post\u0026#39;s ID let records = posts .into_iter() .map(|(id, post)| (Some(RecordData::from(id)), RecordData::from(post))) .collect(); Ok(records) } Looking at the structure of the data this way, we can see that we will eventually want to turn the list of RedditPostData into a list of Records that we can return for our output stream. Let\u0026rsquo;s dive in and take at the code to make that happen.\nThis ArrayMap function essentially has 3 steps that it takes. They are:\nDeserialize the original reddit request Extract the list of posts as a list of (Post ID, Post JSON) Convert that list into a list of Key/Value Records Running the SmartModules ArrayMap Let\u0026rsquo;s take this for a test drive and see if it works as expected! For the following steps you\u0026rsquo;ll need to make sure you have Fluvio downloaded and a cluster running.\nFirst, let\u0026rsquo;s compile our ArrayMap. We\u0026rsquo;ll compile using release mode to make the WASM module as small as possible.\n%copy first-line%\n$ cargo build --release Then, let\u0026rsquo;s create a Fluvio Topic where we\u0026rsquo;ll send our Reddit API responses. Later, we\u0026rsquo;ll consume from this Topic using the ArrayMap we just wrote.\n%copy first-line%\n$ fluvio topic create reddit Now, let\u0026rsquo;s produce our Reddit data into the topic. If you still have the reddit.json file, you can send it using this command:\n%copy first-line%\n$ fluvio produce reddit -f ./reddit.json Or, if you want to be fancy and produce fresh data directly from Reddit, you can pipe your data straight from curl:\n%copy first-line%\n$ curl -H \u0026#34;User-agent:ExampleBot\u0026#34; \u0026#34;https://www.reddit.com/r/rust.json?count=10\u0026#34; | fluvio produce reddit Now, let\u0026rsquo;s apply our SmartModules ArrayMap and see how we did!\n%copy first-line%\n$ fluvio consume reddit --tail --key-value --array-map=target/wasm32-unknown-unknown/release/reddit_array_map.wasm ... [qccbjz] {\u0026#34;id\u0026#34;:\u0026#34;qccbjz\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Can you compile in parallel?\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https://www.reddit.com/r/rust/comments/qccbjz/can_you_compile_in_parallel/\u0026#34;,\u0026#34;selftext\u0026#34;:\u0026#34;Often I\u0026#39;ll have a project that automatically pulls in 150+ dependencies that might not depend on each other.\\n\\nIt\u0026#39;d be great if I could somehow compile 8 of them in parallel instead of only one, but I\u0026#39;m not sure how to make that happen.\\n\\nIt would make sense that this should work, given that as long as you aren\u0026#39;t compiling something before its dependencies are finished, there shouldn\u0026#39;t be any conflicts. But looking online I haven\u0026#39;t found anything.\u0026#34;,\u0026#34;ups\u0026#34;:7,\u0026#34;upvote_ratio\u0026#34;:0.71} [qc3wc2] {\u0026#34;id\u0026#34;:\u0026#34;qc3wc2\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;sqlx: Exist a problem if pick \\\u0026#34;runtime-actix\\\u0026#34; vs \\\u0026#34;runtime-tokio\\\u0026#34;?\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https://www.reddit.com/r/rust/comments/qc3wc2/sqlx_exist_a_problem_if_pick_runtimeactix_vs/\u0026#34;,\u0026#34;selftext\u0026#34;:\u0026#34;Is unclear to me why in sqlx exist this 2 runtimes, if actix is based on tokio.\\n\\nI\u0026#39;m building a utility crate that could later interact with actix or not (depending on the project), so I don\u0026#39;t know if is best to depend from the start on actix even if later chose one with rocket, for example. \\n\\nThe most logical option is to use tokio, but if later put actix, it will conflict??\u0026#34;,\u0026#34;ups\u0026#34;:13,\u0026#34;upvote_ratio\u0026#34;:0.72} [qbngmu] {\u0026#34;id\u0026#34;:\u0026#34;qbngmu\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Announcing Lingua 1.3 - The most accurate natural language detection library for Rust\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https://www.reddit.com/r/rust/comments/qbngmu/announcing_lingua_13_the_most_accurate_natural/\u0026#34;,\u0026#34;selftext\u0026#34;:\u0026#34;Hi, folks!\\n\\nI\u0026#39;ve just released the new major version 1.3 of Lingua, the most accurate natural language detection library for Rust. It is especially well-suited for the classification of short text where other language detectors have problems.\\n\\n[https://github.com/pemistahl/lingua-rs](https://github.com/pemistahl/lingua-rs)\\n\\nThis release introduces each of the so far 75 supported languages as separate Cargo features. If you don\u0026#39;t want to download all statistical language models, you are now able to specify which languages you want to download and detect with the library. This results in much smaller binaries.\\n\\nI hope you find the library useful. I\u0026#39;m looking forward to your feedback. Thanks. :)\\n\\n\u0026amp;amp;#x200B;\\n\\nhttps://preview.redd.it/l2gwwejbohu71.png?width=960\u0026amp;amp;format=png\u0026amp;amp;auto=webp\u0026amp;amp;s=d46eeeabe6cbd72584c20d43a14f49150004b1e4\u0026#34;,\u0026#34;ups\u0026#34;:231,\u0026#34;upvote_ratio\u0026#34;:0.98} Great! We can see that each of our Records represents a Post, and that they contain just the fields that we selected for. Since we used the --key-value option, we can also see that we correctly extracted the id field and applied it to be used as our record Keys.\nConclusion That\u0026rsquo;s it for this post, be sure to join the discussion on Reddit or hop into our Discord server if you want to talk to us or have any questions. Until next time!\nUpdate: SmartModules were originally called SmartStreams. The blog was updated to reflect the new naming convention\nFurther reading About Fluvio: the Programmable Streaming Platform Fluvio SmartModules: Writing a WASM-based filter for application logs ","description":"How to convert a paginated API into a streaming API using Fluvio's WASM ArrayMap.","keywords":null,"summary":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. In our latest release, we introduced SmartModules ArrayMap, a new kind of programmable API that allows you to break apart large records into smaller records. The key thing to know about the ArrayMap pattern is that it converts one input into zero or many outputs.\nOne of the primary motivations for ArrayMap is to control the granularity of your data stream.","title":"Streaming the Reddit API using Fluvio's WASM ArrayMap","url":"http://localhost:1315/blog/2021/10/smartmodule-array-map-reddit/"},{"body":"A recent Forrester report published in Forbes reveals that 90% of enterprise leaders understand the importance of real-time insights and 84% think that being able to execute real-time course corrections is vital to the long-term success of their organizations. This opens the door for CDOs and data leaders to drive innovation and play an even more critical role across e-staff. They must turn data into key insights and insights into value that result in actionable business decisions. But do data teams have a platform that keeps pace with the internal time and cost demands of managing more complex, ever-changing data? Can they continue helping their company grow and maintain brand loyalty by delivering real-time insights?\nStop Falling Behind your Data Data ages quickly. Industry leaders across many verticals are striving for real-time services to improve operations, delight customers, and gain a competitive edge. Yet, most enterprises still rely on a message broker (or log aggregator) to ingest their data into databases, data lakes, or log managers, then perform batch processing to gain analytical insights. The batch processing approach is just too slow for many of the time critical decisions facing today’s organizations.\nPeriodic Lookup vs. Continuous Delivery “Periodic Lookup” is where data is streamed to a data store as quickly as possible, to be then queried for insights at recurring intervals. But how often should you query: every minute, every hour? Also, as data accumulates, how much time does it take for the query to return? Batch processing is simply too slow when customer attention span is measured in low digit seconds. Andrew Lerner of Gartner cites the average downtime cost of $5,600 p/minute. Time and delays processing queries grows exponentially as the volumes of data increases.\nIn today’s data-driven organizations, time-to-value is measured in milliseconds rather than hours. “Continuous Delivery” is when data insights are harvested in real time, while being streamed from users and services in the network. Continuous Delivery enables faster decision making, rapid response-to- market changes, personalized customer marketing and better customer service. Real-Time Matters.\nA Better Way Giants like Amazon, Netflix, Google and Alibaba maintain competitive advantages reacting in real time. So why do 97% of companies struggle to roll out real-time services? The root cause is a lack of intelligent infrastructure. Real-time services and insights require a deep stack of intelligent tools and services that need significant time, skill, and cost to build, deploy and operate.\nThat's the problem we have solved at InfinyOn. We\u0026rsquo;ve developed a programmable and scalable Cloud-native Continuous Intelligence Platform that processes data in real time. InfinyOn helps Enterprises detect, react, and respond to meaningful events in milliseconds vs. hours, days and weeks while integrating with your existing systems and tools. No infrastructure overhaul required! Just add your data sources, compose your intelligent pipelines in minutes and dispatch actionable events to all relevant stakeholders.\nBalance Data - Improve collaboration and efficiency across teams, deliver real-time event correlation and accelerate business actions.\nManage Change - Detect market trends and changes by your competitors and react in real time.\nMake Smart Choices - Accelerate decisions by alerting stakeholders of unusual events and provide real-time data supporting your actions.\nAdd More Value - Produce audit decisions, identify root cause, remediate faulty operations and generate reports.\nLead with Your Data Continuous real-time intelligence has arrived – where company performance metrics are delivered in real time, AI chatbots provide customer support 24-7 and push out special offers based on your interests, customer churn is predicted, financial transactions get automatically processed, supply chains adjust in seconds to demand fluctuations, predictive maintenance for manufacturing, IOT, Smart Cities and more.\nDon’t wait to take action. Partner with InfinyOn today. Together, let’s start accelerating your speed of business by leading your data.\n","description":"A modern data streaming platform that extracts relevant events, computes interesting insights, and informs interested parties in real-time.","keywords":null,"summary":"A recent Forrester report published in Forbes reveals that 90% of enterprise leaders understand the importance of real-time insights and 84% think that being able to execute real-time course corrections is vital to the long-term success of their organizations. This opens the door for CDOs and data leaders to drive innovation and play an even more critical role across e-staff. They must turn data into key insights and insights into value that result in actionable business decisions.","title":"InfinyOn Continuous Intelligence Platform","url":"http://localhost:1315/blog/2021/10/infinyon-continuous-intelligence/"},{"body":"Raspberry Pi is a versatile low-cost computer ideal for prototyping IoT projects such as air quality monitors, water level sensors, autonomous irrigation systems, surveillance cameras, face recognition robots, home automation, and many more. The Raspberry Pi is easy to connect to your home network but quite challenging to connect and control from the Cloud.\nThis tutorial will show how to connect Raspberry Pi to InfinyOn Cloud - a high-performance, distributed, programmable streaming platform. Once connected, you can use InfinyOn Cloud or a Fluvio client anywhere in the world to communicate with your device. In addition, this blog will show the ability to control a mini traffic light in real-time.\n-\u0026gt; This blog is the first of the IoT series. Future blogs will describe how to build an IoT aggregator for many devices using InfinyOn Cloud.\nAll message exchanges between your Raspberry Pi and the Cloud are encrypted to ensure privacy and security.\nPrerequisites This blog assumes you have an InfinyOn Cloud account and access to a Raspberry Pi device and a Pi traffic light. As a substitute for the traffic light, you could wire up an LED on the breadboard.\nInfinyOn Cloud Account - create a free account here. Raspberry Pi - this tutorial uses Model B+ (other models not tested but expected to work). The device must run Ubuntu 20.04 TLS (32 bit) OS or above. Pi Traffic Light - available for purchase here. Check Raspberry Pi The Raspberry Pi should be running Ubuntu, python3 and pip3. To check run the following commands:\n$ uname -a Linux ubuntu 5.8.0-1011-raspi #14-Ubuntu SMP PREEMPT Tue Dec 15 08:58:13 UTC 2020 armv7l armv7l armv7l GNU/Linux $ python3 -V Python 3.8.10 $ pip3 -V pip 21.2.4 from /home/ubuntu/.local/lib/python3.8/site-packages/pip (python 3.8) We are using python to control the GPIO pins that turns the lights on and off.\nConnect the Traffic Lights Now it\u0026rsquo;s time to work on the hardware. First, we\u0026rsquo;ll connect the traffic lights to the Raspberry Pi and ensure they are in working order.\nThe nice thing about the traffic lights component is that we can directly insert it into the Raspberry Pi pins.\nJust make sure the lights face outwards.\nTest the Lights As the traffic lights are controlled through GPIO pins, we need to install a package called gpiozero:\n%copy first-line%\n$ sudo apt install python3-gpiozero It\u0026rsquo;s time to write some code. Create a file called lights.py and paste the following code:\n%copy%\nfrom gpiozero import LED from time import sleep class Lights: def __init__(self): self.red = LED(9) self.yellow = LED(10) self.green = LED(11) def go(self): self.off() self.green.on() def stop(self): self.off() self.yellow.on() sleep(1) self.yellow.off() self.red.on() def on(self): self.red.on() self.yellow.on() self.green.on() def off(self): self.red.off() self.yellow.off() self.green.off() def main(): lights = Lights() lights.go() sleep(1) lights.stop() sleep(1) for x in range(5): sleep(.5) lights.on() sleep(.5) lights.off() if __name__ == \u0026#34;__main__\u0026#34;: main() The code has several routines to control the GPIO pins corresponding to red, yellow, and green— each routine is followed by a sleep statement that keeps the lights on for a short time.\nTo execute the code, run:\n%copy first-line%\n$ python3 lights.py Let\u0026rsquo;s see it in action:\nVery cool, the lights are working corretly.\nInstalling Fluvio Client You\u0026rsquo;ll need to download and install the CLI.\n%copy first-line%\n$ curl -fsS https://hub.infinyon.cloud/install/install.sh | bash This command will download the Fluvio Version Manager (fvm), Fluvio CLI (fluvio) and config files into $HOME/.fluvio, with the executables in $HOME/.fluvio/bin. To complete the installation, you will need to add the executables to your shell $PATH.\nCheck the version to make sure everything is working correctly:\n%copy first-line%\n$ fluvio version As expected, the client is not yet associated with a cluster. We\u0026rsquo;ll do that next.\nSetup InfinyOn Cloud InfinyOn Cloud is the intermediation point between your Raspberry Pi and any other devices (cloud or local). Let\u0026rsquo;s create an account and login with Fluvio client.\nCreate a free InfinyOn Cloud Signup at https://infinyon.cloud/signup.\nLogin with Fluvio Client In Raspberry Pi, at the terminal, type the following command:\n%copy first-line%\n$ fluvio cloud login Infinyon Cloud email: ... Password: ... Fluvio Client establishes a connection with InfinyOn Cloud and downloads your profile. The profile has your TLS settings to ensure message exchanges are encrypted.\nLet\u0026rsquo;s make sure the client can communicate with the cluster in the cloud. Add a topic called lights:\n%copy first-line%\n$ fluvio topic create lights topic \u0026#34;lights\u0026#34; created Congratulations, your Raspberry Pi is now connected to the cloud.\nInstall fluvio-python-client We want to control our light programmatically from our Python code. To accomplish this, we need to install the python native fluvio client.\nOn Raspberry Pi 2/3, download linux_armv7l:\n%copy first-line%\n$ pip3 install https://github.com/infinyon/fluvio-client-python/releases/download/v0.9.5/fluvio-0.9.5-cp38-cp38-linux_armv7l.whl For Pi Zero, download linux_armv6l:\n%copy first-line%\n$ pip3 install https://github.com/infinyon/fluvio-client-python/releases/download/v0.9.5/fluvio-0.9.5-cp38-cp38-linux_armv6l.whl Update lights.py file Next, we\u0026rsquo;ll update the python file from to listen to events from fluvio:\n%copy%\nfrom gpiozero import LED from time import sleep from fluvio import (Fluvio, Offset) class Lights: def __init__(self): self.red = LED(9) self.yellow = LED(10) self.green = LED(11) def go(self): self.off() self.green.on() def stop(self): self.off() self.yellow.on() sleep(1) self.yellow.off() self.red.on() def on(self): self.red.on() self.yellow.on() self.green.on() def off(self): self.red.off() self.yellow.off() self.green.off() def main(): lights = Lights() fluvio = Fluvio.connect() print(\u0026#34;connected to fluvio\u0026#34;) consumer = fluvio.partition_consumer(\u0026#34;lights\u0026#34;, 0) print(\u0026#34;retrieved consumer\u0026#34;) for i in consumer.stream(Offset.end()): value = i.value_string() print(\u0026#34;received: %s\u0026#34; % value) if value == \u0026#34;go\u0026#34;: lights.go() elif value == \u0026#34;stop\u0026#34;: lights.stop() elif value == \u0026#34;on\u0026#34;: lights.on() elif value == \u0026#34;off\u0026#34;: lights.off() if __name__ == \u0026#34;__main__\u0026#34;: main() The code changes are as follows:\nimport Fluvio and Offset from the fluvio python client. extablish a connection to InfinyOn Cloud. open a consumer stream that listens on lights topic on partition 0. for every known command go, stop, on, off, call the light routine. Let\u0026rsquo;s restart the program.\n-\u0026gt; Note: Use \u0026lt;CTRL\u0026gt;-\\ to break out of the program as \u0026lt;CTRL\u0026gt;-C is not yet implemented\nRun the new code:\n%copy first-line%\n$ python3 lights.py connected to fluvio retrieved consumer Control Raspberry Pi from InfinyOn Cloud Open the web browser at https://infinyon.cloud, and navigate to lights topic. In the record field, type go, stop and watch the lights change:\nCongratulations, your Raspberry Pi is connected to the Cloud!\nControl Raspberry Pi from Fluvio CLI You can also use fluvio cli to remotely contol the device.\n-\u0026gt; Note: Make sure that your cli is connected to the cloud.\nIn the terminal type:\n%copy first-line%\n$ fluvio produce lights \u0026gt; go Ok! \u0026gt; stop Ok! \u0026gt; on Ok! \u0026gt; off Ok! \u0026gt; Fluvio supports any number of producers and consumers, which enables your Raspberry to be control with multiple producers at the same time.\nConclusion The blog is an introductory tutorial on how to connect and manage Raspberry Pi from the Cloud. Fluvio and InfinyOn Cloud has many more feature that can help you manage multiple topics and edge devices in parallel. Future blogs will describe how to use SmartModules to perform analytics and control independent devices in a large deployment.\nThat\u0026rsquo;s it for this post, be sure to join or community on Discord if you want to talk to us or have any questions. Until next time!\nFurther reading About Fluvio: the Programmable Streaming Platform Fluvio SmartModules: Writing a WASM-based filter for application logs ","description":"A step-by-step tutorial that connects Raspberry Pi to the Cloud and turns lights on/off in real-time.","keywords":null,"summary":"Raspberry Pi is a versatile low-cost computer ideal for prototyping IoT projects such as air quality monitors, water level sensors, autonomous irrigation systems, surveillance cameras, face recognition robots, home automation, and many more. The Raspberry Pi is easy to connect to your home network but quite challenging to connect and control from the Cloud.\nThis tutorial will show how to connect Raspberry Pi to InfinyOn Cloud - a high-performance, distributed, programmable streaming platform.","title":"How to control Raspberry Pi from the Cloud","url":"http://localhost:1315/blog/2021/09/raspberry-pi-cloud-lights/"},{"body":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. We\u0026rsquo;re making steady progress adding new inline data processing capabilities, building on our SmartModules feature that allows users to write custom code to interact with their streaming data. SmartModules are written in Rust, compiled to WebAssembly, and executed on Fluvio\u0026rsquo;s Streaming Processing Units to manipulate data inline.\nThis week we\u0026rsquo;re happy to announce Aggregations for SmartModules! Aggregates let you define functions that combine each record in a stream with some long-running state, or \u0026ldquo;accumulator\u0026rdquo;. Depending on the data that you\u0026rsquo;re working with, an accumulator can be something as simple as a summed number, or some structured data like a table of aggregated data points. In this blog, I\u0026rsquo;m going to introduce three examples of aggregates: summing a stream of integers, calculating an incremental average, and finally, tracking multiple sums with a key-value accumulator.\nYou can find the full code for the examples covered in the blog in the fluvio-smartmodule-examples repository.\nAggregation concepts To kick things off, I want to give some visual insight into what\u0026rsquo;s happening when we talk about Aggregations. Like I mentioned above, an Aggregate is essentially a function that takes an \u0026ldquo;accumulated\u0026rdquo; value and combines it with a new input value.\nWhen we write our SmartModules Aggregate function in Rust, the inputs and outputs directly correlate with this conceptual model of an aggregation:\nWhen we apply an aggregation function to a stream of input values, we get a stream of output values that represent the values of the accumulator over time. In this visual, the green nodes represent each invocation of our aggregation function, the blue nodes are values from our input stream, and the purple nodes are values in our output stream. We also have an \u0026ldquo;initial\u0026rdquo; accumulator value (shown in the white dotted box), which is not included in the output stream but which is used as the first accumulator input to the aggregate function.\nAlright, with that background out of the way, let\u0026rsquo;s take a look at some code examples!\nFollow along: Use the SmartModule template If you\u0026rsquo;d like to follow along with the code samples, feel free to use the cargo-generate template to get a project up and running quickly. If you don\u0026rsquo;t have cargo-generate installed, you can install it with:\n%copy first-line%\n$ cargo install cargo-generate Then, use this command to apply the template and create a new project folder. Be sure to use the \u0026ldquo;aggregate\u0026rdquo; type.\n%copy first-line%\n$ cargo generate --git https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : aggregate-blog-sum 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · aggregate [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `aggregate-blog-sum`... ✨ Done! New project created aggregate-blog-sum This leads us right into our first example, since the aggregate template code shows off how to sum integers!\nExample #1: Sum integers in a stream Our first example is one of the simplest possible SmartModule Aggregate functions, which just takes an accumulated integer value and adds each new record to it.\nThe code generator created a sample file for us. Let\u0026rsquo;s navigate into our project directory and take a look at the sample code we were given:\n%copy first-line%\n$ cd aggregate-blog-sum \u0026amp;\u0026amp; cat src/lib.rs We should see the following SmartModule code:\nuse fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; #[smartmodule(aggregate)] pub fn aggregate(accumulator: RecordData, current: \u0026amp;Record) -\u0026gt; Result\u0026lt;RecordData\u0026gt; { // Parse the accumulator and current record as strings let accumulator_string = std::str::from_utf8(accumulator.as_ref())?; let current_string = std::str::from_utf8(current.value.as_ref())?; // Parse the strings into integers let accumulator_int = accumulator_string.trim().parse::\u0026lt;i32\u0026gt;().unwrap_or(0); let current_int = current_string.trim().parse::\u0026lt;i32\u0026gt;()?; // Take the sum of the two integers and return it as a string let sum = accumulator_int + current_int; Ok(sum.to_string().into()) } The first thing to know is that both the accumulator and the current record are represented as binary data. This is powerful because it allows us to work with truly arbitrary data, but it does require us to parse that data into a structured form that\u0026rsquo;s easier to work with. In this example, we\u0026rsquo;re representing our records as i32 (32-bit signed) integers, encoded in a UTF-8 string (this makes it easy to interact with the numbers from the CLI).\nLet\u0026rsquo;s look at how this SmartModule behaves when we apply it to a Fluvio stream.\nNote: To follow along, you\u0026rsquo;ll need to install the Fluvio CLI and follow the getting started instructions for setting up Fluvio on your OS. First, let\u0026rsquo;s create a topic where we\u0026rsquo;ll produce and consume our data from.\n%copy first-line%\n$ fluvio topic create aggregate-ints topic \u0026#34;aggregate-ints\u0026#34; created Then we\u0026rsquo;ll produce some data to the topic. Remember, our goal here is to sum up integers in a stream, so we\u0026rsquo;ll produce some input integers to see what happens.\n%copy first-line%\n$ fluvio produce aggregate-ints \u0026gt; 1 Ok! \u0026gt; 1 Ok! \u0026gt; 1 Ok! \u0026gt; 1 Ok! \u0026gt; 1 Ok! \u0026gt; 10 Ok! Finally, to view the output of processing this stream with our aggregate function, we need to compile our SmartModule and then point to it when we open our Consumer. If you\u0026rsquo;re following along with the template, run the following commands to build the SmartModule:\nIf you\u0026rsquo;ve never compiled for WASM before, you\u0026rsquo;ll need to install the proper rustup target. You should only need to do this once.\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown Finally, we can actually compile the SmartModule.\n%copy first-line%\n$ cargo build --release After running cargo build --release, we should be able to find the WASM binary at target/wasm32-unknown-unknown/release/aggregate_blog_sum.wasm.\n%copy first-line%\n$ ls -la target/wasm32-unknown-unknown/release .rwxr-xr-x 303Ki nick 23 Aug 16:50 -- aggregate_blog_sum.wasm At this point, we can now use the Fluvio CLI to consume the records from our stream and process them with our freshly-minted SmartModule. Let\u0026rsquo;s try it out with the following command:\n%copy first-line%\n$ fluvio consume aggregate-ints -B --aggregate=target/wasm32-unknown-unknown/release/aggregate_blog_sum.wasm Consuming records from the beginning of topic \u0026#39;aggregate-ints\u0026#39; 1 2 3 4 5 15 As we can see, our stream of 1\u0026rsquo;s followed by a 10 were summed up and turned into a stream of the aggregation of records so far.\n-\u0026gt; This output stream correlates directly with the purple output stream from the diagram earlier 👆\nExample #2: Aggregate with initial value This is a good opportunity to talk about the \u0026ldquo;initial value\u0026rdquo; of an aggregator. In our output stream above, we can infer that the initial value must have been the integer zero (0), since our first output value was identical to the first input value. However, the actual representation of the \u0026ldquo;empty accumulator\u0026rdquo; is not actually a numeric zero, but rather a literal empty buffer of bytes, the equivalent of an empty Rust Vec\u0026lt;u8\u0026gt;. The reason we get a numeric zero as our initial accumulator in this example is because of this line in the code:\nlet accumulator_int = accumulator_string.trim().parse::\u0026lt;i32\u0026gt;().unwrap_or(0); Here, we are taking our accumulator value as a string (an empty string, to be exact) and attempting to parse it as a 32-bit integer. In Rust, we\u0026rsquo;ll fail to parse an integer from an empty string, so we will fall back to the .unwrap_or(0) clause, which says to use the value we parsed if we were successful, or to use 0 if we were not successful. This is a pattern I\u0026rsquo;ve found to be quite helpful when parsing the accumulator into a structured value: if the accumulator can be parsed, parse it, otherwise, supply a sane default value to be used instead.\nIf we want to specify an initial value other than \u0026ldquo;empty record\u0026rdquo;, we can use the --initial flag in the Fluvio CLI to specify a file to use as the initial file. So let\u0026rsquo;s say we put the value 100 into a text file:\n%copy first-line%\n$ echo \u0026#39;100\u0026#39; \u0026gt; initial.txt Then, we can re-run our consumer and give initial.txt as the initial value to use for our accumulator value in the stream. To relate this back to the diagrams above, this becomes the new value in the dotted white box.\n%copy first-line%\n$ fluvio consume aggregate-ints -B --initial=./initial.txt --aggregate=target/wasm32-unknown-unknown/release/aggregate_blog_sum.wasm 101 102 103 104 105 115 Next, we\u0026rsquo;ll look at a slightly more detailed example that uses a more complex accumulator type!\nExample #3: Calculate an incremental average Summing integers in a stream is nice, but it\u0026rsquo;s rather simplistic. Let\u0026rsquo;s try to create an Aggregator that calculates the average of every number we\u0026rsquo;ve seen so far in a stream. For this example, our input will still be a stream of numbers (floats this time), but we\u0026rsquo;ll need to keep additional information in our accumulator in order to perform our calculations.\nIn order to calculate an average incrementally, we need to know the following information at each step in our aggregation:\nthe average of all the input we have seen so far, the number of inputs we have averaged, and the next input to add to our incremental average We\u0026rsquo;ll store 1) and 2) in our accumulator, and read 3) from our stream of input records.\nFollow along: create a new project Let\u0026rsquo;s create a new SmartModule Aggregate project aggregate-blog-average to play with this new use-case:\n%copy first-line%\n$ cargo generate --git https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : aggregate-blog-average 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · aggregate [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `aggregate-blog-average`... ✨ Done! New project created aggregate-blog-average Let\u0026rsquo;s jump right into the code for this example.\n%copy first-line%\n$ cd aggregate-blog-average Paste the following code into your src/lib.rs file:\n%copy%\nuse serde::{Serialize, Deserialize}; use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; #[derive(Default, Serialize, Deserialize)] struct IncrementalAverage { average: f64, count: u32, } impl IncrementalAverage { /// Implement the formula for calculating an incremental average. /// /// https://math.stackexchange.com/questions/106700/incremental-averageing fn add_value(\u0026amp;mut self, value: f64) { self.count += 1; let new_count_float = f64::from(self.count); let value_average_difference = value - self.average; let difference_over_count = value_average_difference / new_count_float; let new_average = self.average + difference_over_count; self.average = new_average; } } #[smartmodule(aggregate)] pub fn aggregate(accumulator: RecordData, current: \u0026amp;Record) -\u0026gt; Result\u0026lt;RecordData\u0026gt; { // Parse the average from JSON let mut average: IncrementalAverage = serde_json::from_slice(accumulator.as_ref()).unwrap_or_default(); // Parse the new value as a 64-bit float let value = std::str::from_utf8(current.value.as_ref())? .trim() .parse::\u0026lt;f64\u0026gt;()?; average.add_value(value); let output = serde_json::to_vec(\u0026amp;average)?; Ok(output.into()) } I quite like this example, because the most complicated thing that\u0026rsquo;s going on here is just the math for calculating an incremental average. Let\u0026rsquo;s look at some of the key elements at play:\nWe have an IncrementalAverage type that holds our running average value along with the number of values that have been added to this average This is what we use as our accumulator This value is serialized to JSON and becomes part of our output stream IncrementalAverage::add_value implements the formula for incremental averaging Let\u0026rsquo;s look at the body of the main aggregate function piece-by-piece to help us reason about the work that\u0026rsquo;s being done.\n// Parse the average from JSON let mut average: IncrementalAverage = serde_json::from_slice(accumulator.as_ref()).unwrap_or_default(); Here, we\u0026rsquo;re asking serde_json to try to deserialize our accumulator as an instance of IncrementalAverage. Looking at the struct definition, we can see that this should expect JSON objects that look approximately like this:\n{\u0026#34;average\u0026#34;:0.0,\u0026#34;count\u0026#34;:0} Incidentally, this specific value is what this code will give us if serde_json::from_slice fails to parse our JSON from the accumulator input. This is because .unwrap_or_default() will provide Default::default() if an error occurs during parsing, and the default value for IncrementalAverage has a zero in each field.\nAfter we parse our accumulator as an IncrementalAverage, the next bit is:\n// Parse the new value as a 64-bit float let value = std::str::from_utf8(current.value.as_ref())? .trim() .parse::\u0026lt;f64\u0026gt;()?; Here, we\u0026rsquo;re parsing our input record as a string, and then into a 64-bit float, returning an error if either of those steps fails.\nThe next line is where the actual work gets done, by adding our new value to our running average. I\u0026rsquo;ve re-pasted the running-average calculation below as well.\naverage.add_value(value); // this calls the function below // Explanation at https://math.stackexchange.com/questions/106700/incremental-averageing fn add_value(\u0026amp;mut self, value: f64) { self.count += 1; let new_count_float = f64::from(self.count); let value_average_difference = value - self.average; let difference_over_count = value_average_difference / new_count_float; let new_average = self.average + difference_over_count; self.average = new_average; } Without getting too far into the math, an average is basically the sum-of-inputs over the number-of-inputs. To add a new input, we can \u0026ldquo;undo\u0026rdquo; the division, add our new input to the sum, and re-divide by the new number of inputs. This is the reason that we have to keep track of both the average and the count in our IncrementalAverage type.\nThe last thing left to do is serialize our updated accumulator and return it:\nlet output = serde_json::to_vec(\u0026amp;average)?; Ok(output.into()) Let\u0026rsquo;s take this for a test drive. Create a new topic for our averaging.\n%copy first-line%\n$ fluvio topic create aggregate-average topic \u0026#34;aggregate-average\u0026#34; created Let\u0026rsquo;s produce some numbers to average:\n%copy first-line%\n$ fluvio produce aggregate-average \u0026gt; 2 Ok! \u0026gt; 4 Ok! \u0026gt; 6 Ok! \u0026gt; 8 Ok! \u0026gt; 10 Ok! \u0026gt; 12 Ok! \u0026gt; ^C Make sure to build the new SmartModule, then consume with it.\n%copy first-line%\n$ cargo build --release %copy first-line%\n$ fluvio consume aggregate-average -B --aggregate=target/wasm32-unknown-unknown/release/aggregate_blog_average.wasm Consuming records from the beginning of topic \u0026#39;aggregate-average\u0026#39; {\u0026#34;average\u0026#34;:2.0,\u0026#34;count\u0026#34;:1} {\u0026#34;average\u0026#34;:3.0,\u0026#34;count\u0026#34;:2} {\u0026#34;average\u0026#34;:4.0,\u0026#34;count\u0026#34;:3} {\u0026#34;average\u0026#34;:5.0,\u0026#34;count\u0026#34;:4} {\u0026#34;average\u0026#34;:6.0,\u0026#34;count\u0026#34;:5} {\u0026#34;average\u0026#34;:7.0,\u0026#34;count\u0026#34;:6} I happened to find a sequence of inputs that gives very nice round averages, but you can test this out with any decimal numbers and see that the averages are coming out as expected.\nOur last example will showcase both a structured accumulator and structured input records 👇\nExample #4: Sum a key-value object point-by-point For this last example, suppose we had a real-time stream representing new stars on GitHub repositories, and that the events in this stream described which repositories received stars and how many stars were added.\n{\u0026#34;infinyon/fluvio\u0026#34;:7,\u0026#34;serde-rs/serde\u0026#34;:8} {\u0026#34;infinyon/fluvio\u0026#34;:4,\u0026#34;serde-rs/serde\u0026#34;:5,\u0026#34;serde-rs/json\u0026#34;:7} {\u0026#34;serde-rs/serde\u0026#34;:11,\u0026#34;serde-rs/json\u0026#34;:6,\u0026#34;infinyon/node-bindgen\u0026#34;:3} We can create an Aggregate function to sum up these objects point-by-point, so that the numbers on matching keys are added to each other, and unique repositories maintain independent sums. We would expect the aggregate of the above to look like:\n{\u0026#34;infinyon/fluvio\u0026#34;:7,\u0026#34;serde-rs/serde\u0026#34;:8} {\u0026#34;infinyon/fluvio\u0026#34;:11,\u0026#34;serde-rs/serde\u0026#34;:13,\u0026#34;serde-rs/json\u0026#34;:7} {\u0026#34;infinyon/fluvio\u0026#34;:11,\u0026#34;serde-rs/serde\u0026#34;:24,\u0026#34;serde-rs/json\u0026#34;:13,\u0026#34;infinyon/node-bindgen\u0026#34;:3} Follow along: create a new project Let\u0026rsquo;s create one last aggregate-blog-stars template project:\n%copy first-line%\n$ cargo generate --git https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : aggregate-blog-stars 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · aggregate [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `aggregate-blog-stars`... ✨ Done! New project created aggregate-blog-stars Let\u0026rsquo;s jump right into the code for this example.\n%copy first-line%\n$ cd aggregate-blog-average Paste the following code into your src/lib.rs file:\n%copy%\nuse std::collections::HashMap; use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; use serde::{Serialize, Deserialize}; #[derive(Default, Serialize, Deserialize)] struct GithubStars(HashMap\u0026lt;String, u32\u0026gt;); impl std::ops::Add for GithubStars { type Output = Self; fn add(mut self, next: Self) -\u0026gt; Self::Output { for (repo, new_stars) in next.0 { self.0.entry(repo) .and_modify(|stars| *stars += new_stars) .or_insert(new_stars); } self } } #[smartmodule(aggregate)] pub fn aggregate(accumulator: RecordData, current: \u0026amp;Record) -\u0026gt; Result\u0026lt;RecordData\u0026gt; { // Parse accumulator let accumulated_stars: GithubStars = serde_json::from_slice(accumulator.as_ref()).unwrap_or_default(); // Parse next record let new_stars: GithubStars = serde_json::from_slice(current.value.as_ref())?; // Add stars and serialize let summed_stars = accumulated_stars + new_stars; let summed_stars_bytes = serde_json::to_vec_pretty(\u0026amp;summed_stars)?; Ok(summed_stars_bytes.into()) } One thing I\u0026rsquo;d like to point out right off the bat is that the general flow for each of the Aggregate examples we\u0026rsquo;ve looked at is pretty much the same. In each example, we:\nParse the accumulator and input record into a structured form Add the input record to the accumulator Serialize the accumulator and return it I\u0026rsquo;ve personally found it very helpful to define a custom type to represent the accumulator, and to define an associated function that describes how to \u0026ldquo;merge\u0026rdquo; the structured input record into the structured accumulator. Let\u0026rsquo;s take a look at how this works in this example.\nHere, I\u0026rsquo;ve created a custom type called GithubStars that is used both as our structured accumulator and input record. Recall from the previous example that the accumulator and record do not always need to be the same type, but in this case it just-so-happened that way.\nGithubStars is just a wrapper around HashMap\u0026lt;String, u32\u0026gt;, which allows it to deserialize from a JSON object with arbitrary strings as keys and positive integers as values. The interesting part here is the impl std::ops::Add for GithubStars, which describes how to \u0026ldquo;add\u0026rdquo; two of these values together, even allowing us to use the + operator when we have two values of this type. Let\u0026rsquo;s take a closer look at this implementation:\nimpl std::ops::Add for GithubStars { type Output = Self; fn add(mut self, next: Self) -\u0026gt; Self::Output { for (repo, new_stars) in next.0 { self.0.entry(repo) .and_modify(|stars| *stars += new_stars) .or_insert(new_stars); } self } } Here, both self and next are instances of GithubStars. In this case, self will be our accumulator value and next will be the structured input record. We essentially want to add all the entries from next to the corresponding entries of self. Since HashMap\u0026lt;String, u32\u0026gt; implements IntoIterator, we can iterate over the key/value pairs in a plain-old for loop.\nInside the loop, we\u0026rsquo;re making use of the incredible Entry API for Rust HashMaps which allows us to insert and manipulate elements in-place with a buttery-smooth call chain. The three lines here describe what\u0026rsquo;s happening almost in plain English:\nFind the entry in the map for a key called repo If this entry exists, modify the value there by adding new_stars to it If this entry didn\u0026rsquo;t exist, initialize it by inserting new_stars as the value there This is one of my favorite APIs in the Rust standard library, if you haven\u0026rsquo;t seen it before I highly recommend going to read about it!\nAnyways, let\u0026rsquo;s try this out and see if everything works as expected! Let\u0026rsquo;s create a new topic and produce some data:\n%copy first-line%\n$ fluvio topic create aggregate-stars topic \u0026#34;aggregate-stars\u0026#34; created %copy first-line%\n$ fluvio produce aggregate-stars \u0026gt; {\u0026#34;infinyon/fluvio\u0026#34;:7,\u0026#34;serde-rs/serde\u0026#34;:8} Ok! \u0026gt; {\u0026#34;infinyon/fluvio\u0026#34;:4,\u0026#34;serde-rs/serde\u0026#34;:5,\u0026#34;serde-rs/json\u0026#34;:7} Ok! \u0026gt; {\u0026#34;serde-rs/serde\u0026#34;:11,\u0026#34;serde-rs/json\u0026#34;:6,\u0026#34;infinyon/node-bindgen\u0026#34;:3} Ok! Let\u0026rsquo;s make sure to compile our SmartModule, then we can open up a consumer on our Topic.\n%copy first-line%\n$ cargo build --release %copy first-line%\n$ fluvio consume aggregate-stars -B --aggregate=target/wasm32-unknown-unknown/release/aggregate_blog_average.wasm Consuming records from the beginning of topic \u0026#39;aggregate-stars\u0026#39; { \u0026#34;infinyon/fluvio\u0026#34;: 7, \u0026#34;serde-rs/serde\u0026#34;: 8 } { \u0026#34;infinyon/fluvio\u0026#34;: 11, \u0026#34;serde-rs/serde\u0026#34;: 13, \u0026#34;serde-rs/json\u0026#34;: 7 } { \u0026#34;infinyon/node-bindgen\u0026#34;: 3, \u0026#34;infinyon/fluvio\u0026#34;: 11, \u0026#34;serde-rs/serde\u0026#34;: 24, \u0026#34;serde-rs/json\u0026#34;: 13 } Tada, we got the same numbers we were expecting! Note that we\u0026rsquo;re getting an expanded \u0026ldquo;pretty\u0026rdquo; print of the accumulator output, this is because we used serde_json::to_vec_pretty when we serialized our output.\nConclusion I had quite a lot of fun writing this post, I think that Aggregates are going to be one of the most interesting SmartModules in terms of potential use-cases that can be solved with them. I\u0026rsquo;m excited to hear feedback and ideas for how this could be applied, be sure to join the discussion on Reddit and come talk to us on Discord, we\u0026rsquo;re happy to talk and answer any questions. Until next time!\nUpdate: SmartModules were originally called SmartStreams. The blog was updated to reflect the new naming convention\nFurther reading SmartModule Aggregate documentation SmartModule Filter documentation SmartModule Map documentation About Fluvio: the Programmable Streaming Platform Fluvio SmartModules: Writing a WASM-based filter for application logs ","description":"Examples on how to use aggregate functions to calculate streaming sums, averages, and key-value updates.","keywords":null,"summary":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. We\u0026rsquo;re making steady progress adding new inline data processing capabilities, building on our SmartModules feature that allows users to write custom code to interact with their streaming data. SmartModules are written in Rust, compiled to WebAssembly, and executed on Fluvio\u0026rsquo;s Streaming Processing Units to manipulate data inline.\nThis week we\u0026rsquo;re happy to announce Aggregations for SmartModules! Aggregates let you define functions that combine each record in a stream with some long-running state, or \u0026ldquo;accumulator\u0026rdquo;.","title":"Aggregate streaming data in real-time with WebAssembly","url":"http://localhost:1315/blog/2021/08/smartmodule-aggregates/"},{"body":"We\u0026rsquo;re excited to announce that [InfinyOn Cloud] has a new interactive web user interface for managing Fluvio clusters which allows users to produce, consume, and manipulate data.\nIf you are unfamiliar with Fluvio, it is an open-source, high-performance distributed data streaming platform for real-time apps written in Rust. Infinyon Cloud provisions, manages, and scales your Fluvio cluster for you, so that you can focus on your data.\nInteractive UI The new UI allows you to configure and monitor your cluster without switching to external tools such as the Fluvio CLI. You now are able to view, create, and delete topics.\nBy selecting a topic you are presented with tabs for interacting with partitions, records, and SmartModules.\nLow Latency We\u0026rsquo;re able to enable low latency access directly from the web UI to the Fluvio Cluster by using WebSockets. WebSockets allows bi-directional communication in binary format with as little overhead as TCP. We run the official Fluvio client directly in the web browser by compiling it to WebAssembly. This allows us to efficiently encode and decode Fluvio messages directly in the browser. This eliminates the need to use an intermediate format such as JSON, which would require us to buffer and transform messages in a separate backend component, which would add latency. This is significant since Fluvio is used for real-time, high-throughput applications, where even small amounts of latency and overhead are amplified and affect performance.\nProducing and Consuming Under the Records tab you can produce new data to a topic and also consume all previously produced data and new data in realtime. This serves as a starting point for experimenting with the Fluvio platform. Once you are up and running with external producers and/or consumers, it serves as a debugging tool to quickly inspect your data stream and inject test data as needed.\nSmartModule Editor The SmartModules editor is an easy way to experience the powerful compute engine built into Fluvio. The editor currently allows you to create a filter program which filters the records in the current topic. The program has access to the bytes of each record so that it can perform any computation on the record to determine whether to filter. The editor will soon also support SmartModuels of type \u0026ldquo;map\u0026rdquo;, which allows transforming the record contents into any desired format.\nThe editor is initialized with a simple filter which only allows records which contain the character \u0026ldquo;a\u0026rdquo;. Clicking \u0026ldquo;Apply\u0026rdquo; compiles the filter program into a WASM module and uploads it to the SPU for execution. The result is streamed back to the UI and displayed to you. If the code is invalid, the compilation errors are displayed in the \u0026ldquo;Console\u0026rdquo; tab. You may create new records on this page and observe the filter being applied to your input in real-time.\nOnce satisfied with the code, you may compile it yourself, see instructions in SmartModules Quick Start Guide, and use it with the Fluvio client in your applications.\nCurrently the editor only supports writing the program in Rust, however it\u0026rsquo;s possible to write SmartModules in any language that can be compiled to WebAssembly. We chose Rust as the language to start with as our platform itself is written in it. We plan to add support for more languages in the editor.\nSummary Setting up a real-time data streaming app with our platform just became easier.\nDon\u0026rsquo;t forget to join the conversation on Discord, follow the project on github or open an issue if you find a bug or have a feature request. [InfinyOn Cloud]: https://infinyon.cloud/signup?utm_campaign=cloudui\u0026amp;utm_source=website\u0026amp;utm_medium=blog\u0026amp;utm_term=cloudui\u0026amp;utm_content=cloud-registration\n","description":"InfinyOn Cloud adds new capabilities to enable interactive exploration and manipulation of streaming data.","keywords":null,"summary":"We\u0026rsquo;re excited to announce that [InfinyOn Cloud] has a new interactive web user interface for managing Fluvio clusters which allows users to produce, consume, and manipulate data.\nIf you are unfamiliar with Fluvio, it is an open-source, high-performance distributed data streaming platform for real-time apps written in Rust. Infinyon Cloud provisions, manages, and scales your Fluvio cluster for you, so that you can focus on your data.\nInteractive UI The new UI allows you to configure and monitor your cluster without switching to external tools such as the Fluvio CLI.","title":"Announcing InfinyOn Cloud's interactive real-time stream editor","url":"http://localhost:1315/blog/2021/08/cloud-interactive-ui/"},{"body":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. We\u0026rsquo;ve been hard at work building new capabilities for inline data processing, a family of features that we call SmartModules, and with our latest major release we announced the arrival of our new SmartModule Map functionality. This feature allows users to write custom code to inspect and transform each record or data in a stream. Users write SmartModule modules in Rust and compile them to WebAssembly, and they are ultimately executed in the Fluvio cluster on a Streaming Processing Unit (SPU).\nYou can check out the full code for this blog in the SmartModule examples repo!\nThinking about Mapping capabilities and use-cases When we\u0026rsquo;re thinking about Mapping, we\u0026rsquo;re thinking about a function that takes one record as input and gives another record as output. With SmartModule Map, we are writing functions that run inside a WebAssembly sandbox, so these functions do not have access to the outside world, and cannot do certain things such as make network requests or write data to disk. In essence, reading and manipulating the input record is the only thing that a Map function can do.\nThis helps to set the stage for what types of operations we can perform using SmartModule Maps. I tend to think of these operations in the following broad categories (though there are certainly more):\nScrubbing sensitive fields of data to hide from downstream consumers Narrowing large records into a smaller subset of important fields Computing rich, derived fields from simple raw data Parsing unstructured (e.g. textual) data into a structured form (e.g. JSON) Let\u0026rsquo;s narrow in and explore how we can use SmartModules to solve a \u0026ldquo;scrubbing sensitive fields\u0026rdquo; use-case.\nConcrete use-case: Scrubbing Social Security Numbers from account records Let\u0026rsquo;s imagine that we\u0026rsquo;re working with a banking account system, and we have a stream of data that represents account activity. Events in this stream might represent new accounts being created, passwords being changed, or personal info being updated. Suppose we want to write an application which sends a welcome email to new account owners after they sign up. We can structure our email application to consume from the accounts topic and send an email each time an account-created event appears.\nSo far so good, but let\u0026rsquo;s add a twist. Since we\u0026rsquo;re talking about bank accounts, the events contain some private information, such as the account holder\u0026rsquo;s Social Security Number. We would like to edit the records in our accounts stream and scrub out SSNs so that the email application never even has access to that data. That way, there\u0026rsquo;s no chance that a bug or a compromise in the email application could lead to disclosing this private information.\nAlright, so let\u0026rsquo;s get concrete. We\u0026rsquo;ll set up our event schema so that records in our stream look something like this:\n{ \u0026#34;social_security_number\u0026#34;: \u0026#34;123-45-6789\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;account-created\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;1509aaf8-5863-4b41-bfe2-b081691d7a6e\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;Daniel\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Mahoney\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;daniel_mahoney@example.com\u0026#34;, \u0026#34;password_hash\u0026#34;: \u0026#34;db6b535bc9909ecfb7c2ee4550ed7b350a61785e\u0026#34; } After we\u0026rsquo;re done scrubbing, we want our records to look more like this:\n{ \u0026#34;social_security_number\u0026#34;: \u0026#34;***-**-****\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;account-created\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;1509aaf8-5863-4b41-bfe2-b081691d7a6e\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;Daniel\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Mahoney\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;daniel_mahoney@example.com\u0026#34;, \u0026#34;password_hash\u0026#34;: \u0026#34;db6b535bc9909ecfb7c2ee4550ed7b350a61785e\u0026#34; } For this example, I\u0026rsquo;m oversimplifying things to make it easy to follow, so I want to say up front that this is a toy example, and real-world privacy protection should take many more precautions than I\u0026rsquo;m doing here. Let\u0026rsquo;s create a new SmartModule project to follow along with the code we\u0026rsquo;re about to see. We can use a cargo-generate template to get the project set up easily. To install cargo-generate, run the following command:\n%copy first-line%\n$ cargo install cargo-generate Then to create a new Map project, run the following:\n%copy first-line%\n$ cargo generate --git https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : regex-scrubbing 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · map [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `regex-scrubbing`... ✨ Done! New project created regex-scrubbing For this example, we\u0026rsquo;ll need to add dependencies on the regex and once_cell crates Let\u0026rsquo;s navigate into our project directory and take a look at Cargo.toml:\n%copy first-line%\n$ cd regex-scrubbing \u0026amp;\u0026amp; cat Cargo.toml Add the following lines to the Cargo.toml file:\n[dependencies] fluvio-smartmodule = { version = \u0026#34;0.1\u0026#34; } serde = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1\u0026#34; regex = \u0026#34;1\u0026#34; once_cell = \u0026#34;1\u0026#34; Now that we have a project skeleton set up, let\u0026rsquo;s look at the code we can use to solve our use-case. Paste the following code into the src/lib.rs file.\n%copy%\nuse regex::Regex; use once_cell::sync::Lazy; use fluvio_smartmodule::{smartmodule, Result, Record, RecordData}; // A compiled Regex for detecting SSNs that look like XXX-XX-XXXX static SSN_RE: Lazy\u0026lt;Regex\u0026gt; = Lazy::new(|| Regex::new(r\u0026#34;\\d{3}-\\d{2}-\\d{4}\u0026#34;).unwrap()); #[smartmodule(map)] pub fn map(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;(Option\u0026lt;RecordData\u0026gt;, RecordData)\u0026gt; { let key = record.key.clone(); let string = std::str::from_utf8(record.value.as_ref())?; let output = SSN_RE.replace_all(string, \u0026#34;***-**-****\u0026#34;).to_string(); Ok((key, output.into())) } We are ready to build our SmartModule.\nIf you\u0026rsquo;ve never compiled for WASM before, you\u0026rsquo;ll need to install the proper rustup target. You should only need to do this once.\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown Let\u0026rsquo;s go ahead and compile using --release mode to get the smallest WASM binary possible:\n%copy first-line%\n$ cargo build --release That\u0026rsquo;s it. That\u0026rsquo;s the whole SmartModule.\nOf course, this is an over-simplified solution to a toy problem, but it illustrates how we can quickly manipulate the data in our stream with relatively little effort. In this SmartModule, we\u0026rsquo;re using a Regex to detect any string that has digits in the typical SSN layout, and replace them with a meaningless substitute string ***-**-****. Another potential solution to this problem would be to parse the JSON and delete the social_security_number field, but for our purposes this strategy was simpler.\nFor completeness on this first example, here is an example session showing some data being processed by this SmartModule:\nFirst, create a new topic for us to produce and consume our data %copy first-line%\n$ fluvio topic create accounts topic \u0026#34;accounts\u0026#34; created Generate some sample data in a file named accounts.json: %copy first-line%\n$ echo \u0026#39;{\u0026#34;social_security_number\u0026#34;:\u0026#34;123-45-6789\u0026#34;,\u0026#34;event_type\u0026#34;:\u0026#34;account-created\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1509aaf8-5863-4b41-bfe2-b081691d7a6e\u0026#34;,\u0026#34;first_name\u0026#34;:\u0026#34;Daniel\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Mahoney\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;daniel_mahoney@example.com\u0026#34;,\u0026#34;password_hash\u0026#34;:\u0026#34;db6b535bc9909ecfb7c2ee4550ed7b350a61785e\u0026#34;}\u0026#39; \u0026gt; account.json Produce the sample input data to our topic %copy first-line%\n$ fluvio produce accounts \u0026lt; account.json Consume from our topic after applying our SmartModule Map %copy first-line%\n$ fluvio consume accounts -B --map=target/wasm32-unknown-unknown/release/regex_scrubbing.wasm Consuming records from the beginning of topic \u0026#39;accounts\u0026#39; {\u0026#34;social_security_number\u0026#34;:\u0026#34;***-**-****\u0026#34;,\u0026#34;event_type\u0026#34;:\u0026#34;account-created\u0026#34;,\u0026#34;account_id\u0026#34;:\u0026#34;1509aaf8-5863-4b41-bfe2-b081691d7a6e\u0026#34;,\u0026#34;first_name\u0026#34;:\u0026#34;Daniel\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Mahoney\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;daniel_mahoney@example.com\u0026#34;,\u0026#34;password_hash\u0026#34;:\u0026#34;db6b535bc9909ecfb7c2ee4550ed7b350a61785e\u0026#34;} If we were trying to deploy this to production we would obviously want a much more intelligent function for detecting sensitive data in order to scrub it out, but it\u0026rsquo;s clear to see that even with this simplified example, the consumer never witnesses any of the SSN information.\nThat\u0026rsquo;s an important point, so I want to restate it:\nThe consumer never witnesses any of the SSN information\nThat\u0026rsquo;s because SmartModule code is executed in the Fluvio cluster, before it even touches the network on the way to the consumer. Having this ability to perform server-side data processing opens up a whole new world of interesting possible applications, and we\u0026rsquo;re excited to see what new use-cases the community discovers.\nConclusion That\u0026rsquo;s it for this post, be sure to hop into our Discord server if you want to talk to us or have any questions. Until next time!\nUpdate: SmartModules were originally called SmartStreams. The blog was updated to reflect the new naming convention\nFurther reading SmartModule Map documentation About Fluvio: the Programmable Streaming Platform Fluvio SmartModules: Writing a WASM-based filter for application logs ","description":"Exploring use-cases and examples for real-time stream processing","keywords":null,"summary":"Fluvio is a high-performance, distributed, programmable streaming platform for real-time data. We\u0026rsquo;ve been hard at work building new capabilities for inline data processing, a family of features that we call SmartModules, and with our latest major release we announced the arrival of our new SmartModule Map functionality. This feature allows users to write custom code to inspect and transform each record or data in a stream. Users write SmartModule modules in Rust and compile them to WebAssembly, and they are ultimately executed in the Fluvio cluster on a Streaming Processing Unit (SPU).","title":"Transform streaming data in real-time with WebAssembly","url":"http://localhost:1315/blog/2021/08/smartmodule-map-use-cases/"},{"body":"SANTA CLARA, CA \u0026ndash; We are thrilled to announce that InfinyOn is now a member of the Bytecode Alliance (BA). An organization committed to establishing a capable, secure platform that allows application developers and service providers to confidently run untrusted code on any infrastructure, operating system, or device.\nThe BA is a vendor-neutral organization for building reusable WebAssembly engines and toolings. With industry-leading members such as Fastly, Intel, Google, Mozilla, and Microsoft, the BA fosters standards such as WebAssembly System Interface (WASI) to make it easier to embed WebAssembly in platforms such as Fluvio. Fluvio is an Open Source programmable data platform spearheaded by the InfinyOn team. The platform leverages the power of WebAssembly to build safe and composable data streaming processing.\n\u0026ldquo;As organizations look to gain value out of their data, traditional monolithic one-size-fits data infrastructures are no longer sufficient,\u0026rdquo; said Sehyo Chang, CTO and co-founder of InfinyOn. \u0026ldquo;WebAssembly is an ideal mechanism for in-line data processing without compromising performance and security.\u0026rdquo;\nBy joining BA, InfinyOn will participate in events and collaborate with other members of the organization to extend WebAssembly to the data streaming space.\n\u0026ldquo;We are thrilled to welcome InfinyOn to the Bytecode Alliance, said Till Schneidereit, Chairperson of the Bytecode Alliance Board of Directors. The BA was founded to create new, secure software foundations based on WebAssembly and WASI. We are excited to see InfinyOn using WebAssembly, and we look forward to working with the team and extend the technology to the data streaming space.\u0026rdquo;\nWe believe WebAssembly is the crucial technology to democratize real-time data processing and accelerate digital transformation, and we are excited to be part of it.\nAbout InfinyOn InfinyOn, a real-time data streaming company, has architected a programmable platform for data in motion built on Rust and enables continuous intelligence for connected apps. SmartModules enable enterprises to intelligently program their data pipelines as they flow between producers and consumers for real-time services. With InfinyOn Cloud, enterprises can quickly correlate events, apply business intelligence, and derive value from their data. To learn more, please visit infinyon.com.\n","description":"InfinyOn uses WebAssembly for the industy's first programmable data streaming platform.","keywords":null,"summary":"SANTA CLARA, CA \u0026ndash; We are thrilled to announce that InfinyOn is now a member of the Bytecode Alliance (BA). An organization committed to establishing a capable, secure platform that allows application developers and service providers to confidently run untrusted code on any infrastructure, operating system, or device.\nThe BA is a vendor-neutral organization for building reusable WebAssembly engines and toolings. With industry-leading members such as Fastly, Intel, Google, Mozilla, and Microsoft, the BA fosters standards such as WebAssembly System Interface (WASI) to make it easier to embed WebAssembly in platforms such as Fluvio.","title":"InfinyOn joins the Bytecode Alliance","url":"http://localhost:1315/press-releases/infinyon-joins-bytecode-alliance/"},{"body":"We\u0026rsquo;re thrilled to announce that InfinyOn is now a member of the Bytecode Alliance (BA). An organization committed to establishing a capable, secure platform that allows application developers and service providers to confidently run untrusted code on any infrastructure, operating system, or device.\nThe BA is a vendor-neutral organization for building reusable WebAssembly engines and toolings. With industry-leading members such as Fastly, Intel, Google, Mozilla, and Microsoft, the BA fosters standards such as WebAssembly System Interface (WASI) to make it easier to embed WebAssembly in platforms such as Fluvio. Fluvio is an Open Source programmable data platform spearheaded by the InfinyOn team. The platform leverages the power of WebAssembly to build safe and composable data streaming processing.\n\u0026ldquo;As organizations look to gain value out of their data, traditional monolithic one-size-fits data infrastructures are no longer sufficient,\u0026rdquo; said Sehyo Chang, CTO and Co-founder of InfinyOn. \u0026ldquo;WebAssembly is an ideal mechanism for in-line data processing without compromising performance and security.\u0026rdquo;\nBy joining BA, InfinyOn will participate in events and collaborate with other members of the organization to extend WebAssembly to the data streaming space.\n\u0026ldquo;We are thrilled to welcome InfinyOn to the Bytecode Alliance, said Till Schneidereit, Chairperson of the Bytecode Alliance Board of Directors. The BA was founded to create new, secure software foundations based on WebAssembly and WASI. We are excited to see InfinyOn using WebAssembly, and we look forward to working with the team and extend the technology to the data streaming space.\u0026rdquo;\nWe believe WebAssembly is the crucial technology to democratize real-time data processing and accelerate digital transformation, and we are excited to be part of it.\n","description":"InfinyOn joins the Bytecode Alliance, an organization focused on building reusable WebAssembly(WASM) engines and tools.","keywords":null,"summary":"We\u0026rsquo;re thrilled to announce that InfinyOn is now a member of the Bytecode Alliance (BA). An organization committed to establishing a capable, secure platform that allows application developers and service providers to confidently run untrusted code on any infrastructure, operating system, or device.\nThe BA is a vendor-neutral organization for building reusable WebAssembly engines and toolings. With industry-leading members such as Fastly, Intel, Google, Mozilla, and Microsoft, the BA fosters standards such as WebAssembly System Interface (WASI) to make it easier to embed WebAssembly in platforms such as Fluvio.","title":"InfinyOn joins the Bytecode Alliance to bring WebAssembly to data streaming","url":"http://localhost:1315/blog/2021/07/infinyon-joins-bytecode-alliance/"},{"body":"Fluvio is a distributed streaming platform written in Rust. One of our primary goals with Fluvio is to provide excellent performance, especially with regard to latency.\nHowever, until now we haven\u0026rsquo;t had a comprehensive benchmarking suite to help us pinpoint exactly where we stand in terms of concrete numbers. We\u0026rsquo;ve been hard at work building up our benchmark game, so in this post we\u0026rsquo;re going to talk about what we\u0026rsquo;ve discovered about where we are and what we\u0026rsquo;ve learned about where we can improve.\nFluvio is still in alpha and we have not spent much time optimizing for performance, so we welcome constructive feedback with respect to performance development and measurement.\nTesting Methodology My methodology took inspiration from the Pulsar performance report, the Confluent report, and Open Messaging Benchmark.\nWe will be sharing performance metrics of our workflow in terms of:\nPerformance metrics Source of metric Max Throughput Maximum value observed from all records produced. Calculated by: Size of Records sent from Producer / Produce Latency TimeMaximum value observed from all records consumed. Calculated by: Size of Records received by Consumer/ Consume Latency Time Latency Time starting when Producer sends a record to a topic, until the record reported received by FluvioTime starting when Consumer start and completes downloading a record from a topicThe combined End-to-End (E2E) time from the same record being sent by the Producer then received by a Consumer Host machine resource usage Memory, by sampling the difference between starting memory and current memoryCPU, by sampling the current CPU load average We will only be measuring one workflow against locally installed Fluvio cluster because it is the most common type of cluster application developers will use (other than our Fluvio Cloud service).\nA Topic is a handle that Producers or Consumers use to interact with a Fluvio data stream. They can be configured for parallelism (with partitions) or data resilency (with replication).\nThe test starts with our producers sending 10K records (~1KB each) to a Fluvio topic.\nA Producer is an client that streams data to the Fluvio cluster, via a topic.\nAt the same time a consumer starts listening to a topic for new records, until all 10K records are received.\nA Consumer is an client that requests a data stream from the Fluvio cluster, via topic\nWe use our custom test harness to do the cluster testing that created the data for the visualizations we’re going to share. (Shameless self-promotion: If you’re a Rust dev too, just so you know, I also wrote a post about writing your own custom test harness including why and how we did it. But it is not required reading for this post.)\nNote: We did not use Open Messaging Benchmark even though we have a Java client, on account of my struggle-failure to set it up to reproduce Pulsar or Kafka tests.\nPerhaps in a future report if there is enough interest? Let me know!\nHost and Cluster configuration Test Host Host environment AWS instance type i3.4xlarge Rust version 1.53.0 Fluvio version 0.8.5 All our testing is run off of a single i3.4xlarge.\nInstance vCPU Mem (GiB) Local Storage (GB) Networking Performance (Gbps) i3.4xlarge 16 122 2 x 1,900 NVMe SSD Up to 10 This is the same instance type referenced in Pulsar and Kafka\u0026rsquo;s Terraform vars in the Open Messaging Benchmark repo.\nCluster We will test using 1 type of Fluvio cluster and comparing their results with other relatable benchmarks when possible.\nRunning a Fluvio cluster locally, which is common for devs working locally on their own applications. We will configure our topics, the storage for our test events, with different replication settings:\nThe Fluvio cluster is using 3 stream processing units or SPU (equivilent to Kafka/Pulsar \u0026ldquo;brokers\u0026rdquo;) and the topic is configured for 3x replication. Cluster instance # SPU Topic partitions Topic replication Fluvio 3 1 3x (The Fluvio cluster is similar to the configuration used in Pulsar\u0026rsquo;s configuration for Max throughput #4. Our test measurements don\u0026rsquo;t properly support partitions at the time of this writing.)\nNow let\u0026rsquo;s get on with the test results.\nMax Throughput This is the quantity of our record size sent/received within a period of time. Such as the latency of our producer or consumer.\nData tranfer size / Data transfer latency\nWe\u0026rsquo;ll be measuring throughput in Megabytes per second (MB/s). Larger values are desired.\nProducer First we’re going to check out the maximum data rate. We transfered 10MB (10K records with ~1KB message sizes) from the Producer and to Consumer and calculated the throughput. Higher values are better.\nProducer Throughput Bytes Max rate (MBytes/s) Fluvio 10358890 1.50 The closest data comparison we have from chart data from Pulsar\u0026rsquo;s Max Throughput #4 Figure #7.\nProducer Throughput Max rate (MBytes/s) Kafka (ack-1) ~230 Pulsar (ack-1) ~300 My first thought was that these results were a bit slow for sending ~10MB, if we compare the the rates seen by the more mature Pulsar or Kafka. However, now we know, and we can improve these rates with some optimization.\nConsumer For the Consumer, we see higher values than the Producer.\nConsumer Throughput Bytes Max rate (MBytes/s) Fluvio 10358890 4.75 Latency Another commonly used term for latency is “response time”.\nThe reason we use latency as a quantitative measure of user experience is because users equate fast and accurate service with high quality. We\u0026rsquo;ll be measuring latency in milliseconds (ms).\nSmaller values of latency are desired.\n(For more background on latency as a metric, check out this very approachable post on latency which covers the math tools for measuring and how to interpret latency.)\nNext we\u0026rsquo;ll review the latency of the Producer uploading a record, a Consumer downloading and the End-to-End (E2E) latency of the Producer and Consumer. Lower values are better.\nProducer For the producer latency, we measure the time to send 10K records. These contents of these records include a 1KB payload, plus some metadata and a timestamp used by the E2E measurement.\nWe have the Producer latency over the test run on the left, and the data organized into their percentiles. What we can see from the percentile chart is mostly how low the values are on the left-side of the chart. We see in the 99.9th percentile that we see the latency increase.\nProducer Latency Average P50 P90 P99 P999 Fluvio 1.20ms 1.26ms 1.57ms 1.89ms 7.14ms Average is calculated by adding up all the measurement values, and dividing by the number of measurements. But it is given as just one of the aggregated values for our test results. Its intention is to communicate the common performance, but may be misleading in certain contexts, such as latency. So when we talk about latency, we offer averages with percentile measurements.\nPercentiles are another calculated value based on the collection of measured values. They are a useful tool for communicating the distribution at different slices of the raw data.\nWe describe our latency results with multiple percentile values: P50, P99, P999 to describe the worst latency experienced by 50%, 99% and 99.9% of all requests.\nThe closest data comparison we have from chart data from Pulsar\u0026rsquo;s Max Throughput #4 Figure 8.\nProducer Latency P50 P90 P99 P999 Kafka (ack-1) ~136ms ~141ms ~155ms ~191ms Pulsar (ack-1) ~8ms ~10ms ~11ms ~22ms Consumer Consumer latency is time between when the Consumer starts an attempt to stream a record, and when it succeeds.\nThe Consumer also appears to have consistent latency. Glancing at the percentile graph, latency minimally increases until we look above the 99th percentile.\nConsumer Latency Average P50 P90 P99 P999 Fluvio 11.37ms 11.47ms 11.80ms 12.12ms 21.50ms E2E The E2E charts look similar to the Consumer, signaling that the Consumer latency primarily represents the E2E experience, with the Producer latency contributing an overall smaller share of time. Whatever the reason may be for this Consumer latency, we still see that consistent percentile line until the P999.\nE2E Latency Average P50 P90 P99 P999 Fluvio 11.99ms 12.12ms 12.39ms 12.91ms 18.87ms The closest data comparison we have from chart data from Pulsar\u0026rsquo;s Max Throughput #4 Figure 9.\nProducer Latency P50 P90 P99 P999 Kafka (ack-1) ~171ms ~177ms ~203ms ~227ms Pulsar (ack-1) ~40ms ~41ms ~43ms ~57ms Memory usage To measure memory usage, we take the snapshot of memory usage at the beginning of the test, and frequently sample the difference between the beginning usage and current usage.\nOne way to describe the memory usage over time is that it slowly increases, and occasionally jumps in usage by several megabytes before jumping back down to the main trend line.\nWe want to take these measurements, because one of the facts of Fluvio is that it is developed in Rust instead of a JVM language, which has a reputation of requiring memory tuning for their applications. As such, it is imperative that we show our memory usage.\nI am unaware of any official memory usage benchmarks from Kafka or Pulsar to compare against, but we wanted to capture where we are now so we can follow up in the future.\nMax Memory usage MB Fluvio 21 CPU Just out of curiosity, we took measurements of CPU utilization during the tests too.\nMax CPU usage % Fluvio 315 Conclusion Our memory footprint is fairly low compared to a JVM application. But our latency for sending records to and from a Fluvio cluster are consistent.\nWe will continue to improve Fluvio, and report back. You can join our journey by signing up for Fluvio Cloud, trying out Fluvio locally or giving us some Github stars!\n","description":"","keywords":null,"summary":"Fluvio is a distributed streaming platform written in Rust. One of our primary goals with Fluvio is to provide excellent performance, especially with regard to latency.\nHowever, until now we haven\u0026rsquo;t had a comprehensive benchmarking suite to help us pinpoint exactly where we stand in terms of concrete numbers. We\u0026rsquo;ve been hard at work building up our benchmark game, so in this post we\u0026rsquo;re going to talk about what we\u0026rsquo;ve discovered about where we are and what we\u0026rsquo;ve learned about where we can improve.","title":"First look at Fluvio's Performance in Development","url":"http://localhost:1315/blog/2021/07/fluvio-perf-dev-first-look/"},{"body":"Fluvio is a high-performance, distributed streaming platform for real-time data. Lately, we\u0026rsquo;ve been working hard on our most exciting feature yet: the ability to write custom code that operates inline on your streaming data. We call this feature SmartModules, and it\u0026rsquo;s powered by WASM to be as lightweight and high-performance as possible. In this blog, I want to dive in and talk about how to get started writing your own SmartModule, and how to install it into Fluvio to process your streaming data.\nHere\u0026rsquo;s a sneak peek at what we\u0026rsquo;ll be doing with SmartModules in this blog: filtering server logs by log-level!\nYou can check out the full code for this blog in the SmartModule examples repo!\nFilters The simplest type of SmartModule is a filter, which can examine each record in a stream and decide whether to accept or reject it. All records that are accepted by a filter will be delivered down the pipeline to the consumer, but records that are rejected will be discarded from the stream.\nNote that this does not mean that records are deleted from the partition they are persisted in, it simply means that those records are not delivered to the consumer.\nSome good use-cases for filters include:\nFiltering application logs based on log-level (explored in this blog), Detecting and filtering out records containing Social Security numbers, Selecting a subset of records based on a user or group ID. Example use-case: Filter Records by JSON fields For this example, we\u0026rsquo;re going to work with streams of JSON data, and we\u0026rsquo;re going to filter our records based on the contents of specific JSON fields. SmartModules are written using arbitrary Rust code, so we can also pull in other crates as dependencies. We\u0026rsquo;re going to use serde and serde_json to help us work with our JSON data. If you want to jump ahead and see the finished code, check out our JSON filter example .\nCreate a new Project SmartModules require some special build configurations, so to make it easy to get started we created a cargo-generate template with all the setup already done. You can install cargo-generate using cargo install:\n%copy first-line%\n$ cargo install cargo-generate Now let\u0026rsquo;s use cargo-generate to set up our new SmartModule project. We\u0026rsquo;ll want to give the project a name and choose the \u0026ldquo;filter\u0026rdquo; option.\n%copy first-line%\n$ cargo generate --git https://github.com/infinyon/fluvio-smartmodule-template ⚠️ Unable to load config file: ~/.cargo/cargo-generate.toml 🤷 Project Name : log-level 🔧 Generating template ... ✔ 🤷 Which type of SmartModule would you like? · filter [1/7] Done: .cargo/config.toml [2/7] Done: .cargo [3/7] Done: .gitignore [4/7] Done: Cargo.toml [5/7] Done: README.md [6/7] Done: src/lib.rs [7/7] Done: src 🔧 Moving generated files into: `log-level`... ✨ Done! New project created log-level Alright, now that we have our setup all ready, let\u0026rsquo;s talk about what we\u0026rsquo;re going to be filtering.\nThe Data: Server Logs Suppose we have a web server that accepts HTTP requests from clients, does some stuff, and then returns a response. It is common for such servers to have an application logging system where they report various events taking place within the server so that it may be monitored. We can imagine that this web server is exporting logs to Fluvio via a producer, and that the logs are formatted as JSON describing the event that occurred.\nFor the purposes of this exercise, let\u0026rsquo;s say we have a file that we\u0026rsquo;ve stored our logs into, so that we can manually produce them to a Fluvio topic and consume them back using our JSON SmartModule. Create a file called server.log with the following contents:\n$ cat server.log {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Server listening on 0.0.0.0:8000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Deserializing request from client\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client request deserialized\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Connecting to database\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client dropped connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Deserializing request from client\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client request deserialized\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Connecting to database\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Unable to connect to database\u0026#34;} Each line in this file represents one event that occurred in our server. We can see that each event is tagged with a \u0026ldquo;level\u0026rdquo; describing the significance of the event, and a \u0026ldquo;message\u0026rdquo; with a description about what happened. This style of rating logs with different levels is a common pattern in application logging, and we\u0026rsquo;re going to use it as the basis of our filter. Specifically, we\u0026rsquo;re going to write a filter that excludes all \u0026ldquo;debug\u0026rdquo; log, but accepts any \u0026ldquo;info\u0026rdquo;, \u0026ldquo;warn\u0026rdquo;, or \u0026ldquo;error\u0026rdquo; logs. In a real-world scenario, this could dramatically help reduce the traffic and noise in the logs if we were to consume these records into an analytics platform for inspection.\nThe Code: Writing our Filter Let\u0026rsquo;s look at the starter code crated by the Filter generator:\n%copy first-line%\n$ cd log-level \u0026amp;\u0026amp; cat src/lib.rs We should see the following code:\n// src/lib.rs use fluvio_smartmodule::{smartmodule, Result, Record}; #[smartmodule(filter)] pub fn filter(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;bool\u0026gt; { let string = std::str::from_utf8(record.value.as_ref())?; Ok(string.contains(\u0026#39;a\u0026#39;)) } The Record type contains the binary data for a single event in our topic. In our case, this will be a UTF-8 encoded string that is also a valid JSON value. The first step we\u0026rsquo;ll need to take is to parse our Record as JSON so that we can inspect it and determine what level the log is. We can use serde\u0026rsquo;s derive feature to define types that represents our log data.\n%copy%\n#[derive(PartialEq, Eq, PartialOrd, Ord, serde::Deserialize)] #[serde(rename_all = \u0026#34;lowercase\u0026#34;)] enum LogLevel { Debug, Info, Warn, Error } #[derive(serde::Deserialize)] struct StructuredLog { level: LogLevel, #[serde(rename = \u0026#34;message\u0026#34;)] _message: String, } We\u0026rsquo;re using #[derive(serde::Deserialize)] to implement Deserialize for our types, which will allow us to convert our raw data into instances of StructuredLog. We have also defined a LogLevel enum that implements Deserialize as well as Ord, or \u0026ldquo;Ordering\u0026rdquo;. When deriving Ord for an enum, the variants may be compared to one another using \u0026lt; and \u0026gt;, where later-defined variants are \u0026ldquo;greater than\u0026rdquo; earlier-defined variants. In other words, we have LogLevel::Error \u0026gt; LogLevel::Debug and so on for each pair of LogLevels. Notice also that we have defined a field for our logs\u0026rsquo; messages, but it is unused (which is why it is named _message). This is because our filter will not care about the message in the log, just the level. However, by including it in our StructuredLog definition, we can be sure that all logs that we pass through the filter do indeed have a \u0026ldquo;message\u0026rdquo; field. In this way, our filter is also acting as a sort of schema validator, only accepting records that properly conform to the shape that we expect.\nNow, let\u0026rsquo;s write the logic for our filter. We\u0026rsquo;ll start by parsing our raw data into instances of StructuredLog.\n%copy%\nuse fluvio_smartmodule::{smartmodule, Record, Result}; #[smartmodule(filter)] fn filter(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;bool\u0026gt; { let log: StructuredLog = serde_json::from_slice(record.value.as_ref())?; todo!() } Here, we\u0026rsquo;re parsing our input data from JSON into an instance of our StructuredLog struct that we defined earlier. If the parsing fails, the ? after serde_json::from_slice says that we\u0026rsquo;ll return the Err, sort of like throwing an exception. When parsing succeeds, we receive an instance of our StructuredLog struct.\nNow for the final step, we want our filter to accept all records except for \u0026ldquo;debug\u0026rdquo; logs. In other words, we actually want to keep the records that are \u0026ldquo;more important\u0026rdquo; or \u0026ldquo;greater than\u0026rdquo; LogLevel::Debug. Since we have implemented Ord for LogLevel, this will be a piece of cake! Let\u0026rsquo;s look at all the code for the finished filter.\n%copy%\nuse fluvio_smartmodule::{smartmodule, Record, Result}; #[derive(PartialEq, Eq, PartialOrd, Ord, serde::Deserialize)] #[serde(rename_all = \u0026#34;lowercase\u0026#34;)] enum LogLevel { Debug, Info, Warn, Error } #[derive(serde::Deserialize)] struct StructuredLog { level: LogLevel, #[serde(rename = \u0026#34;message\u0026#34;)] _message: String, } #[smartmodule(filter)] fn filter(record: \u0026amp;Record) -\u0026gt; Result\u0026lt;bool\u0026gt; { let log: StructuredLog = serde_json::from_slice(record.value.as_ref())?; // We keep records that are \u0026#34;greater than\u0026#34; debug Ok(log.level \u0026gt; LogLevel::Debug) } Let\u0026rsquo;s make sure our code compiles.\nIf you\u0026rsquo;ve never compiled for WASM before, you\u0026rsquo;ll need to install the proper rustup target. You should only need to do this once.\n%copy first-line%\n$ rustup target add wasm32-unknown-unknown We\u0026rsquo;ll use release mode in order to get the smallest and fastest binary possible. We should be able to see the .wasm file appear in the target directory.\n%copy first-line%\n$ cargo build --release Compiling log-level v0.1.0 (/home/user/log-level) Finished release [optimized] target(s) in 2.33s %copy first-line%\n$ ls -la target/wasm32-unknown-unknown/release .rwxr-xr-x 135Ki user 19 May 13:29 log_level.wasm Test Drive: Producing and Consuming the Data Now that we\u0026rsquo;ve written our filter, let\u0026rsquo;s play with some data and make sure we get the results we expect! If you haven\u0026rsquo;t installed Fluvio yet, head on over and download the Fluvio CLI and then follow the getting started guide for your OS, or sign up for Infinyon Cloud for a free account and a hosted cluster.\nOnce we have our CLI and cluster all set up, we\u0026rsquo;ll start by creating a new topic where we\u0026rsquo;ll produce our data.\n%copy first-line%\n$ fluvio topic create server-logs topic \u0026#34;server-logs\u0026#34; created In order to see the impact of our SmartModule filter, let\u0026rsquo;s open two terminals, with each running a consumer that watches our server-logs topic. One of these will be a plain consumer that consumes all the records, and the other one will use our filter, so we should only see non-debug logs.\nTo run the plain consumer, use the following command:\n%copy first-line%\n$ fluvio consume server-logs -B In the other terminal, run a consumer with the SmartModule filter using this command:\n%copy first-line%\n$ fluvio consume server-logs -B --filter=\u0026#34;target/wasm32-unknown-unknown/release/log_level.wasm\u0026#34; Finally, we can take our server.log file and use fluvio produce to send each line of the file as one record to our topic. In a third terminal, run the following command to produce the server logs to our topic:\n%copy first-line%\n$ fluvio produce server-logs -f server.log In the plain consumer, we should see all the records get passed through:\n%copy first-line%\n$ fluvio consume server-logs -B {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Server listening on 0.0.0.0:8000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Deserializing request from client\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client request deserialized\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Connecting to database\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client dropped connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Deserializing request from client\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client request deserialized\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;debug\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Connecting to database\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Unable to connect to database\u0026#34;} But in the consumer with our SmartModule, we\u0026rsquo;ll no longer see any of the records whose log level was debug!\n%copy first-line%\n$ fluvio consume server-logs -B --filter=\u0026#34;target/wasm32-unknown-unknown/release/log_level.wasm\u0026#34; {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Server listening on 0.0.0.0:8000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Client dropped connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Accepted incoming connection\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Unable to connect to database\u0026#34;} At this point, feel free to play around with the filtering logic and try out SmartModule on your own sample data!\nConclusion We hope this blog was able to give you a good hands-on feeling for what SmartModules are all about. We have a lot of exciting plans for the future, such as SmartModules that can transform data (mapping) in addition to filtering. Be sure to stay tuned for future posts as we roll out these features in upcoming releases!\nUpdate: SmartModules were originally called SmartStreams. The blog was updated to reflect the new naming convention\nFurther reading Read about Fluvio: the Programmable Data Platform Learn more about Fluvio\u0026rsquo;s SmartModules Create a free Infinyon Cloud account ","description":"How to write custom filtering code to run inline over your streaming data.","keywords":null,"summary":"Fluvio is a high-performance, distributed streaming platform for real-time data. Lately, we\u0026rsquo;ve been working hard on our most exciting feature yet: the ability to write custom code that operates inline on your streaming data. We call this feature SmartModules, and it\u0026rsquo;s powered by WASM to be as lightweight and high-performance as possible. In this blog, I want to dive in and talk about how to get started writing your own SmartModule, and how to install it into Fluvio to process your streaming data.","title":"Write a WASM-based filter for application logs","url":"http://localhost:1315/blog/2021/06/smartmodule-filters/"},{"body":"In the last few years, organizations started to adopt stream processing architectures to power a new generation of data-driven services that can detect events, predict behaviors, and respond to customer demand in real-time. As these early pilots become production-ready services, organizations gradually expand these stream processing and analytics pipelines to other services. Moreover, as data volumes double every few years, organizations that extract valuable and relevant business signals in the shortest amount of time gain a significant competitive advantage. We believe organizations that choose data streaming technology will enjoy an impactful, long-lasting competitive advantage.\nYesterday\u0026rsquo;s Monolithic Stream Processing Platforms Most stream processing frameworks available today - Kafka, Pulsar, Flink, Spark, etc. - were born in the Big Data era and designed as monolithic platforms that require sizeable specialized staff to deploy, operate and maintain. Some admin operations such as setting up data sharing or re-balancing stream after config update require an IT ticket to be handled by the operation team.\nThese Java-based stream processing platforms assume a homogenous and monolithic enterprise development environment of Y2000 where one language rules it all. Some have reluctantly added partial support for Python. Other languages such as Node, Go, Ruby offer a subset of functionality in independent client libraries. However, Java-derived languages remains the only reliable way to customize stream processing. This barrier makes it difficult for many non-Java developer communities to leverage the power of real-time stream processing. Github expects 100M developers by 2025; most of them will be new developers and will not be familiar with Java.\nSoftbank estimates over 1 trillion devices connected on the Internet of things by 2025 driven by wearables, drones, self-driving cars, inter-connected devices, and more. These networks need stream processing for immediate feedback and real-time analytics for mission-critical decisions. Java-based systems demand significant CPU and memory resources, making them unsuitable for extending stream processing to edge devices.\nOne of the most significant drawbacks of Java-based stream processing frameworks is the Jar wrapper required for distribution. Jars were designed at the dawn of the Internet when Browsers were rudimentary HTML readers and programs required runtime applets (aka. sandboxes) to operate. These sandboxes have been riddled with security vulnerabilities, and new browsers are gradually deprecating them. Some frameworks resorted to container technologies such as Docker to add another layer of isolation and a workaround for dynamic loading. Unfortunately, the container introduces another layer of security issues and introduces more latencies and cold-startup time.\nThe lack of adequate tooling available in the market makes the journey to real-time data stream processing challenging, error-prone, and packed with customizations often reserved for organizations with highly skilled architects and a virtually unlimited budget.\nDemocratizing data-in-motion Companies are striving to accelerate digital transformation and become agile data-driven organizations. Yet data is a precious asset often locked down in data lakes or specialized data silos and managed by data teams responsible for storage and safekeeping. As a result, data users lack visibility on what data is available to them and must wait on a lengthy approval process to gain access. This segregated approach to data hinders learning, prevents fast-paced innovation, and ultimately slows down the pace of the business. The Data Mesh white paper written by Zhamak Dehghani explains how current data paradigms are ill-suited for modern organizations.\nMonolithic data lakes and data silos should be divided into data domains and managed by decentralized teams. These data owners treat data as products and manage the data lifecycle end-to-end. They are responsible for data discovery, quality, and the SLA required by data consumers. This level of autonomy is critical for stream processing, where teams are responsible for generating actionable signals in real-time.\nDemocratized stream processing requires a self-serviced operational model on top of shared infrastructure. In this model, an infrastructure maintains the shared infrastructure, and the data domain team manages the data. The infrastructure team scales on-demand and re-balances the data dynamically across entire organizations. The data domain teams operate their data stream products independently and export interfaces as needed.\nA modern streaming platform must have the following attributes to meet these conditions:\nCloud-Native by design Cloud-native-based infrastructure is a loosely coupled system where each component can run and scale dynamically. As a result, these systems are well suited for dynamic platforms such as public, private clouds. The cloud-native streaming platform offer:\nHorizontal scale - to meet data elasticity requirements. Self-healing - to recover from failures without human intervention. Declarative management - to reduce the management burden. Kubernetes native - to plug-in native in K8 environments. Small footprint and resource-efficient The data-at-motion stream processing must handle an order of magnitude higher data than the products storing data-at-rest. Consequently, stream process platforms must be small enough to boot within milliseconds and operate efficiently on any system architecture. Moreover, it must support a variety of deployments from small organizations to large enterprises. The ideal platform has:\nSmall memory footing - to save on cloud resources and run on IoT devices. Low latency - to meet real-time latency requirements. Leverage multi-core CPU architecture - to operate at maximum performance. Fully event-driven with async architecture - to support large I/O. Support Data protection and isolation Shared infrastructures require a new level of security and privacy protection. In today\u0026rsquo;s zero-trust environment, data centers, clouds, and edges are considered insecure by default. Products for data-in-motion must segregate data streams and isolate users and teams from each other. Fine-grained context-area rules are used to define:\nRoles - to limit user access based on their role in a group and organization. Geo-Locations - to restrict access based on geographical location. Identities - to recognize and process data based on their identity. Stream processing engines with access to data must have a robust sandboxing environment that can enforce access control and protect data records from impacting each other.\nFull featured data APIs All companies are becoming technology companies. Since each team is the owner of their data, they must have API and tools to automate the data product. The self-service stream processing platform must offer granular APIs for developers who want to build robust real-time data services and ease-to-use tools for non-technical users. While SQL-based tools may be adequate for querying data-at-rest or a data lake, they offer limited functionality for developers who need full API access for automation. Data owners need the ability to\nCustomize and manage Data Lifecycle. Orchestrate long-running data process. Assign declarative API for stream processing. Development APIs must be available in many widely used programming languages such as Node/JavaScript, Python, Go, Ruby, etc.\nSupport for data governance Self-service and de-centralization allow teams to become independent data owners. However, to leverage the team\u0026rsquo;s data as a whole, self-service infrastructure must implement federal governance to aggregate and correlate data cross-organization with policy enforcement. Consumers and regulatory agencies have raised expectations for data protection and securities by regulating access to PII and imposing consumer data protection policies such as GDPR.\nThe shared platform is well-positioned to implement federal data governess by baking consistent interoperability and policy standards across teams in conjunction with common access control and audit trails.\nFluvio: Programmable Platform for Data-in-motion At Infinyon, we are building Fluvio as a purpose-built stream processing platform for data-in-motion. Although the platform starts with similar standard stream processing functionalities such as consumer and producer as with legacy Java-based stream processing frameworks, Fluvio’s performance, scalability, deployment flexibility, and programmability allow building the data-in-motion infrastructure of the future.\nPowered by Rust We start with a strong foundation; The Rust language powers Fluvio. Rust, a modern programming language built for speed, low overhead, cross-platform interoperability, and code safety. AWS, Mozilla, Google, Facebook, Discord, Dropbox, and others use Rust to create a new class of high-performance products, such as browsers, chat servers, network proxies, database servers, real-time systems, and more.\nFluvio needs to perform stream processing at a massive scale. By choosing Rust, we gained the following benefits:\nPerformance by Default Rust compiles to native code for blazing-fast performance. Without Garbage Collector pauses, Rust can process streams with very low consistent latency. The language implements a zero-cost async framework capable of handling many concurrent I/O streams with minimum CPU usage. Rust developers can write high-level functional code similar to Java and Python without rolling out hand-crafted machine code to get performance. Performance and zero-cost cost abstraction make Rust an ideal language for data-in-motion.\nSafety by Default Rust is safe by default programming language, unlike any other language such as C, C++, Go, or other. It performs many safety checks during compile time instead of discovering fault during production. Rust doesn\u0026rsquo;t allow NULL, which happens to be one of the worst in the software mistakes. With borrow checker, Rust prevents buffer overflow and dangling pointer that malicious hackers could take advantage of. Microsoft stated that memory safety issues cause 70% of CVE in Windows. The borrow checkers also check for concurrent logic, which is essential for creating scalable stream processing infrastructure\nSafety and Performance of Rust enable the Fluvio community to ship a robust, high-performance data streaming platform from day one.\nCloud-Native Control Plane Fluvio’s control plane took its inspiration from Kubernetes. Fluvio manages its software components using declarative programming with eventual consistency. With declarative management, the user specifies intent, and the platform attempts to fulfill the request by feedback loop monitoring controllers. For example, wait for additional resources, re-balance workloads, perform reconciliation, trigger self-healing from hardware or software failures, and more.\nFluvio objects can be provisioned through various mechanisms: Kubernetes (kubectl) commands, Fluvio CLI, Infinyon Cloud, or programmatic admin API available in all supported programming languages.\nSmartModules: Programmable Stream Processing At the heart of all stream-processing frameworks is the Stream Processor. SP treats a stream as the core unit of work. Most of the legacy SP uses a fixed pipeline, which only implements fixed subsets of data-in-motion needs:\nIngestion Persistence Transmission Dispatching Computation (filters, maps, aggregates, joins, derivates) Fluvio’s SPU (Stream Processing Unit) comes with a revolutionary programmable stream pipeline. Data-in-motion pipelines need more customization than data-at-rest. For example, filtering and cleaning data, idempotent producers, and others. With a Java-based streaming framework, it is challenging to provide a programmable pipeline with performance and security.\nProgrammability by WebAssembly SPU implements programmability by integration with WebAssembly technology. WebAssembly is a portable binary-code format designed to run in a secure sandbox. It is proven W3 technology to bring programmability to software such as Envoy Proxy, Cloudflare worker, Microsoft flight simulator, CDN proxies, and more. SmartModules bring WebAssembly technology to real-time data streaming offering an unprecedented level of customization.\nSPU\u0026rsquo;s programmability capabilities eliminate the need to stitching together multiple clusters, as seen in other data streaming platforms. Smart pipelines runs at native speed, decreasing delays, increasing security, and reducing operational complexity.\nSecure by Default SmartModules separates user stream operations from system stream. All user stream operations are executed as a WebAssembly module in a protected sandbox with separate memory space for user operations. Since user modules can only access data supplied by SmartModules, it can\u0026rsquo;t access or modify protected information as PII data.\nFast Inline Computation With processing time measured in low milliseconds, SmartModules offer the fastest and most convenient way to manipulate data in motion. A Java-based system performing a similar operation in memory-hungry JARS would see delays from 10 to 100 fold higher with garbage collection and out-of-band management. A container-based system performing a similar function has a significantly higher image size and an out-of-band communication channel, increasing startup time and introducing communication overhead.\nSupport any development language WebAssembly supports any language with bindings to the LLVM toolchain - Rust, JavaScript, Python, Ruby, and Go. Fluvio offers abstractions, templates, utilities, and tools to make it easy and convenient to build and customize stream processing modules.\nWe believe WebAssembly technology is the key to building high-performance, customizable data streaming platforms.\nSolomon Hykes, the creator of Docker, said the following:\nIf WASM+WASI existed in 2008, we wouldn\u0026rsquo;t have needed to created Docker. That\u0026rsquo;s how important it is. WebAssembly on the server is the future of computing. A standardized system interface was the missing link. Let\u0026rsquo;s hope WASI is up to the task!\nConclusion Infinyon\u0026rsquo;s mission is to accelerate the world\u0026rsquo;s transition to the real-time economy.\nProgrammable Data: Cancelling Data Gravity Democratizing data is really about overcoming data\u0026rsquo;s gravity well. Unless it moves, data sits in silos and accumulates gravity. Silo data are difficult to move due to physical storage limitations and fear of exposing ourselves to security breaches. The key to canceling data gravity is programmability. When we apply programmability to data as it moves between services, we can protect, enrich, track and extract information in real-time. Data-in-motion will gradually become an intelligence layer that connects the organization - people, tools, and services. Programmable data, a simple concept that will change the way we manage data.\nFluvio is Open Source. Join Us Fluvio is an open-source project, and we are committing to make it accessible for everyone. We are at the beginning of our journey. Join us in building the next-generation platform for data in motion. Whether you have feedback, ideas, suggestions, and want to become a contributor, reach out. You can find us on Github and in Discord → We look forward to seeing you soon.\n","description":"Fluvio is a blazing-fast programmable streaming platform for data-in-motion. Apply your own custom logic to protect, normalize, and enrich the data as it moves between services.","keywords":null,"summary":"In the last few years, organizations started to adopt stream processing architectures to power a new generation of data-driven services that can detect events, predict behaviors, and respond to customer demand in real-time. As these early pilots become production-ready services, organizations gradually expand these stream processing and analytics pipelines to other services. Moreover, as data volumes double every few years, organizations that extract valuable and relevant business signals in the shortest amount of time gain a significant competitive advantage.","title":"Introducing Fluvio: The Programmable Data Platform","url":"http://localhost:1315/blog/2021/06/introducing-fluvio/"},{"body":"At InfinyOn we are seeing an increasing amount of traffic in Infinyon Cloud. Given our current projections we anticipate over 1 million edges to connect our platform in the next 12-18 months. One of bottlenecks is the compute resources needed to process ingress traffic.\nWe have a couple of places in our cloud offering where we needed to intercept TCP network traffic, perform some analysis on the first packet and make a decision on where to route the rest of the TCP stream.\nInfinyon Cloud provides a fully managed solution for running Fluvio, an open source streaming platform for real-time applications. Low-latency and high-thoughput is a key benefit of using Fluvio, therefore Infinyon Cloud needs to provide infrastructure that can keep up with the traffic that customers send and receive from their Fluvio Cluster. To remain compitetive we need to maximize the efficiency of how our software uses hardware resources. Simply throwing hardware at the problem is not acceptable for us at scale.\nLet\u0026rsquo;s get started In various cases, the first packet may contain metadata which determine the destination endpoint or authentication data which may determine whether to terminate the connection. We can approach this by building a reverse proxy but with custom logic. The constraints that we need to consider are that we run on Linux in a container and our application is written in Rust.\nIn general, this can be accomplished in 6 steps:\nCreate a TCP socket Wait for a new connection Buffer and read the first packet Determine destination Open a TCP connection to the destination Forward traffic The naive way to do step 6 would be to forward packets by simultaneously copying packet payloads from the receive buffer of each socket to the send buffer of the other until either side terminates the TCP stream.\nExample code: %copy%\nlet listener = TcpListener::bind(\u0026#34;127.0.0.1:10000\u0026#34;).unwrap(); let mut stream = listener.accept().await.unwrap(); let mut buf = [0u8; 16]; loop { let bytes_read = stream.read(\u0026amp;mut buf).await.unwrap(); if bytes_read == 0 { break; } else { stream.write(\u0026amp;buf).await.unwrap(); println!(\u0026#34;Echoed from userspace: {}\u0026#34;, String::from_utf8_lossy(\u0026amp;buf)); } } Why is this not good enough? The naive approach concerns us since we need to handle large amounts of traffic. Let\u0026rsquo;s take a deeper look at what happens when we receive data that needs to be forwarded. The application, in a loop, will perform a read syscall to the linux kernel to read data that was received on the socket. This will copy the data from the read buffer in kernel-space to a buffer in user-space. Then, once again the application will perform a write syscall to copy data from the previously filled user-space buffer to the kernel-space write buffer of the other socket. By the way, none of this is affected by whether blocking or non-blocking IO is used.\nThe copying of data from kernel-space to user-space and back is wasteful usage of memory bandwidth, however just the cost of performing the two syscalls is a significant cause for concern. Syscalls add latency due to the switch between user mode and kernel mode which saves and restores CPU register values to and from the stack in memory.\nThe copying of data between TCP sockets is not a new problem, applications such as proxies and load balancers have had this need for a long time. Some of them have used techniques such as using the splice syscall to pipe data between sockets without copying to a user-space buffer. However this only solves the problem of copying memory, as this still requires two syscalls to forward a packet.\nA novel approach The good news is that there is a solution to this problem which avoids both of the costs mentioned above. The solution is to use an eBPF program to handle the forwarding of packets between sockets. eBPF is a framework for userspace applications to run sandboxed programs inside kernel space without modifying the kernel or loading kernel modules. These programs can be attached to various kernel resources waiting to be triggered by events such as a socket receiving data.\neBPF Stream Parser and Sockmap One way to leverage eBPF for our use case is to create a BPF Sockmap and attach a program to it which will redirect packets to the proper destination socket. The Sockmap functions as a list of sockets, the mapping of source to destination sockets will need to be defined as a separate BPF map. The attached program will be executed when any socket in the map receives data.\nOur application just needs to create a BPF Sockmap and a BPF generic map, which we will refer to as the destination map. Then we attach a program to the Sockmap to route packets according to the destination map. Once our application determines the destination for the incoming connection, it adds the socket file descriptor to the Sockmap along with an entry in the destination map. At this point all incoming packets will be processed and forwarded by the eBPF program and our application in user-space is not involved in handling any of the data for the lifetime of this connection. The only thing left is to remove the map entries once the socket is disconnected.\nDoes it really work? At this point we want to demonstrate that we can write an application in Rust that uses the eBPF functionality described above. To do this we wrote a simple TCP echo server where the user-space application only handles the initial connection and uses the techniques mentioned above to echo all packets back to the source.\nThere is a series of crates under the RedBPF project that handle the grunt work of compiling the eBPF program and provide an API for us to load and attach the eBPF program in our application. Typically eBPF programs are written in C and loaded with the BCC toolchain. With RedBPF we write our eBPF code in Rust and use the safe Rust API provided by RedBPF to load and attach the eBPF program.\nExample eBPF Program and Map: We will put the part of our application that run in eBPF in one crate which we will name echo-probe. RedBPF refers to all eBPF programs as probes, so we shall follow this convention for now.\n%copy%\n#![no_std] #![no_main] use core::mem; use core::ptr; use redbpf_probes::sockmap::prelude::*; program!(0xFFFFFFFE, \u0026#34;GPL\u0026#34;); #[map(link_section = \u0026#34;maps/sockmap\u0026#34;)] static mut SOCK_MAP: SockMap = SockMap::with_max_entries(1); #[stream_parser] fn parse_message_boundary(sk_buff_wrapper: SkBuff) -\u0026gt; StreamParserResult { let len: u32 = unsafe { (*sk_buff_wrapper.skb).len }; Ok(StreamParserAction::MessageLength(len)) } #[stream_verdict] fn verdict(sk_buff_wrapper: SkBuff) -\u0026gt; SkAction { let index = 0; match unsafe { SOCK_MAP.redirect(sk_buff_wrapper.skb as *mut _, index) } { Ok(_) =\u0026gt; SkAction::Pass, Err(_) =\u0026gt; SkAction::Drop, } } In the above example we are using a Stream Verdict program to redirect the packet back out throught the same socket. This is easy since the only socket FD in the sockmap is the one of the incoming connection, so we use an index of 0.\nIn order to attach the Stream Verdict program we must also attach a Stream Parser program, this is a requirment of the API. In this example we do not need to inspect or modify the socket buffer, so we just need to return back the length of the buffer in the Stream Parser program.\nExample User-Space Application: We will put our application in a separate crate names echo. In this crate we need to include custom build login in build.rs. This uses the cargo-bpf crate to compile our echo-probe crate to eBPF code. Internally cargo-bpf uses rustc but with specific flags. The output is emitted to the output dir of this crate.\nbuild.rs: %copy%\nuse std::env; use std::path::{Path, PathBuf}; use cargo_bpf_lib as cargo_bpf; fn main() { let cargo = PathBuf::from(env::var(\u0026#34;CARGO\u0026#34;).unwrap()); let target = PathBuf::from(env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap()); let probes = Path::new(\u0026#34;../echo-probe\u0026#34;); cargo_bpf::build(\u0026amp;cargo, \u0026amp;probes, \u0026amp;target.join(\u0026#34;target\u0026#34;), Vec::new()) .expect(\u0026#34;couldn\u0026#39;t compile probes\u0026#34;); cargo_bpf::probe_files(\u0026amp;probes) .expect(\u0026#34;couldn\u0026#39;t list probe files\u0026#34;) .iter() .for_each(|file| { println!(\u0026#34;cargo:rerun-if-changed={}\u0026#34;, file); }); println!(\u0026#34;cargo:rerun-if-changed=../echo-probe/Cargo.toml\u0026#34;); } In the main loop we embed the compiled eBPF bytecode as data into our userspace application using include_bytes!. Then we load the eBPF resources, attach the sockmap, and we are ready to populate the sockmap with the incomming connection.\nmain.rs: %copy%\nuse std::os::unix::io::AsRawFd; use futures_lite::{AsyncReadExt, AsyncWriteExt}; use glommio::net::{TcpListener}; use glommio::prelude::*; use redbpf::load::Loader; use redbpf::SockMap; fn main() { let server_handle = LocalExecutorBuilder::new().spawn(|| async move { let loaded = Loader::load(probe_code()).expect(\u0026#34;error loading BPF program\u0026#34;); let bpf_map = loaded.map(\u0026#34;sockmap\u0026#34;).unwrap(); // Reference to the sockmap, which we can add and remove sockets from let mut sockmap = SockMap::new(bpf_map).unwrap(); // Attach the sockmap to the stream parser program loaded .stream_parsers() .next() .unwrap() .attach_sockmap(\u0026amp;sockmap) .expect(\u0026#34;error attaching sockmap to stream parser\u0026#34;); // Attach the sockmap to the stream verdict program loaded .stream_verdicts() .next() .unwrap() .attach_sockmap(\u0026amp;sockmap) .expect(\u0026#34;error attaching sockmap to stream verdict\u0026#34;); let listener = TcpListener::bind(\u0026#34;127.0.0.1:10000\u0026#34;).unwrap(); println!( \u0026#34;Server Listening on {}\u0026#34;, listener.local_addr().unwrap() ); let mut stream = listener.accept().await.unwrap(); // Add the socket of the new connection to the sockmap // This is where the magic happens sockmap.set(0, stream.as_raw_fd()).unwrap(); println!( \u0026#34;Sockmap set fd {}\u0026#34;, stream.as_raw_fd() ); loop { let mut buf = [0u8; 16]; let b = stream.read(\u0026amp;mut buf).await.unwrap(); if b == 0 { break; } else { stream.write(\u0026amp;buf).await.unwrap(); println!(\u0026#34;Echoed from userspace: {}\u0026#34;, String::from_utf8_lossy(\u0026amp;buf)); } } }).unwrap(); server_handle.join().unwrap(); } fn probe_code() -\u0026gt; \u0026amp;\u0026#39;static [u8] { include_bytes!(concat!( std::env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/target/bpf/programs/echo/echo.elf\u0026#34; )) } Unfortunately, the RedBPF crates require us to write a couple parts of our eBPF code in unsafe blocks. We are happy to see that since we initially prototyped this echo server (February 2021), RedBPF has added better support for Sockmap and Stream Parser program types, which allowed us to remove many instances of unsafe code blocks from our example echo server. However, there is still room for improvment since we are forced to use unsafe due to the lack of safe abstractions provided by RedBPF.\nFull example source code is available at github.com/nacardin/ebpf-proxy\nNow what? An important fact to note is that the user-space application must be run as root in order to load and attach eBPF programs. The alternative is to set the unprivileged_bpf_disabled systl option to 0, this is a system-wide change and requires security consideration.\nThere are other ways to leverage eBPF, such as with XDP, which may even have better performance as it is triggered before a kernel socket buffer is allocated. However this requires the NIC driver to support this feature set. Also we would lose the ability to take action based on the contents of the packet. This may be an acceptable solution for us and requires further consideration.\nSo it seems we still have some research to do, so stay tuned\u0026hellip;\nMore about BPF Sockmap (lwn.net/Articles/731133)\nMore about performance of BPF Sockmap (blog.cloudflare.com/sockmap-tcp-splicing-of-the-future)\nInfinyon Cloud Infinyon Cloud is currently in alpha stage, and you can create a free account using the link below:\nTry Infinyon Cloud ","description":"Writing a routing application in Rust using eBPF to minimize syscalls and copying memory","keywords":null,"summary":"At InfinyOn we are seeing an increasing amount of traffic in Infinyon Cloud. Given our current projections we anticipate over 1 million edges to connect our platform in the next 12-18 months. One of bottlenecks is the compute resources needed to process ingress traffic.\nWe have a couple of places in our cloud offering where we needed to intercept TCP network traffic, perform some analysis on the first packet and make a decision on where to route the rest of the TCP stream.","title":"Routing traffic in Rust using eBPF","url":"http://localhost:1315/blog/2021/05/ebpf-routing-rust/"},{"body":"Lately on Fluvio, we\u0026rsquo;ve been doing a lot of work to improve our productivity and speed up our development cycles. One of the easiest and most effective things we\u0026rsquo;ve done in order to achieve this goal is to integrate the Bors-ng GitHub bot into our development workflow. In this post, I\u0026rsquo;ll talk about some of the problems we were facing, what Bors is and why it was a good solution for our team, and how adopting it has helped us to increase our development speed and confidence. I\u0026rsquo;ll then describe how to set up Bors for your own repository and what to expect out of the new development workflow.\nWhat even is Bors? Bors is a GitHub application/bot that you put in charge of merging your PRs into master. Importantly, it merges commits in such a way that ensures that the exact code that lands in master is the code that has been tested by your CI workflow, which, perhaps surprisingly, is not how typical merges work. The problem with classic merges is that they can cause problems such as semantic merge conflicts, in which two separate PRs each make changes that work in isolation, but which cause failures when they are merged together. This problem occurs because regular GitHub CI workflows are run on the PR\u0026rsquo;s branch before it\u0026rsquo;s merged into master, rather than after.\nTo solve this problem, Bors merges branches by first creating a staging branch at the head of master, then merging your branches into it. This creates a merged branch that is equivalent to what would previously have been pushed directly to master, except now there is an opportunity to run CI workflows on this already-merged branch to decide whether it should be accepted or rejected. If the CI workflow fails, then Bors simply does not update master. If CI passes, then Bors fast-forwards master to match the merged commit on the staging branch - the same exact commit which has already been tested. This style of CI workflow - merging, testing, then fast-forwarding - greatly increases our confidence in the correctness of the code living in master.\nHands-on: The Bors workflow To give you a more concrete sense of how Bors operates, let me walk you through the experience as a developer using Bors on a day-to-day basis. Essentially, we follow these steps when working on a PR:\nPush code changes to your branch and open a PR Ensure your CI jobs are in a passing state Get reviews and approvals from team members When ready to merge, write a comment with the text \u0026ldquo;bors r+\u0026rdquo; See how Bors creates a staging branch at master, then merges the PR into it.\nNotice that you still have to supply your own CI job definition, and that your CI may run each time new commits are pushed to a branch. The only difference in the development process is when it comes time to actually merge the branch. Instead of using GitHub\u0026rsquo;s big green \u0026ldquo;Merge/Squash/Rebase\u0026rdquo; button, we simply tell Bors that we think this PR is ready to merge.\nWhen we say \u0026ldquo;bors r+\u0026rdquo;, we tell Bors to add this PR to the \u0026ldquo;ready queue\u0026rdquo;. When there are one or more PRs in the ready queue, Bors will attempt to batch together all the ready PRs, merge them into the staging branch (which, remember, begins at the head of master), and run CI once again on the merged staging branch. Bors will watch the status of the CI jobs, and once all the required jobs have passed, it will push the staging branch to master, which is guaranteed to be a fast-forward.\nIncreased Productivity I want to touch on one of the nice side effects of using Bors to merge PRs. It has actually helped us to reduce the amount of time we spend on preparing and babysitting PRs. Prior to using Bors, one of the strategies we used to avoid semantic merge conflicts was to require all branches to be \u0026ldquo;up-to-date with master\u0026rdquo; before merging. This is enforceable by GitHub and essentially means that you need to rebase against master any time another change lands before yours does. Because of this, we would often find ourselves trapped in a vicious cycle:\nGet the PR tested, approved, and ready to go Get ready to press the Big Green Merge Button Find out another PR was merged first and need to rebase This was especially painful because after rebasing, we would need to once again wait on our CI jobs to pass and hope that we don\u0026rsquo;t get beaten to the merge again. One way to avoid this problem would have been to coordinate with team members before trying to merge PRs, but that requires more time and synchronization across the entire team, and does not scale well.\nUsing Bors allows us to sidestep these issues entirely by simply letting it manage the merging process. After sending \u0026ldquo;bors r+\u0026rdquo; on a PR, you can usually move on and work on the next thing without needing to keep it in the back of your mind. The exception to this is if Bors finds a merge conflict or semantic conflict between your PR and another one that came before yours in the queue. Note, however, that in this scenario you already would have needed to fix regular merge conflicts, and that Bors provides the benefit of notifying you when a semantic conflict causes a failure, which previously would have failed after reaching master rather than before.\nSetting up Bors on a GitHub repository Like I mentioned before, Bors is a GitHub bot, so setting it up is a pretty straightforward process. In order to use it, you\u0026rsquo;ll need to first add the Bors application to the GitHub account where the repositories you want to use it live. Then, you need to grant it access to some or all of the repositories in that account. You can start this process by visiting the Bors website and clicking \u0026ldquo;Log into dashboard\u0026rdquo;. This should prompt you to log in with GitHub or something similar. I have already added Bors to an account previously, so the steps you take may be slightly different from the ones I show, but they should be similar enough and very easy to follow.\nOn my dashboard, I already have the infinyon/fluvio repository added to Bors, but if you\u0026rsquo;re starting from scratch you will probably get a prompt right away to add Bors to an account and a repository. If you are adding a second repository like I am for this demonstration, there will be a Repositories page where you can add a new repository.\nThe first page should ask you which GitHub account or organization to add Bors to. You\u0026rsquo;ll want to select the account which owns the repository you want. Note that if this is an organization, you need to have the appropriate access within the organization to add an application.\nThe next page will prompt for whether you want to add Bors to all the repositories on the given account or just a specific one. I always recommend choosing specific access for things like this, in order to grant the least amount of privilege to tools where necessary. You can always come back and add new repositories if you really like Bors.\nAt this point, you should be done setting up the actual Bors application. All that\u0026rsquo;s left to do is set up the bors.toml configuration file in your repository to tell Bors which workflows to monitor, and to set some options to customize the behavior for Bors on that repo.\nConfiguring Bors with a simple CI workflow I\u0026rsquo;ve put together a small sample repository with a basic Rust project and a simple CI workflow using GitHub Actions. I\u0026rsquo;ll talk through the key options in the bors.toml file and how those options interact with the CI workflow.\nFirst, let\u0026rsquo;s look at the CI workflow we\u0026rsquo;re working with. This has a handful of jobs that should be useful for any Rust crate.\n# .github/workflows/ci.yml name: CI permissions: contents: read on: workflow_dispatch: pull_request: push: branches: - staging # Causes CI to run when Bors pushes to staging - trying # Causes CI to run when Bors pushes to trying (bors try) jobs: build: name: ${{ matrix.task.name }} (${{ matrix.os }}) runs-on: ${{ matrix.os }} strategy: matrix: os: [ubuntu-latest] rust: [stable] task: - name: Format run: cargo fmt - name: Clippy run: cargo clippy - name: Build run: cargo build - name: Test run: cargo test steps: - uses: actions/checkout@v2 - name: Install Rust ${{ matrix.rust }} uses: actions-rs/toolchain@v1 with: toolchain: ${{ matrix.rust }} profile: minimal override: true components: rustfmt, clippy - name: ${{ matrix.task.name }} run: ${{ matrix.task.run }} # This job should depend on all required jobs. # We will make Bors watch this job to tell whether to merge or not. done: name: Done needs: [build] runs-on: ubuntu-latest steps: - name: Done run: echo Done This workflow creates four jobs for building and testing our Rust crate, and creates one job that depends on all the other jobs passing. The reason for setting it up this way is that Bors needs to know which jobs must pass in order to merge to master, which must be specified by name. Unfortunately, GitHub\u0026rsquo;s default job naming scheme is somewhat nuanced and confusing, and it can be tricky to remember exactly how to specify those job names in the bors.toml. Instead of bothering to remember all of those rules, I like to just create one simple \u0026ldquo;Done\u0026rdquo; job that only runs when all the required jobs pass. Then, all we need to do is tell Bors to watch for the Done job to complete.\n# bors.toml status = [ \u0026#34;Done\u0026#34;, ] At this point, Bors should be up and ready to go. If you open a PR and comment \u0026ldquo;bors r+\u0026rdquo;, within a minute you should see a notice that \u0026ldquo;Bors has added a commit that references this pull request\u0026rdquo;. You can click on the yellow bubble next to that commit to view the status of the CI workflow that Bors is watching.\nBors config pro tips The bors.toml I showed above is the most minimal configuration you can use to get up and running with Bors. However, there are some other Bors options and GitHub repository options that we can use to make things nicer and more foolproof. I have a few big points I want to walk through:\nConfiguring Bors to use squash commits Disabling the Big Green Merge Button Specifying a minimum number of PR approvals Using Squash Commits A squash commit is a way of taking all the commits on a branch and \u0026ldquo;squashing\u0026rdquo; them down into one commit. This is very useful for minimizing the amount of noise in the commit history. Bors supports a form of commit squashing in which it does the following:\nFor every PR in the ready queue, it squashes that PR\u0026rsquo;s branch into one commit The PR title and description are used as the commit\u0026rsquo;s message It then cherry-picks each squashed commit into the staging branch To illustrate how merging versus squashing impacts your git history differently, I made five branches originating on master and used \u0026ldquo;bors r+\u0026rdquo; on all of their PRs at once. This first image shows the resulting history when using plain merges.\nAnd this next image shows the resulting history when using squash merges.\nWe tend to prefer the squash merges because of the tidier history, but you can decide for yourself which mode works best for you. To enable squash merges, simply set the use_squash_merge configuration in bors.toml:\n# bors.toml status = [ \u0026#34;Done\u0026#34;, ] use_squash_merge = true Disabling the Big Green Merge Button When we started considering the Bors workflow, we wanted to make sure that there was no way for developers (present or future) to get confused about the merging process. If we were going to be using Bors and closing PRs using the \u0026ldquo;bors r+\u0026rdquo; command, we did not want it to be possible for newcomers to use GitHub\u0026rsquo;s merge button to accidentally bypass the Bors merging process.\nWe discovered that we could practically disable the button by adding a branch protection rule requiring the \u0026ldquo;bors\u0026rdquo; status check. This is effective because the bors status check only passes when bors witnesses a passing CI workflow on staging. By that time, Bors will have merged and closed the PR, so the button will not be available anyway.\nIn repositories where you have administrator privileges, adding this branch protection rule will demote the Big Green Button into the Red Admin Override Button, but it is still a good visual indicator that you should not press the button. Plus, if you are an administrator, you probably know to use Bors instead.\nIn repositories where you do not have administrator privileges, the button is disabled completely.\nSpecifying a minimum number of PR approvals One big question we had before adopting Bors was: \u0026ldquo;Will this change the way that we need to do reviews?\u0026rdquo;. When looking at the Bors reference, it seemed almost like Bors was introducing its own review system. There are extra Bors commands such as \u0026ldquo;bors delegate+\u0026rdquo;, or \u0026ldquo;bors delegate=[list]\u0026rdquo; which seemed to allude to a custom reviewer flow.\nWhile we have not yet tested out how the \u0026ldquo;delegate\u0026rdquo; commands work (we have not needed to), we did find this tidbit in the reference that seemed to answer our question:\nrequired_approvals: Number of project members who must approve the PR (using GitHub Reviews) before it is pushed to master.\nPerfect. So we do not need to know any other Bors commands or complicated workflows in order to conduct our reviews. However, there is a small nuance to be aware of. If you are setting a minimum number of reviewers, you should use the configuration in bors.toml rather than a branch protection rule on GitHub. If you use only a branch protection rule, then Bors will inevitably encounter errors when it tries to merge a PR with zero approvals into a protected branch. This results in an ugly API error on the Bors console rather than a tidy Bors message.\nThe proper way to set a number of reviewers with Bors is with the required_approvals config in bors.toml:\n# bors.toml status = [ \u0026#34;Done\u0026#34;, ] use_squash_merge = true required_approvals = 1 Conclusion I hope you found this post interesting and useful, and I encourage you to try out Bors on your own repositories! It is freely available for open-source repositories but unfortunately the publicly-hosted instance does not work with private repositories. I will be writing a follow-up blog in the coming weeks about how we took Bors a step further and used it to create a fully automated release pipeline. If you\u0026rsquo;d like a sneak-peek at how that all works, feel free to check out our workflow configuration on Fluvio. I would also like to thank the team over at bors.tech for creating and maintaining such a wonderful and freely available tool!\nFeel free to reach out with any questions on Twitter, Reddit, or our Fluvio team Discord, I love to hear from interested readers!\n","description":"How a little GitHub bot keeps our Rust project passing CI in master 100% of the time.","keywords":null,"summary":"Lately on Fluvio, we\u0026rsquo;ve been doing a lot of work to improve our productivity and speed up our development cycles. One of the easiest and most effective things we\u0026rsquo;ve done in order to achieve this goal is to integrate the Bors-ng GitHub bot into our development workflow. In this post, I\u0026rsquo;ll talk about some of the problems we were facing, what Bors is and why it was a good solution for our team, and how adopting it has helped us to increase our development speed and confidence.","title":"Increasing our development confidence and productivity with Bors","url":"http://localhost:1315/blog/2021/05/bors-confident-merges/"},{"body":"This week, we\u0026rsquo;re happy to announce the addition of a Java client library for Fluvio. Using the Java client is just as easy as using our other clients. Check out the hello world in Java tutorial or documentation for usage.\nThis post will talk about how we bundled and distributed our Rust code into a Java Jar using Gradle to target a desktop enviroment. To do this for android, we recommend the Flapigen android example.\nOverview Similar to our python post, we used flapigen for packaging, but then it took a bit of research and many cups of coffee to figure out how to bundle Rust into a jar for publishing.\nIn this post we will describe the process for:\nSetting up a Gradle and Rust Project Configuring flapigen to run at build-time and generate Java bindings from our Rust interface Bundling the Rust native library into the Java jar Loading the native library at runtime Writing tests to verify the behavior Let\u0026rsquo;s get started.\nSetup Before doing anything, make sure you\u0026rsquo;ve got the Rust toolchain and Gradle installed.\nTo get started, we\u0026rsquo;ll create a new project folder set up for both Rust and Gradle.\n%copy%\ncargo new --lib my-java-lib cd my-java-lib gradle init Gradle will guide you through the installation. Follow the Building Java Libraries Sample. It should look similar to this:\n%copy first-line%\n$ gradle init Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1..4] 3 Select implementation language: 1: C++ 2: Groovy 3: Java 4: Kotlin 5: Scala 6: Swift Enter selection (default: Java) [1..6] 3 Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1..2] 1 Select test framework: 1: JUnit 4 2: TestNG 3: Spock 4: JUnit Jupiter Enter selection (default: JUnit 4) [1..4] 1 Project name (default: my-java-lib): Source package (default: my.java.lib): \u0026gt; Task :init Get more help with your project: https://docs.gradle.org/6.8.3/samples/sample_building_java_libraries.html BUILD SUCCESSFUL in 37s 2 actionable tasks: 2 executed The above creates a Rust crate named my-java-lib and a gradle library called my-java-lib.\nRunning tree, you will see a bunch of directories:\n%copy first-line%\n$ tree . |-gradle |---wrapper |-lib |---src |-----main |-------java |---------my |-----------java |-------------lib |-------resources |-----test |-------java |---------my |-----------java |-------------lib |-------resources |-src Rust glue We\u0026rsquo;ll need to add this to your Cargo.toml:\n%copy%\n[lib] crate-type = [\u0026#34;cdylib\u0026#34;] name = \u0026#34;my_java_lib\u0026#34; [dependencies] log = \u0026#34;^0.4.6\u0026#34; [build-dependencies] flapigen = \u0026#34;0.6.0-pre7\u0026#34; bindgen = { version = \u0026#34;0.57.0\u0026#34;, default-features = false} build.rs Now we\u0026rsquo;ll create a build script in build.rs by adding the following:\n%copy%\nuse flapigen::{LanguageConfig, JavaConfig}; use std::{ env, path::{Path, PathBuf}, }; fn main() { let out_dir = env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap(); let jni_c_headers_rs = Path::new(\u0026amp;out_dir).join(\u0026#34;jni_c_header.rs\u0026#34;); gen_jni_bindings(\u0026amp;jni_c_headers_rs); let java_cfg = JavaConfig::new( Path::new(\u0026#34;lib\u0026#34;) .join(\u0026#34;src\u0026#34;) .join(\u0026#34;main\u0026#34;) .join(\u0026#34;java\u0026#34;) .join(\u0026#34;my\u0026#34;) .join(\u0026#34;java\u0026#34;) .join(\u0026#34;lib\u0026#34;), \u0026#34;my.java.lib\u0026#34;.into(), ); let in_src = Path::new(\u0026#34;src\u0026#34;).join(\u0026#34;java_glue.rs.in\u0026#34;); let out_dir = env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap(); let out_src = Path::new(\u0026amp;out_dir).join(\u0026#34;java_glue.rs\u0026#34;); let flap_gen = flapigen::Generator::new( LanguageConfig::JavaConfig(java_cfg) ).rustfmt_bindings(true); flap_gen.expand(\u0026#34;java bindings\u0026#34;, \u0026amp;in_src, \u0026amp;out_src); println!(\u0026#34;cargo:rerun-if-changed={}\u0026#34;, in_src.display()); } fn gen_jni_bindings(jni_c_headers_rs: \u0026amp;Path) { let java_home = env::var(\u0026#34;JAVA_HOME\u0026#34;).expect(\u0026#34;JAVA_HOME env variable not settted\u0026#34;); let java_include_dir = Path::new(\u0026amp;java_home).join(\u0026#34;include\u0026#34;); let target = env::var(\u0026#34;TARGET\u0026#34;).expect(\u0026#34;target env var not setted\u0026#34;); let java_sys_include_dir = java_include_dir.join(if target.contains(\u0026#34;windows\u0026#34;) { \u0026#34;win32\u0026#34; } else if target.contains(\u0026#34;darwin\u0026#34;) { \u0026#34;darwin\u0026#34; } else { \u0026#34;linux\u0026#34; }); let include_dirs = [java_include_dir, java_sys_include_dir]; println!(\u0026#34;jni include dirs {:?}\u0026#34;, include_dirs); let jni_h_path = search_file_in_directory(\u0026amp;include_dirs[..], \u0026#34;jni.h\u0026#34;).expect(\u0026#34;Can not find jni.h\u0026#34;); println!(\u0026#34;cargo:rerun-if-changed={}\u0026#34;, jni_h_path.display()); gen_binding(\u0026amp;include_dirs[..], \u0026amp;jni_h_path, jni_c_headers_rs).expect(\u0026#34;gen_binding failed\u0026#34;); } fn search_file_in_directory\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;(dirs: \u0026amp;[P], file: \u0026amp;str) -\u0026gt; Result\u0026lt;PathBuf, ()\u0026gt; { for dir in dirs { let dir = dir.as_ref().to_path_buf(); let file_path = dir.join(file); if file_path.exists() \u0026amp;\u0026amp; file_path.is_file() { return Ok(file_path); } } Err(()) } fn gen_binding\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;( include_dirs: \u0026amp;[P], c_file_path: \u0026amp;Path, output_rust: \u0026amp;Path, ) -\u0026gt; Result\u0026lt;(), String\u0026gt; { let mut bindings: bindgen::Builder = bindgen::builder().header(c_file_path.to_str().unwrap()); bindings = include_dirs.iter().fold(bindings, |acc, x| { acc.clang_arg(\u0026#34;-I\u0026#34;.to_string() + x.as_ref().to_str().unwrap()) }); let generated_bindings = bindings .generate() .map_err(|_| \u0026#34;Failed to generate bindings\u0026#34;.to_string())?; generated_bindings .write_to_file(output_rust) .map_err(|err| err.to_string())?; Ok(()) } This buildscript is long because:\nit looks at the JAVA_HOME environment variable looks for the JNI headers uses rust-bindgen to generate bindings to the Java runtime Generates flaipgen FFI functions Generates Java classes for the flapigen classes and puts them in lib/src/main/java/my/java/lib/ src/*.rs Now we\u0026rsquo;ll add a src/java_glue.rs.in file with something like the following:\n%copy%\n// src/java_glue.rs.in use crate::jni_c_header::*; pub struct Foo { val: i32 } impl Foo { pub fn new(val: i32) -\u0026gt; Self { Self { val } } pub fn set_field(\u0026amp;mut self, new_val: i32) { self.val = new_val; } pub fn val(\u0026amp;self) -\u0026gt; i32 { self.val } } foreign_class!(class Foo { self_type Foo; constructor Foo::new(_: i32) -\u0026gt; Foo; fn Foo::set_field(\u0026amp;mut self, _: i32); fn Foo::val(\u0026amp;self) -\u0026gt; i32; }); This simple example was published in the flapigen book, and we can copy and paste it here.\nThe src/lib.rs should currently have some basic tests. We\u0026rsquo;ll change it to the following:\n%copy%\n// src/lib.rs #![allow( clippy::enum_variant_names, clippy::unused_unit, clippy::let_and_return, clippy::not_unsafe_ptr_arg_deref, clippy::cast_lossless, clippy::blacklisted_name, clippy::too_many_arguments, clippy::trivially_copy_pass_by_ref, clippy::let_unit_value, clippy::clone_on_copy )] mod jni_c_header; include!(concat!(env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/java_glue.rs\u0026#34;)); This is a typical Rust pattern when using build scripts. The code takes the file in ${OUT_DIR}/java_glue.rs and includes the contents into src/lib.rs in the build directory. The result will be as if we hand-wrote the generated code in our lib.rs file.\nWe will also need a src/jni_c_header.rs with the following:\n%copy%\n// src/jni_c_header.rs #![allow( non_upper_case_globals, dead_code, non_camel_case_types, improper_ctypes, non_snake_case, clippy::unreadable_literal, clippy::const_static_lifetime )] include!(concat!(env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/jni_c_header.rs\u0026#34;)); This section uses flapigen to expand the foreign_class macro into many Java functions. If you want to see what that looks like, install cargo-expand and run cargo expand. You will get a lot of generated Rust code.\nOnce everything is setup, run cargo check or cargo build to generate the Java files in lib/src/main/java/my/java/lib/\nJava Glue Make sure to run cargo build as we\u0026rsquo;ll be using the files generate the following directory lib/src/main/java/my/java/lib/ to continue:\nFoo.java InternalPointerMarker.java JNIReachabilityFence.java Library.java The gradle setup step created Library.java, and flapigen the other three files. Foo.java is the file we care about, and it should look like this:\n// Automatically generated by flapigen package my.java.lib; public final class Foo { public Foo(int a0) { mNativeObj = init(a0); } private static native long init(int a0); public final void set_field(int a0) { do_set_field(mNativeObj, a0); } private static native void do_set_field(long self, int a0); public final int val() { int ret = do_val(mNativeObj); return ret; } private static native int do_val(long self); public synchronized void delete() { if (mNativeObj != 0) { do_delete(mNativeObj); mNativeObj = 0; } } @Override protected void finalize() throws Throwable { try { delete(); } finally { super.finalize(); } } private static native void do_delete(long me); /*package*/ Foo(InternalPointerMarker marker, long ptr) { assert marker == InternalPointerMarker.RAW_PTR; this.mNativeObj = ptr; } /*package*/ long mNativeObj; } The src/java_glue.rs file we wrote in the section above instructed flappigen to provision and manipulate mNativeObj.\nTo run the Java tests, run ./gradlew test. This won\u0026rsquo;t test that we\u0026rsquo;ve hooked up the Rust quite yet but it\u0026rsquo;s important to make sure your tests run correctly before we go and break them. \u0026#x1f609;\nFooTest.java Now, let\u0026rsquo;s add a failing Java unit test that calls our Rust. The test ensures that the unit test is actually called and that the library is loaded correctly. Create the file lib/src/test/java/my/java/lib/FooTest.java:\n%copy%\n// lib/src/test/java/my/java/lib/FooTest.java package my.java.lib; import org.junit.Test; import static org.junit.Assert.*; import my.java.lib.Foo; public class FooTest { @Test public void testSomeLibraryMethod() { Foo foo = new Foo(10); assertTrue(\u0026#34;Foo.val\u0026#34;, foo.val() == 15); // This will fail later. } } Now if you run ./gradlew test you should get the following error:\n%copy first-line%\n$ ./gradlew test \u0026gt; Task :lib:test FAILED my.java.lib.FooTest \u0026gt; testSomeLibraryMethod FAILED java.lang.UnsatisfiedLinkError at FooTest.java:9 2 tests completed, 1 failed FAILURE: Build failed with an exception. * What went wrong: Execution failed for task \u0026#39;:lib:test\u0026#39;. \u0026gt; There were failing tests. See the report at: file:///home/simlay/projects/infinyon/fluvio-demo-apps-rust/my-java-lib-blog-post/lib/build/reports/tests/test/index.html * Try: Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org BUILD FAILED in 741ms 3 actionable tasks: 2 executed, 1 up-to-date This means we need to link our Rust as a static library.\nGradle build The gradle init step in the setup generated a project template. We need to add the project template to lib/build.gradle:\n%copy%\n// Append to `lib/build.gradle` def rustBasePath = \u0026#34;..\u0026#34; // execute cargo metadata and get path to target directory tasks.create(name: \u0026#34;cargo-output-dir\u0026#34;, description: \u0026#34;Get cargo metadata\u0026#34;) { new ByteArrayOutputStream().withStream { os -\u0026gt; exec { commandLine \u0026#39;cargo\u0026#39;, \u0026#39;metadata\u0026#39;, \u0026#39;--format-version\u0026#39;, \u0026#39;1\u0026#39; workingDir rustBasePath standardOutput = os } def outputAsString = os.toString() def json = new groovy.json.JsonSlurper().parseText(outputAsString) logger.info(\u0026#34;cargo target directory: ${json.target_directory}\u0026#34;) project.ext.cargo_target_directory = json.target_directory } } // Build with cargo tasks.create(name: \u0026#34;cargo-build\u0026#34;, type: Exec, description: \u0026#34;Running Cargo build\u0026#34;, dependsOn: \u0026#34;cargo-output-dir\u0026#34;) { workingDir rustBasePath commandLine \u0026#39;cargo\u0026#39;, \u0026#39;build\u0026#39;, \u0026#39;--release\u0026#39; } tasks.create(name: \u0026#34;rust-deploy\u0026#34;, type: Sync, dependsOn: \u0026#34;cargo-build\u0026#34;) { from \u0026#34;${project.ext.cargo_target_directory}/release\u0026#34; include \u0026#34;*.dylib\u0026#34;,\u0026#34;*.so\u0026#34; into \u0026#34;rust-lib/\u0026#34; } clean.dependsOn \u0026#34;clean-rust\u0026#34; tasks.withType(JavaCompile) { compileTask -\u0026gt; compileTask.dependsOn \u0026#34;rust-deploy\u0026#34; } sourceSets { main { java { srcDir \u0026#39;src/main/java\u0026#39; } resources { srcDir \u0026#39;rust-lib\u0026#39; } } } In short, this:\nruns cargo build --release as a build step Copies the cydylib from ./target/release/ into lib/rust-lib And then adds the rust-lib directory as a resource to the Gradle SourceSets. To verify that the Rust is in our Jar, run:\n%copy first-line%\n$ ./gradlew build -x test \u0026amp;\u0026amp; jar tf lib/build/libs/lib.jar It should look like this:\nMETA-INF/ META-INF/MANIFEST.MF my/ my/java/ my/java/lib/ my/java/lib/Foo.class my/java/lib/Library.class my/java/lib/InternalPointerMarker.class my/java/lib/JNIReachabilityFence.class libmy_java_lib.so The magic step - loading the runtime library from your jar It seems there is no way to test if your library is linked correctly at compile time using Gradle or Java. When the program starts, the user must reference the internal shared library:\nstatic { System.loadLibrary(\u0026#34;my_java_lib\u0026#34;); } Our library is now loaded at runtime.\nrequiring the user to add a shared library to their environment is inconvenient. Fortunately, a nice internet patron has a workaround. The solution looks at the jar, unzips it in a temp directory, and then calls System.load.\nSimply download Adam Heinrich\u0026rsquo;s NativUtils.java to lib/src/main/java/my/java/lib/NativeUtils.java, change the package to my.java.lib.\nFinally, we go back to your flaipgen Foo class and update it as follows:\n%copy%\nforeign_class!(class Foo { self_type Foo; constructor Foo::new(_: i32) -\u0026gt; Foo; fn Foo::set_field(\u0026amp;mut self, _: i32); fn Foo::val(\u0026amp;self) -\u0026gt; i32; foreign_code r#\u0026#34; static { try { NativeUtils.loadLibraryFromJar(\u0026#34;/libmy_java_lib.so\u0026#34;); // for macOS, make sure this is .dylib rather than .so } catch (java.io.IOException e) { e.printStackTrace(); } }\u0026#34;#; }); The foreign_code block will add a static { ... } routine to the Foo class declaration in java.\n-\u0026gt; Note: On macOS, you will need to use .dylib rather than .so for your loadLibraryFromJar call. In our client, detect the operating system at runtime.\nTesting it all out In our FooTest.java, we wrote an assert function that would fail. Let\u0026rsquo;s verify that this actually fails. Do this by running ./gradlew test. It should fail with the following assertion error:\n%copy first-line%\n$ ./gradlew test \u0026gt; Task :lib:cargo-build Finished release [optimized] target(s) in 0.04s \u0026gt; Task :lib:test FAILED JNI_OnLoad begin my.java.lib.FooTest \u0026gt; testSomeLibraryMethod FAILED java.lang.AssertionError at FooTest.java:10 2 tests completed, 1 failed FAILURE: Build failed with an exception. * What went wrong: Execution failed for task \u0026#39;:lib:test\u0026#39;. \u0026gt; There were failing tests. See the report at: file:///home/simlay/projects/infinyon/fluvio-demo-apps-rust/my-java-lib-blog-post/lib/build/reports/tests/test/index.html * Try: Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org BUILD FAILED in 943ms 6 actionable tasks: 2 executed, 4 up-to-date Now, let\u0026rsquo;s update our FooTest.java to the following:\n%copy%\npackage my.java.lib; import org.junit.Test; import static org.junit.Assert.*; import my.java.lib.Foo; public class FooTest { @Test public void testSomeLibraryMethod() { Foo foo = new Foo(10); assertTrue(\u0026#34;Foo.val\u0026#34;, foo.val() == 10); foo.set_field(15); assertTrue(\u0026#34;Foo.val\u0026#34;, foo.val() == 15); } } And you should see:\n%copy first-line%\n$ ./gradlew test \u0026gt; Task :lib:cargo-build Finished release [optimized] target(s) in 0.04s BUILD SUCCESSFUL in 624ms 6 actionable tasks: 1 executed, 5 up-to-date And there you go! You\u0026rsquo;ve now called Rust from Java and can distribute your Rust code in a Java Jar! \u0026#x1f389;\nConclusion This post is a bit longer and the code is a bit more verbose than I normally post about but this is a pretty clean setup. You can get the source for this post in our fluvio-demo-apps-rust repository.\nAdding cleaner docs to JavaDoc is also another thing we could talk about we wanted to keep this post short. You can checkout how we are generating the documentation in our Fluvio Java Client repository.\n","description":"Learn how to wrap your Rust crate in a Java Jar for the desktop/server.","keywords":null,"summary":"This week, we\u0026rsquo;re happy to announce the addition of a Java client library for Fluvio. Using the Java client is just as easy as using our other clients. Check out the hello world in Java tutorial or documentation for usage.\nThis post will talk about how we bundled and distributed our Rust code into a Java Jar using Gradle to target a desktop enviroment. To do this for android, we recommend the Flapigen android example.","title":"Embedding Rust code in Java Jar for distribution","url":"http://localhost:1315/blog/2021/05/java-client/"},{"body":"I ran into a problem effectively using cargo test in Fluvio for integration testing.\nLet’s talk about integration testing in Rust While creating integration testing for Fluvio, I ran into a problem. Organizing and executing integration tests with cargo test was becoming inefficient. We needed to standardize the setup of a test environment.\nAs a lone developer, you can apply one-off customizations when running tests locally. But if you try to extend that strategy to continuous integration, you’ll quickly find that making changes manually becomes burdensome. CI encourages testing many different configurations, which means a successful CI plan requires easy management of test harness variables (a.k.a. Not manually updating variables for every test you need to run).\ncargo test is just not equipped to handle this specialized focus on environment setup, or the cleanup/teardown needed after a test is run. When using cargo test, these crucial tasks could only occur outside of the harness or within the logic of a test. Neither of these are good choices. Outside of the harness is not ideal because these processes end up too disconnected and hard to maintain. Likewise, including setup/teardown within the logic of a test is inappropriate because it creates mental overhead for a test writer, and may obscure the results of tests.\nI needed to find a way around the limited functionality of cargo test \u0026ndash; keep reading to find out how I did it by creating a standardized setup and teardown as part of our testing harness.\nHow does cargo test work by default? There is a distinction between unit tests and integration tests in the Rust book. The distinction is less about testing strategy and more about defining Rust’s conventions for test organization.\nThe main points are that:\nYour tests are annotated with #[test] libtest harness enumerates through all of your tests (a point we’ll revisit later in more detail) libtest returns the pass/fail status of the execution What do I need from integration testing? Libtest doesn\u0026rsquo;t specifically offer anything to support integration testing patterns.\nSetup of a standard test environment – especially in a complex system – is essential for managing expected behavior when making code changes.\nUnfortunately libtest does not assist with setup or teardown. I needed the ability to abstract away the setup and teardown of my test environment from test code.\nThis task will be performed either way. Without harness support, setup/teardown will be performed via external shell scripts or padding the setup/teardown process within every single integration test\u0026hellip; (no one\u0026rsquo;s idea of fun).\nIt isn’t convenient to manage setup and teardown in a different context than the integration test. This kind of testing overhead leads to hard-to-reproduce and time consuming mistakes.\nWhere do we get started with a custom test harness? By default, libtest will compile each of your #[test] labeled functions into their own binary crates (with its own main()) and executes it as part of the test. But we’re going to build all our integration tests into a single crate. This is recommended in order to speed up compile time ([1], [2])\nFirst we’re going to create an integration test directory at the root of the crate where we’re going to build our integration test focused binary.\n%copy%\n$ mkdir integration $ touch integration/main.rs # Then create a main() function in main.rs In your Cargo.toml, you want to add\n%copy%\n# Cargo.toml # We\u0026#39;ll revisit the `inventory` crate later in the post [dev-dependencies] inventory = \u0026#34;0.1\u0026#34; [[test]] name = \u0026#34;integration\u0026#34; path = \u0026#34;integration/main.rs\u0026#34; harness = false This tells cargo test to not use libtest when running the integration test.\nWhen we run cargo test integration, what cargo will compile integration/main.rs and execute it in the same manner as cargo run. This is all a harness is from cargo’s perspective.\nAdd Setup and teardown steps Next we’ll lay the foundation for our testing pattern. We’ll create 2 functions, setup() and teardown(), and add them to our main() (with reserved space in between for our future tests to be called).\n%copy%\n// main.rs fn setup() { println!(\u0026#34;Setup\u0026#34;) } fn teardown() { println!(\u0026#34;Teardown\u0026#34;) } fn main() { // Setup test environment setup(); // TODO: Run the test // Teardown test environment teardown(); } Collect all integration tests To do its job, our test runner needs to create a list of all the test functions. Initially, I thought there would be an easy way to do this by leveraging libtest\u0026rsquo;s #[test] attribute.\nI dug around in relevant areas of libtest and Cargo test and Rustc macros code, but long (sad) story short, there is no straightforward way to reuse libtest for the purpose of test collection.\nIf that surprises you, then you\u0026rsquo;re like me. I had hoped to use the test collection functionality from #[test], but it wasn’t clear how I could accomplish this. My mental model for how cargo test works needed a refresh.\nNow that we’ve removed the option of using libtest, so that gives you 2 practical options for collecting tests:\nManually modify integration/main.rs and add your test in between the setup and teardown A quick and straightforward solution if you have a small set of tests This option requires us to add new tests to this list, which can be error-prone and tedious as we grow. Build a test collector. We generate an external test catalog, and modify integration/main.rs to execute tests from the catalog. This is a long term solution, which we’ll be covering for the rest of the post. Building the test collector For this test collector, we\u0026rsquo;ll be utilizing a crate. The inventory crate is a plugin registry system. We\u0026rsquo;ll be using it for all the heavy lifting in our test framework, which means we\u0026rsquo;ll be treating our tests as plugins.\nIn our main.rs, let’s declare a new module tests, where we can organize all the integration tests.\n// main.rs + pub mod tests; fn setup() { println!(\u0026#34;Setup\u0026#34;) } fn teardown() { println!(\u0026#34;Teardown\u0026#34;) } fn main() { // Setup test environment setup(); // TODO: Run the test // Teardown test environment teardown(); } In our new module, we’ll start by creating a struct to represent a single test for the plugin registry.\n%copy%\n// tests/mod.rs #[derive(Debug)] pub struct IntegrationTest { pub name: \u0026amp;\u0026#39;static str, pub test_fn: fn(), } inventory::collect!(IntegrationTest); In this example, our struct IntegrationTest has 2 fields.\nname is a human-readable name, which can be used as a key for test selection. test_fn is a pointer to a function whose signature is non-async, takes no args, and does not return anything. Note: You can use functions that take args, and return things.\nFor example:\npub test_fn: fn(String) -\u0026gt; Result\u0026lt;(), ()\u0026gt;, Then we call the inventory::collect!()macro to instantiate a plugin registry. When we write our tests, we’ll add to the plugin registry. More on this next.\nAdding new tests to plugin registry We’re going to add a new basic test to the plugin registry. Start by adding a new submodule called basic in the tests module.\n%copy%\n// tests/mod.rs pub mod basic; In the basic module, we write our basic test basic_test()\n%copy%\n// tests/basic.rs use super::IntegrationTest; fn basic_test() { println!(\u0026#34;Running basic test\u0026#34;) } inventory::submit!(IntegrationTest { name: \u0026#34;basic\u0026#34;, test_fn: basic_test }); We\u0026rsquo;ll use inventory::submit!() to register our new test with the IntegrationTest struct we defined earlier.\nname is a friendly, human-readable name. We can use this name as a key to search through the plugin registry.\ntest_fn takes the name of our test function. It has the same function signature as we defined.\nRunning tests from registry We’ll finish this example up by running all of our registered tests\n// main.rs pub mod tests; + use tests::IntegrationTest; fn setup() { println!(\u0026#34;Setup\u0026#34;) } fn teardown() { println!(\u0026#34;Teardown\u0026#34;) } fn main() { setup(); - // TODO: Run the test + // Run the tests + for t in inventory::iter::\u0026lt;IntegrationTest\u0026gt; { + (t.test_fn)() + } teardown(); } %copy first-line%\n$ cargo test integration Compiling blog-post-example v0.1.0 (/home/telant/Documents/blog-post-example) Finished test [unoptimized + debuginfo] target(s) in 0.21s Running target/debug/deps/blog_post_example-e042d787684bb333 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s Running target/debug/deps/integration-7ed2452642c6f3b6 Setup Running basic test Teardown Tips for extending the example The example runs all of the registered tests. But here are some useful impls if you want to extend even further. For example, adding a CLI, if you want to select individual tests. Or provide options to customize setup or teardown behavior.\n%copy%\nimpl IntegrationTest { pub fn all_test_names() -\u0026gt; Vec\u0026lt;\u0026amp;\u0026#39;static str\u0026gt; { inventory::iter::\u0026lt;IntegrationTest\u0026gt; .into_iter() .map(|x| x.name) .collect::\u0026lt;Vec\u0026lt;\u0026amp;str\u0026gt;\u0026gt;() } pub fn from_name\u0026lt;S: AsRef\u0026lt;str\u0026gt;\u0026gt;(test_name: S) -\u0026gt; Option\u0026lt;\u0026amp;\u0026#39;static IntegrationTest\u0026gt; { inventory::iter::\u0026lt;IntegrationTest\u0026gt; .into_iter() .find(|t| t.name == test_name.as_ref()) } } If you want to see more of these ideas extended even further, check out Fluvio’s integration test runner.\nWe use the CLI to customize setup, handle async testing, and we use an attribute macro to collect tests.\nConclusion Rust’s testing ecosystem in the 2018 edition is great for unit testing. But for integration testing it still has room for improvement. Custom harnesses will become more necessary as Rust/master gains new developers.\nIf we want to avoid reinventing the wheel, we need stable support from libtest or more examples of how to perform test collection and patterns for setup, test, and teardown workflows.\nIf you made it this far, thank you for following along with me! I wrote this because I could not find a guide to do this before trying to do this myself, and knowing these things beforehand would have made it much faster. Hopefully others find my experience helpful.\n","description":"Building a custom test harness in Rust is less complicated following these patterns.","keywords":null,"summary":"I ran into a problem effectively using cargo test in Fluvio for integration testing.\nLet’s talk about integration testing in Rust While creating integration testing for Fluvio, I ran into a problem. Organizing and executing integration tests with cargo test was becoming inefficient. We needed to standardize the setup of a test environment.\nAs a lone developer, you can apply one-off customizations when running tests locally. But if you try to extend that strategy to continuous integration, you’ll quickly find that making changes manually becomes burdensome.","title":"How to Build a Custom Test Harness in Rust","url":"http://localhost:1315/blog/2021/04/rust-custom-test-harness/"},{"body":"Fluvio is a high-performance distributed streaming platform written in Rust. As a fairly large project, we have a lot of build configurations and testing scenarios that we automate in order to make sure we don\u0026rsquo;t break things by accident. We\u0026rsquo;ve been using GitHub Actions in order to run our CI/CD workflows, but as we\u0026rsquo;ve grown, things have naturally gotten messy over time. This week, I took some time to re-visit our workflow definitions to clean things up and try to increase our team\u0026rsquo;s productivity. I specifically want to talk about two main improvements I worked on:\nConsolidating multiple jobs using the build matrix This cut our workflow file size almost in half, from 477 lines to 264, making CI easier to maintain. Setting up sccache to improve our building and testing speed We actually had already set up sccache but it was misconfigured. I\u0026rsquo;ll talk about how to check that everything is set up properly. The first half of this post should be generally useful for anybody who needs to use GitHub Actions and wants to learn more about the build matrix. The second half should be useful to Rust developers who want a good starting point for a solid CI setup.\nUsing the GitHub workflows build matrix The reason I was working on workflows this week was because our CI build and test time had grown to a point that it was interfering with our team\u0026rsquo;s ability to move quickly. When I started reading through our workflow definitions, what I saw was a lot of independent jobs with a lot of duplicated steps. Most of the jobs would install the Rust toolchain, install sccache, cache the Cargo registry and the sccache directory, and then run a single task from our project Makefile. The boilerplate to set up each of these jobs came out to at least 60 lines of configuration. I won\u0026rsquo;t post any of the \u0026ldquo;before\u0026rdquo; workflow code here, but if you are interested in seeing it you can look at this old commit.\nInstead, I\u0026rsquo;m going to show you the new job definition, and the matrix setup that goes with it. One thing I learned while doing this is that GitHub\u0026rsquo;s workflow documentation does not really give the matrix feature justice because they use such simple examples. Here\u0026rsquo;s the matrix definition I came up with for our new job definition. I\u0026rsquo;ll briefly explain how the matrix feature works in case you are unfamiliar with it:\n# .github/workflows/ci.yml tests: name: ${{ matrix.make.name }} (${{ matrix.os }}) runs-on: ${{ matrix.os }} strategy: fail-fast: false matrix: os: [ubuntu-latest, macos-latest] rust: [stable] make: - name: Clippy task: \u0026#34;check-clippy\u0026#34; - name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; include: - os: ubuntu-latest sccache-path: /home/runner/.cache/sccache - os: macos-latest sccache-path: /Users/runner/Library/Caches/Mozilla.sccache exclude: - os: macos-latest rust: stable make: name: Clippy The part of this that we\u0026rsquo;re interested in is the matrix object. Notice that it is a proper key-value object, it has keys os, rust, and make. These keys are arbitrary, you can choose any names that you want for them. I like to think of them as the \u0026ldquo;dimensions\u0026rdquo; of the matrix. The value at each of these keys must be a list:\nFor the key os, the list is [ubuntu-latest, macos-latest]. For the key rust, we have a list with a single element: [stable]. And even though the value under make looks different, notice that it is still a list, it is just a list of more yaml objects. When GitHub Actions reads your job definition, it performs a sort of cross-product on each entry in your matrix, creating a list of all the combinations of items in your lists. For this matrix definition, GitHub will generate the following list of configurations to run the job with:\n- os: ubuntu-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; - os: macos-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; - os: ubuntu-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - os: macos-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - os: ubuntu-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; - os: macos-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; In the rest of the job definition, you can access the fields of the active configuration using the ${{ matrix.KEY }} syntax. You can see in the job definition above that this is used in the line runs-on: ${{ matrix.os }}, which is how we tell the runner which type of machine to run the job on.\nInclude and Exclude rules You may be wondering right now, \u0026ldquo;Hey, what happened to include and exclude? They didn\u0026rsquo;t get mixed into the matrix!\u0026rdquo;, and you would be right. include and exclude are special keys that allow you to manually edit the resulting configurations.\nWhen writing a rule for include, we are essentially writing a pattern that matches against the output configurations and may add new data to them. Let\u0026rsquo;s look at the effect of a specific include rule:\ninclude: - os: ubuntu-latest sccache-path: /home/runner/.cache/sccache This says: \u0026ldquo;for any configuration that has os: ubuntu-latest, add another key that has sccache-path: /home/runner/.cache/sccache\u0026rdquo;. If we apply this rule to our output configuration, it would look like this:\n- os: ubuntu-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; + sccache-path: /home/runner/.cache/sccache - os: macos-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - os: ubuntu-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; + sccache-path: /home/runner/.cache/sccache - os: macos-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; - os: ubuntu-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; + sccache-path: /home/runner/.cache/sccache - os: macos-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; Note: the + at the beginning of the green lines is not part of the workflow file, it is part of the diff syntax I\u0026rsquo;m using to show you that this line was added\nSimilarly, we can use exclude rules to describe objects in the output configuration to discard. For example, we currently have two configurations that will cause Clippy to be run, but we really only need Clippy to run once. Let\u0026rsquo;s look at the effect the following exclude rule from our matrix has on the output configuration:\nexclude: - os: macos-latest rust: stable make: name: Clippy When this rule gets applied, it removes the entry for Clippy on MacOS:\n- os: ubuntu-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - os: macos-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - os: ubuntu-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; - os: macos-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; - os: ubuntu-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; - - os: macos-latest - rust: stable - make: - name: Clippy - task: \u0026#34;check-clippy\u0026#34; Note: The first - on the red lines is also not part of the workflow file, it is part of the removed-lines diff syntax\nPutting it all together, our matrix definition with all the include and exclude rules applied will look like the following:\n- os: ubuntu-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; + sccache-path: /home/runner/.cache/sccache - os: macos-latest rust: stable make: name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; + sccache-path: /Users/runner/Library/Caches/Mozilla.sccache - os: ubuntu-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; + sccache-path: /home/runner/.cache/sccache - os: macos-latest rust: stable make: name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; + sccache-path: /Users/runner/Library/Caches/Mozilla.sccache - os: ubuntu-latest rust: stable make: name: Clippy task: \u0026#34;check-clippy\u0026#34; + sccache-path: /home/runner/.cache/sccache - - os: macos-latest - rust: stable - make: - name: Clippy - task: \u0026#34;check-clippy\u0026#34; Ok, awesome. So now we have a strategy for adding new configurations as well as for tweaking options on specific configurations. If you\u0026rsquo;re trying to consolidate a bunch of duplicate jobs, a good strategy is to start identifying the small pieces of each job that are different from the others, and put those as options in the matrix. Next I\u0026rsquo;ll walk through the rest of the job definition and talk about how we set it up to meet our Rust project needs.\nOptimizing Rust\u0026rsquo;s build speed with sccache For the rest of this post I\u0026rsquo;ll just be talking about how I set up the rest of this job definition to build and test our Rust binaries using sccache. sccache is a tool built by Mozilla that can cache the output of rustc and re-use those build artifacts if nothing has changed. The basic setup we\u0026rsquo;ll need in our job is:\nInstall sccache for the OS we\u0026rsquo;re running on Use the GitHub actions cache to save the sccache cache directory Let me start by just posting the job definition up front, including the matrix definition we\u0026rsquo;ve already seen:\n# .github/workflows/ci.yml name: CI on: pull_request: workflow_dispatch: jobs: tests: name: ${{ matrix.make.name }} (${{ matrix.os }}) runs-on: ${{ matrix.os }} strategy: fail-fast: false matrix: os: [ubuntu-latest, macos-latest] rust: [stable] make: - name: Clippy task: \u0026#34;check-clippy\u0026#34; - name: Unit tests task: \u0026#34;build-all-test run-all-unit-test\u0026#34; - name: Doc tests task: \u0026#34;run-all-doc-test\u0026#34; include: - os: ubuntu-latest sccache-path: /home/runner/.cache/sccache - os: macos-latest sccache-path: /Users/runner/Library/Caches/Mozilla.sccache exclude: - os: macos-latest rust: stable make: name: Clippy env: RUST_BACKTRACE: full RUSTC_WRAPPER: sccache RUSTV: ${{ matrix.rust }} SCCACHE_CACHE_SIZE: 2G SCCACHE_DIR: ${{ matrix.sccache-path }} # SCCACHE_RECACHE: 1 # Uncomment this to clear cache, then comment it back out steps: - uses: actions/checkout@v2 - name: Install sccache (ubuntu-latest) if: matrix.os == \u0026#39;ubuntu-latest\u0026#39; env: LINK: https://github.com/mozilla/sccache/releases/download SCCACHE_VERSION: 0.2.13 run: | SCCACHE_FILE=sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl mkdir -p $HOME/.local/bin curl -L \u0026#34;$LINK/$SCCACHE_VERSION/$SCCACHE_FILE.tar.gz\u0026#34; | tar xz mv -f $SCCACHE_FILE/sccache $HOME/.local/bin/sccache echo \u0026#34;$HOME/.local/bin\u0026#34; \u0026gt;\u0026gt; $GITHUB_PATH - name: Install sccache (macos-latest) if: matrix.os == \u0026#39;macos-latest\u0026#39; run: | brew update brew install sccache - name: Install Rust ${{ matrix.rust }} uses: actions-rs/toolchain@v1 with: toolchain: ${{ matrix.rust }} profile: minimal override: true - name: Cache cargo registry uses: actions/cache@v2 continue-on-error: false with: path: | ~/.cargo/registry ~/.cargo/git key: ${{ runner.os }}-cargo-${{ hashFiles(\u0026#39;**/Cargo.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-cargo- - name: Save sccache uses: actions/cache@v2 continue-on-error: false with: path: ${{ matrix.sccache-path }} key: ${{ runner.os }}-sccache-${{ hashFiles(\u0026#39;**/Cargo.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-sccache- - name: Start sccache server run: sccache --start-server - name: ${{ matrix.make.name }} run: make ${{ matrix.make.task }} - name: Print sccache stats run: sccache --show-stats - name: Stop sccache server run: sccache --stop-server || true This job definition is 95% setup and about 5% task-running. In fact, the entirety of our project-specific building and testing specifications are defined inside our Makefile, so the step where we call make ${{ matrix.make.task }} is the only step that is unique to our project, the rest could be re-used as boilerplate for other Rust projects ;)\nLet\u0026rsquo;s start with the env section. First, we set RUST_BACKTRACE: full because if any of our tests panic we want to know why. RUSTV is an environment variable that we use inside our Makefile to tell cargo commands which release channel to use.\nThe rest of the env has to do with sccache. As I mentioned before, we want to use sccache to reduce the number of times we need to re-build crates when possible. Here is a quick summary of the other variables:\nRUSTC_WRAPPER: sccache is read by cargo itself and tells it to run compiler commands using sccache rustc ... rather than just rustc .... This is the easiest way to use sccache with Rust.\nSCCACHE_CACHE_SIZE tells sccache the maximum size you want to allow your cache to grow to. This is one of the most important things to get right: if you set this too small and the cache runs out of space, sccache will start evicting artifacts from the cache and you will end up recompiling more than you need to.\nSCCACHE_DIR tells sccache where to store build artifacts. This path is different on different OS\u0026rsquo;s, which is why we assign it using a matrix parameter.\nNow let\u0026rsquo;s run through the step definitions:\n- uses: actions/checkout@v2 This is a pretty standard GH Actions step that just checks out your git repository in the current directory. This gives further steps the ability to build or interact with your code.\nNext up is installing sccache:\n- name: Install sccache (ubuntu-latest) if: matrix.os == \u0026#39;ubuntu-latest\u0026#39; env: LINK: https://github.com/mozilla/sccache/releases/download SCCACHE_VERSION: 0.2.13 run: | SCCACHE_FILE=sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl mkdir -p $HOME/.local/bin curl -L \u0026#34;$LINK/$SCCACHE_VERSION/$SCCACHE_FILE.tar.gz\u0026#34; | tar xz mv -f $SCCACHE_FILE/sccache $HOME/.local/bin/sccache echo \u0026#34;$HOME/.local/bin\u0026#34; \u0026gt;\u0026gt; $GITHUB_PATH - name: Install sccache (macos-latest) if: matrix.os == \u0026#39;macos-latest\u0026#39; run: | brew update brew install sccache This is a good example of adapting steps according to a matrix. Depending on whether we are running on ubuntu-latest or macos-latest, there is a different procedure for installing sccache. Notice that only one of these two steps will ever run in a given execution of a job: the if: conditional checks which matrix.os value is specified in this job instance.\nThis is also a great example of a good opportunity for creating a reusable GitHub Action definition. If there\u0026rsquo;s an action definition out there for more easily installing sccache (or if one of you readers decides to put one together), let me know!\nThe next step just installs the Rust toolchain. If you have used Rust on GitHub actions before you have probably seen this one already:\n- name: Install Rust ${{ matrix.rust }} uses: actions-rs/toolchain@v1 with: toolchain: ${{ matrix.rust }} profile: minimal override: true Even though in our matrix we only have one value, rust: [stable], it is nice to use toolchain: ${{ matrix.rust }} so that in the future if you (or a coworker of yours) decides to run your workflow against other release channels, the only edit that will need to be made is in the matrix.\nThis next one has to do with sccache again, and it has to do with saving the cache directory itself. When using the free GitHub Action runners, jobs are always run on a fresh instance of a container or virtual machine somewhere. This means that our sccache would have to start from scratch on every job and we wouldn\u0026rsquo;t get any benefit out of it. To fix this, we can use the actions/cache@v2 action to preserve certain directories between job runs.\n- name: Cache cargo registry uses: actions/cache@v2 continue-on-error: false with: path: | ~/.cargo/registry ~/.cargo/git key: ${{ runner.os }}-cargo-${{ hashFiles(\u0026#39;**/Cargo.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-cargo- - name: Save sccache uses: actions/cache@v2 continue-on-error: false with: path: ${{ matrix.sccache-path }} key: ${{ runner.os }}-sccache-${{ hashFiles(\u0026#39;**/Cargo.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-sccache- The actions/cache@v2 action lets us specify directories that the GitHub runner will save at the end of each job, and attempt to restore at the beginning of the next job. You can have multiple different types of things you want to cache, so you have to choose a key for each cache to be labeled with when it gets saved. You also need to give it restore-keys that tell it how to choose an existing saved cache that you want to restore from. You can think of restore-keys as a pattern for matching some prefix of an actual key: the more specifically a key matches your restore-key, the higher precedence it will have.\nOn a side note about GitHub caches, does anybody know whether it makes more sense to keep separate caches separate or to combine them? Here I have separate caches for the Cargo registry and for the Sccache directory, but does it make any practical difference? If you know one way or another, let me know! Finally, we have the last steps of our jobs. Here we start the sccache server, run our Make task, then print statistics and stop the sccache server.\n- name: Start sccache server run: sccache --start-server - name: ${{ matrix.make.name }} run: make ${{ matrix.make.task }} - name: Print sccache stats run: sccache --show-stats - name: Stop sccache server run: sccache --stop-server || true There are a couple of things you will want to check on when you first set this up to make sure you are actually getting benefit from the cache:\nMake sure your cargo commands are actually running sccache Make sure that sccache is actually hitting the cache rather than always rebuilding Here\u0026rsquo;s an example of output when the sccache server has just started up and has not processed any compile requests:\n$ sccache --show-stats Compile requests 8 Compile requests executed 0 Cache hits 0 Cache misses 0 Cache timeouts 0 Cache read errors 0 Forced recaches 0 Cache write errors 0 Compilation failures 0 Cache errors 0 Non-cacheable compilations 0 Non-cacheable calls 8 Non-compilation calls 0 Unsupported compiler calls 0 Average cache write 0.000 s Average cache read miss 0.000 s Average cache read hit 0.000 s Failed distributed compilations 0 Non-cacheable reasons: incremental 7 crate-type 1 Cache location Local disk: \u0026#34;/Users/runner/Library/Caches/Mozilla.sccache\u0026#34; Cache size 545 MiB Max cache size 2 GiB If you build your project and see this immediately afterwards, something went wrong. I would recommend running your build with --verbose and double-checking that the commands cargo is running all begin with sccache rustc rather than just rustc\nIf your verbose cargo output looks like this, your sccache setup is working:\nCompiling fluvio-spu v0.5.0 (/Users/nick/infinyon/fluvio/src/spu) Running `/Users/nick/.cargo/bin/sccache rustc --crate-name fluvio_spu --edition=2018 ... But if you see output like this, sccache is not being invoked at all, and you should probably double-check that your RUSTC_WRAPPER environment variable is set properly.\nCompiling fluvio-spu v0.5.0 (/Users/nick/infinyon/fluvio/src/spu) Running `rustc --crate-name fluvio_spu --edition=2018 Verifying the sccache results When you have set things up so that sccache is properly running, you will see stats that have actual numbers in them rather than zeros. The next step is to make sure that those numbers are telling you that you hit the cache rather than rebuilding (missing the cache).\nYou might miss the cache for a couple of reasons:\nThis is the first time you have built using sccache and so you are populating the cache for the first time In this case, just run your job again to test if you hit the cache on the second go-round Something went wrong when re-loading the cache directory (e.g. from the GitHub cache). You have added a ton of dependencies to your project that you haven\u0026rsquo;t built before, so they\u0026rsquo;re not in the cache Here is an example of what your stats might look like if you have missed the cache. Notice that the number of hits is actually not zero, it\u0026rsquo;s just a very low number. I think this is probably because of caching duplicate dependencies within your dependency tree. However, if you see these results after a build, then your build did not really benefit from sccache.\nCompile requests 484 Compile requests executed 343 Cache hits 13 Cache hits (Rust) 13 Cache misses 330 Cache misses (Rust) 330 Cache timeouts 0 Cache read errors 0 Forced recaches 0 Cache write errors 0 Compilation failures 0 Cache errors 313 Cache errors (Rust) 313 Non-cacheable compilations 0 Non-cacheable calls 141 Non-compilation calls 0 Unsupported compiler calls 0 Average cache write 0.000 s Average cache read miss 2.249 s Average cache read hit 0.001 s Failed distributed compilations 0 Non-cacheable reasons: crate-type 111 incremental 28 - 2 Cache location Local disk: \u0026#34;/Users/runner/Library/Caches/Mozilla.sccache\u0026#34; Cache size 4 GiB Max cache size 10 GiB Usually if you get these results, all you need to do is run the build again, and the second build will be able to leverage the pre-compiled artifacts from the previous run. The stats from the second build might look more like the following:\nCompile requests 481 Compile requests executed 330 Cache hits 318 Cache hits (Rust) 318 Cache misses 12 Cache misses (Rust) 12 Cache timeouts 0 Cache read errors 0 Forced recaches 0 Cache write errors 0 Compilation failures 0 Cache errors 0 Non-cacheable compilations 0 Non-cacheable calls 151 Non-compilation calls 0 Unsupported compiler calls 0 Average cache write 0.001 s Average cache read miss 1.242 s Average cache read hit 0.001 s Failed distributed compilations 0 Non-cacheable reasons: crate-type 108 incremental 41 - 2 Cache location Local disk: \u0026#34;/Users/runner/Library/Caches/Mozilla.sccache\u0026#34; Cache size 545 MiB Max cache size 2 GiB Notice that we have many more cache hits than cache misses, that\u0026rsquo;s the indicator that we are getting good value out of sccache.\nThe end result When you push this workflow and trigger it, you\u0026rsquo;ll see a job start running for each combination of matrix parameters you provided:\nI took this screenshot from our full workflow file which includes a job for Rustfmt. The rest of the jobs in this list were generated from the single job definition I showed above.\nConclusion Thanks for reading, I hope this can be helpful to others who are also setting up GitHub actions with Rust projects. If you want to learn more about the Fluvio project, feel free to check out our GitHub or come join our community Discord.\nQuick links: Getting started with Fluvio Fluvio CLI reference Fluvio Architecture ","description":"How I cleaned up Rust CI boilerplate using the GitHub Actions build matrix.","keywords":null,"summary":"Fluvio is a high-performance distributed streaming platform written in Rust. As a fairly large project, we have a lot of build configurations and testing scenarios that we automate in order to make sure we don\u0026rsquo;t break things by accident. We\u0026rsquo;ve been using GitHub Actions in order to run our CI/CD workflows, but as we\u0026rsquo;ve grown, things have naturally gotten messy over time. This week, I took some time to re-visit our workflow definitions to clean things up and try to increase our team\u0026rsquo;s productivity.","title":"GitHub Actions best practices for Rust projects","url":"http://localhost:1315/blog/2021/04/github-actions-best-practices/"},{"body":"This week in Fluvio, I want to talk about an interesting problem I encountered while implementing a Batch Producer API for the Fluvio client. As part of our feature development process, we update each of our language clients with new APIs for interacting with the new functionality. This particular problem cropped up while I was implementing the Node client API for batch record producing, and has to do with passing Tuples from Node to Rust.\nOverview In this post, I\u0026rsquo;ll talk you through the problem solving journey I went on this week, by:\nDescribing the Rust API I wanted to call from Node Discovering why the Node API didn\u0026rsquo;t work out-of-the-box, and Explaining how I solved the problem with a patch to node-bindgen Send-All API In our batch producer API, the goal is to send multiple records to Fluvio in one request. Records are represented as a value and potentially a key. I chose to expose this functionality with a function that accepts any iterator over items of type (Option\u0026lt;K\u0026gt;, V). The elements of this tuple represent the potential key and the definite value. The specific function I wanted to write in Rust and call from Node is the following:\n%copy%\nasync fn send_all\u0026lt;I, K, V\u0026gt;(records: I) where K: Into\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, V: Into\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, I: IntoIterator\u0026lt;Item=(Option\u0026lt;K\u0026gt;, V)\u0026gt;, { /* ... */ } To be as flexible as possible, we\u0026rsquo;re using a couple of generics so that callers have more freedom with the values they can pass to us. An english description of this API might go something like this:\nProvide a value records which can be turned into an iterator over tuples (Option\u0026lt;K\u0026gt;, V), where K and V are each types that can be converted into byte buffers.\nHere are some quick examples of values you could provide to this API:\n%copy%\nlet records = vec![ (Some(\u0026#34;Hello\u0026#34;), String::from(\u0026#34;World!\u0026#34;)) ]; let records = Some( ( Some(vec![0x48, 0x65, 0x6c, 0x6c, 0x6f]), \u0026#34;World!\u0026#34; ) ); let records = { let mut r = HashSet::new(); r.insert( (Some(\u0026#34;Hello\u0026#34;), \u0026#34;World!\u0026#34;) ); r }; Now let\u0026rsquo;s think about what we want out of a Node API. In Javascript, we don\u0026rsquo;t really abstract over different types of iterators often enough for it to be relevant, so we can settle with passing a plain array. What about the elements of our array though? Javascript doesn\u0026rsquo;t technically have tuples, but a common pattern to use instead is simply choosing a fixed-size array and putting all of their values into it. In fact, typescript even supports annotating this situation! Let\u0026rsquo;s take a look at how this plays out:\n%copy%\ntype KeyValue = [string, string]; interface TopicProducer { sendAll(records: KeyValue[]): Promise\u0026lt;void\u0026gt; } I created the type alias KeyValue to represent our tuple type, where each item in the tuple is a string. Our function sendAll wants us to give it an array of these tuples.\nAlright, so now we\u0026rsquo;ve got a handle of the underlying Rust API that we want to bind to, and we know what we want our Node API to look like as well. Let\u0026rsquo;s take a look at how to glue them together using node-bindgen.\nNode-bindgen Node-bindgen is a Rust crate for automatically generating glue code for Node programs that want to interact with Rust functions and types. It works by providing an attribute, #[node_bindgen], that can be applied to functions and implementation blocks. At compile-time, node-bindgen generates conversion code for Node and Rust code to pass values back and forth, leveraging Node\u0026rsquo;s N-API. An example node-bindgen function might look like this:\n%copy%\nuse node_bindgen::derive::node_bindgen; #[node_bindgen] fn sum_all(ints: Vec\u0026lt;i32\u0026gt;) -\u0026gt; i32 { ints.iter().fold(0, |a, b| a + b) } The way this works is via a pair of traits for converting values to and from the Node representation. We have trait JSValue\u0026lt;'_\u0026gt;, which describes how to convert a JS value into a Rust value, and we have trait TryIntoJs, which does the opposite. This is nice because it allows us to define conversions for new types, as well as to define conversions for compound types, built from other convertible types.\nWe\u0026rsquo;ve already seen an example of a compound data type, Vec\u0026lt;i32\u0026gt;! Vec has a blanket implementation of JSValue for any other type T as long as T also implements JSValue. Great! So in principle, as long as we\u0026rsquo;re just combining basic types, everything should be fine and dandy, right? Let\u0026rsquo;s try it out with the API we want:\n%copy%\n#[node_bindgen] async fn send_all(records: Vec\u0026lt;(String, String)\u0026gt;) { // todo } When I compiled it, I was met with this error message:\n%copy first-line%\n$ cargo build error[E0277]: the trait bound `(String, String): JSValue\u0026lt;\u0026#39;_\u0026gt;` is not satisfied --\u0026gt; tuples/src/lib.rs:3:1 | 3 | #[node_bindgen] | ^^^^^^^^^^^^^^^ the trait `JSValue\u0026lt;\u0026#39;_\u0026gt;` is not implemented for `(String, String)` | = note: required because of the requirements on the impl of `JSValue\u0026lt;\u0026#39;_\u0026gt;` for `Vec\u0026lt;(String, String)\u0026gt;` = note: required because of the requirements on the impl of `ExtractArgFromJs\u0026lt;\u0026#39;_\u0026gt;` for `Vec\u0026lt;(String, String)\u0026gt;` = note: this error originates in an attribute macro (in Nightly builds, run with -Z macro-backtrace for more info) error: aborting due to previous error For more information about this error, try `rustc --explain E0277`. error: could not compile `nj-example-tuple` To learn more, run the command again with --verbose. Ok, well I didn\u0026rsquo;t actually think this was going to be easy, did I? From the looks of it, node-bindgen is missing implementations of JSValue for tuple types! But this is actually kind of cool, now I have a chance to make an improvement to node-bindgen that will be useful to other users even outside of Fluvio.\nImplementing JSValue for (A, B, ...) So now I know that I\u0026rsquo;ll need to write some new JSValue implementations. We will need a special implementation for each size of tuple that we want to support. Right away, it smells like a perfect recipe for a macro! But before diving straight into writing macros, I wanted to first try out this strategy with just one case and make sure it works.\nFor the first go-round, I\u0026rsquo;ll write an implementation for 2-Tuples. This implementation will need to be generic over the two component pieces, and it will require that both pieces also implement JSValue. Our impl block will look like this:\n%copy%\nimpl\u0026lt;\u0026#39;a, A, B\u0026gt; JSValue\u0026lt;\u0026#39;a\u0026gt; for (A, B) where A: JsValue\u0026lt;\u0026#39;a\u0026gt;, B: JsValue\u0026lt;\u0026#39;a\u0026gt;, { fn convert_to_rust(env: \u0026amp;\u0026#39;a JsEnv, js_value: napi_value) -\u0026gt; Result\u0026lt;Self, NjError\u0026gt; { // TODO } } Alright, so far so good, but how to we implement convert_to_rust? Well JsEnv is a type that represents the environment surrounding the value that we are attempting to convert to Rust. Contained somewhere within it are all the values that were in scope when the Node program called into our Rust function. We also have js_value, which I think of as a handle to the specific value that we want to retrieve from the environment.\nSo let\u0026rsquo;s take a step back for a moment and think about the values that we\u0026rsquo;ll actually be dealing with. In Node, a tuple is just a list of values. And we already have a way of receiving lists from Node, it\u0026rsquo;s the impl\u0026lt;T\u0026gt; JSValue\u0026lt;'_\u0026gt; for Vec\u0026lt;T\u0026gt;! Let\u0026rsquo;s take a look at how that works:\n%copy%\nimpl\u0026lt;\u0026#39;a, T\u0026gt; JSValue\u0026lt;\u0026#39;a\u0026gt; for Vec\u0026lt;T\u0026gt; where T: JSValue\u0026lt;\u0026#39;a\u0026gt;, { fn convert_to_rust(env: \u0026amp;\u0026#39;a JsEnv, js_value: napi_value) -\u0026gt; Result\u0026lt;Self, NjError\u0026gt; { use crate::sys::napi_get_array_length; if !env.is_array(js_value)? { return Err(NjError::Other(\u0026#34;not array\u0026#34;.to_owned())); } let mut length: u32 = 0; napi_call_result!(napi_get_array_length(env.inner(), js_value, \u0026amp;mut length))?; let mut elements = vec![]; for i in 0..length { let js_element = env.get_element(js_value, i)?; elements.push(T::convert_to_rust(env, js_element)?); } Ok(elements) } } Alright, so it looks like what\u0026rsquo;s happening is:\nFirst, we check with the JsEnv that the value given to us was actually an array. I think of this as the moral equivalent in JS of using Array.isArray(js_value).\nThen, we ask the environment to give us the length of the array that was passed to us.\nFinally, we loop over all the slots in the JS array and ask the environment for the object handle of the value at that position in the array. Then, since we are expecting all of those values to have the same type T, we use T\u0026rsquo;s own implementation of convert_to_rust to convert the element of the array and put it into our Vec.\nOk, awesome. I think there is a lot of information here that we can re-purpose for what we need to do for tuples. Here\u0026rsquo;s the plan for things we\u0026rsquo;ll do the same and things we\u0026rsquo;ll change about this process:\nSince we know that JS \u0026ldquo;tuples\u0026rdquo; are arrays, we\u0026rsquo;ll use the same techniques to check that our value is an array and to get values out of it.\nWe know that tuples have a fixed length, so we\u0026rsquo;ll check that the length of the array that is passed to us matches the number of elements in the tuple we are expecting. For our initial implementation with (A, B), we\u0026rsquo;ll make sure the array has length 2.\nWhen we convert the array elements to Rust, instead of using just a single type T for all the elements like Vec did, we\u0026rsquo;ll use A and B\u0026rsquo;s implementations of convert_to_rust for the corresponding elements of the array.\nAlright, let\u0026rsquo;s see if we can execute on this plan:\n%copy%\nimpl\u0026lt;\u0026#39;a, A, B\u0026gt; JSValue\u0026lt;\u0026#39;a\u0026gt; for (A, B) where A: JSValue\u0026lt;\u0026#39;a\u0026gt;, B: JSValue\u0026lt;\u0026#39;a\u0026gt;, { fn convert_to_rust(env: \u0026amp;\u0026#39;a JsEnv, js_value: napi_value) -\u0026gt; Result\u0026lt;Self, NjError\u0026gt; { use crate::sys::napi_get_array_length; if !env.is_array(js_value)? { return Err(NjError::Other(\u0026#34;not array\u0026#34;.to_owned())); } let required_length = 2; // Since this tuple has 2 elements let mut length: u32 = 0; napi_call_result!(napi_get_array_length(env.inner(), js_value, \u0026amp;mut length))?; if length != required_length { return Err(NjError::Other(format!(\u0026#34;Expected array of length {}\u0026#34;, required_length))); } let napi_value_a = env.get_element(js_value, 0)?; let a = A::convert_to_rust(env, napi_value_a)?; let napi_value_b = env.get_element(js_value, 1)?; let b = B::convert_to_rust(env, napi_value_b)?; let tuple = (a, b); Ok(tuple) } } Ok, let\u0026rsquo;s try it out with the send_all function and see if it worked!\n%copy first-line%\n$ cargo build Compiling nj-core v4.2.0 (/Users/nick/infinyon/node-bindgen/nj-core) Compiling node-bindgen v4.3.0 (/Users/nick/infinyon/node-bindgen) Compiling nj-example-tuple v0.1.0 (/Users/nick/infinyon/node-bindgen/examples/tuples) warning: unused variable: `records` --\u0026gt; tuples/src/lib.rs:4:19 | 4 | async fn send_all(records: Vec\u0026lt;(String, String)\u0026gt;) { | ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_records` | = note: `#[warn(unused_variables)]` on by default warning: 1 warning emitted Finished dev [unoptimized + debuginfo] target(s) in 4.05s Woohoo! We\u0026rsquo;ve got it compiling. Let\u0026rsquo;s write a quick body that just prints everything it receives so that we can just check that everything Really Actually Works.\n%copy%\n#[node_bindgen] async fn send_all(records: Vec\u0026lt;(String, String)\u0026gt;) { for (key, value) in records { println!(\u0026#34;Got Key={}, Value={}\u0026#34;, key, value); } } We\u0026rsquo;ll call it from a typescript module like this:\n%copy%\nconst assert = require(\u0026#39;assert\u0026#39;); let addon: TestTuples = require(\u0026#39;./dist\u0026#39;); type KeyValue = [string, string]; interface TestTuples { sendAll(records: KeyValue[]): Promise\u0026lt;void\u0026gt; } const records: KeyValue[] = [ [\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;] ]; addon.sendAll(records) And the moment of truth:\n%copy first-line%\n$ npx ts-node ./test.ts Got Key=Apple, Value=Banana Great! We can now pass tuples from Node to Rust, and everything gets converted as expected! I won\u0026rsquo;t write it all out here, but I did test with other permutations of types, array lengths, etc., and everything works pretty much how you\u0026rsquo;d expect it to. If you want to see a bit more of an example, check out the new node-bindgen example for tuples :)\nBonus: Writing a macro to implement more tuples Alright, so we\u0026rsquo;ve written enough to fit our use-case, but we can\u0026rsquo;t stop there! We want to be able to pass other sizes of tuples between Rust and Node. However, we don\u0026rsquo;t want to have to hand-write the implementations for each size of tuple, so let\u0026rsquo;s see if we can make a macro do the job for us!\nWhat we want is a macro that looks something like this:\nimpl_js_value_for_tuple!(A); impl_js_value_for_tuple!(A, B); impl_js_value_for_tuple!(A, B, C); Ok, so let\u0026rsquo;s give this a shot:\n%copy%\nmacro_rules! impl_js_value_for_tuple_first_attempt { ( $( $t:ident ),+ $(,)? ) =\u0026gt; { impl\u0026lt;\u0026#39;a $(, $t)+ \u0026gt; crate::JSValue\u0026lt;\u0026#39;a\u0026gt; for ($($t,)+) where $($t: JSValue\u0026lt;\u0026#39;a\u0026gt; + Send,)+ { fn convert_to_rust(env: \u0026amp;\u0026#39;a JsEnv, js_value: napi_value) -\u0026gt; Result\u0026lt;Self, NjError\u0026gt; { use crate::sys::napi_get_array_length; if !env.is_array(js_value)? { return Err(NjError::Other(\u0026#34;Tuples must come from JS arrays\u0026#34;.to_owned())); } let mut length: u32 = 0; napi_call_result!(napi_get_array_length(env.inner(), js_value, \u0026amp;mut length))?; let required_length = ???; if length != required_length { return Err(NjError::Other(format!(\u0026#34;{n}Tuple must have exactly length {n}\u0026#34;, n = required_length))); } $( let js_element = env.get_element(js_value, ???)?; #[allow(non_snake_case)] let $t = $t::convert_to_rust(env, js_element)?; )+ Ok(( $($t,)+ )) } } } } So this comes pretty close, but there are a couple problems. Firstly, we want each tuple implementation to check that the provided Node array has the same length as the tuple type we are dealing with, but this form of macro doesn\u0026rsquo;t provide any amenities for counting the size of the type, or for selecting individual elements from the array by index. There are some tricks that we might be able to leverage from the little book of macros, but that seems like it would make things much more complicated than I want to deal with.\nWell, let\u0026rsquo;s look for inspiration. I know I\u0026rsquo;ve seen libraries use this pattern of implementing traits for compound types, where the component types implement the traits. Let\u0026rsquo;s take a look at serde and see how that handles tuples.\nAha! It looks like serde does almost the exact same thing I tried to do, it just does it in a way that is straightforward and actually works! The serde tuple_impls macro takes a $len expression with the number of elements in the tuple, identifiers for the generic parameter, and tokens $n:tt indicating the indices of each item in the tuple.\nLet\u0026rsquo;s see if we can leverage this pattern to do what we want.\n%copy%\nmacro_rules! impl_js_value_for_tuple { ( $( $len:expr =\u0026gt; ( $( $n:tt $t:ident ),+ $(,)? ))+ ) =\u0026gt; { $( impl\u0026lt;\u0026#39;a $(, $t)+ \u0026gt; crate::JSValue\u0026lt;\u0026#39;a\u0026gt; for ($($t,)+) where $($t: JSValue\u0026lt;\u0026#39;a\u0026gt; + Send,)+ { fn convert_to_rust(env: \u0026amp;\u0026#39;a JsEnv, js_value: napi_value) -\u0026gt; Result\u0026lt;Self, NjError\u0026gt; { use crate::sys::napi_get_array_length; if !env.is_array(js_value)? { return Err(NjError::Other(\u0026#34;Tuples must come from JS arrays\u0026#34;.to_owned())); } let mut length: u32 = 0; napi_call_result!(napi_get_array_length(env.inner(), js_value, \u0026amp;mut length))?; let required_length = $len; if length != required_length { return Err(NjError::Other(format!(\u0026#34;{n}Tuple must have exactly length {n}\u0026#34;, n = required_length))); } $( let js_element = env.get_element(js_value, $n)?; #[allow(non_snake_case)] let $t = $t::convert_to_rust(env, js_element)?; )+ Ok(( $($t,)+ )) } } )+ } } impl_js_value_for_tuple! { 1 =\u0026gt; (0 T0) 2 =\u0026gt; (0 T0, 1 T1) 3 =\u0026gt; (0 T0, 1 T1, 2 T2) 4 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3) 5 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3, 4 T4) 6 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3, 4 T4, 5 T5) 7 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3, 4 T4, 5 T5, 6 T6) 8 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3, 4 T4, 5 T5, 6 T6, 7 T7) 9 =\u0026gt; (0 T0, 1 T1, 2 T2, 3 T3, 4 T4, 5 T5, 6 T6, 7 T7, 8 T8) } Bingo! We now have working implementations for converting tuple types from Node to Rust. This pattern also works out in the opposite direction, and I was able to write a macro to impl TryIntoJs for tuples as well! Mission accomplished, we can now pass values both ways in tuples.\nConclusion Thanks for reading this far! I hope you enjoyed my little foray into the world of Rust/Node glue code with node-bindgen, I know I had a lot of fun working with the blanket impls and tuple macros, it gives me the warm fuzzy feeling of writing slick composable code 😎.\nIf you\u0026rsquo;d like to learn more about the work we do on Fluvio or node-bindgen, be sure to check out our website at fluvio.io and our Github repo. If you have any questions or want to learn more about Fluvio, you can come talk to us on our Discord server. Until next time!\nQuick links: Getting started with Fluvio Fluvio CLI reference Fluvio Architecture ","description":"How I added tuple support to node-bindgen.","keywords":null,"summary":"This week in Fluvio, I want to talk about an interesting problem I encountered while implementing a Batch Producer API for the Fluvio client. As part of our feature development process, we update each of our language clients with new APIs for interacting with the new functionality. This particular problem cropped up while I was implementing the Node client API for batch record producing, and has to do with passing Tuples from Node to Rust.","title":"Sending tuples from Node to Rust and back","url":"http://localhost:1315/blog/2021/04/node-bindgen-tuples/"},{"body":"This week, we\u0026rsquo;re happy to announce the addition of a Key/Value API for Fluvio producers and consumers! The ability to define a Key for your records gives you more control over how your data is distributed and stored within Fluvio. In this blog, we\u0026rsquo;ll talk more about the guarantees that key/value records give you, as well as how to use key/value records from Fluvio\u0026rsquo;s various producers and consumers.\nWhat are Key/Value records and why use them? Key/Value records are all about determining which partition each record gets sent to. The golden rule is: records with the same key always go to the same partition. This is great, because we also know that all records that go to the same partition will be well-ordered, and will be consumed in the same order they were produced. Generally, you would pick some property of your data to use as the key, such as an account ID or a username, so that all records belonging to the same user will be delivered in order. This also means that records belonging to different users may be distributed across different partitions, making the system free to spread traffic out across multiple servers and increase throughput.\nUsing Key/Value records with the Fluvio CLI In this section, we\u0026rsquo;ll be showing fluvio commands for producing and consuming key/value records. If you want to follow along, make sure you\u0026rsquo;ve followed the getting started guide and set up a Fluvio cluster, either locally or with a free Infinyon Cloud account.\nOnce the cluster is set up, create a fresh topic to use for this example:\n%copy first-line%\n$ fluvio topic create bank-transactions Producing key/value records from the CLI The producer and consumer built into Fluvio\u0026rsquo;s CLI can send and receive key/value records. Let\u0026rsquo;s look at a quick example of producing data from a text file.\n%copy first-line%\n$ cat transactions.txt alice=Deposit 100.00 bob=Withdraw 50.00 bob=Withdraw 25.00 Here we have a file, transactions.txt, with keys and values separated by a = and with one record on each line of the file. We can use the following command to send each line as a key/value record:\n%copy first-line%\n$ fluvio produce bank-transactions -v --key-separator \u0026#34;=\u0026#34; -f transactions.txt [alice] Deposit 100.00 [bob] Withdraw 50.00 [bob] Withdraw 25.00 Ok! Let\u0026rsquo;s break down this command:\nfluvio produce is how we start up the producer key-value-text is the name of the topic we want to produce to -v or (--verbose) tells the producer to print each record after it\u0026rsquo;s sent --key-separator \u0026quot;=\u0026quot; tells the producer to split each line on an =, using the left side as the key and the right side as the value -f transactions.txt tells the producer to read data from the transactions.txt file We can tell that the producer recognized the keys correctly because it prints them back out in square brackets. Next, let\u0026rsquo;s look at how to use a consumer to read back records that have been stored.\nConsuming key/value records with the CLI Let\u0026rsquo;s get right to it and consume our records:\n%copy first-line%\n$ fluvio consume bank-transactions -B -d Deposit 100.00 Withdraw 50.00 Withdraw 25.00 By default, the consumer does not print the keys of each record. This highlights the fact that key/value records are the same as regular records, they just happen to have keys. We can tell the consumer to print the keys that belong to each record with --key-value:\n%copy first-line%\n$ fluvio consume bank-transactions -B -d --key-value [alice] Deposit 100.00 [bob] Withdraw 50.00 [bob] Withdraw 25.00 Key/Value records using the Rust API If you\u0026rsquo;re writing an application in Rust and want to send key/value records to Fluvio, you can use the new key/value APIs of the fluvio crate. Let\u0026rsquo;s set up a project with everything we need.\n%copy first-line%\n$ cargo new rusty-streams \u0026amp;\u0026amp; cd rusty-streams For this project, we\u0026rsquo;ll need the fluvio crate, as well as an async runtime and some Futures helpers. Update your Cargo.toml to include these dependencies:\n%copy%\n# Cargo.toml [dependencies] fluvio = \u0026#34;0.6.0\u0026#34; async-std = { version = \u0026#34;1\u0026#34;, features = [\u0026#34;attributes\u0026#34;] } The attributes feature from async_std will let us write async code directly in main!\nProducing from Rust Alright, let\u0026rsquo;s write a function that counts from 0 to 4, sending a key/value record where the key is the number, and the value is a string with the number in it.\n%copy%\n#[async_std::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let producer = fluvio::producer(\u0026#34;rusty-topic\u0026#34;).await?; for i in 0..5 { producer.send(i.to_string(), format!(\u0026#34;This is rusty record {}\u0026#34;, i)).await?; println!(\u0026#34;Sent record {}\u0026#34;, i); } /// Ensure that the producer batches are sent to the SPU. producer.flush().await?; Ok(()) } For this new example, we\u0026rsquo;re using a new topic name, so let\u0026rsquo;s not forget to create the topic!\n%copy first-line%\n$ fluvio topic create rusty-topic Let\u0026rsquo;s run our producer and check that we get the expected output:\n%copy first-line%\n$ cargo run Sent record 0 Sent record 1 Sent record 2 Sent record 3 Sent record 4 To check if the records were sent, let\u0026rsquo;s use the handy-dandy CLI consumer.\n%copy first-line%\n$ fluvio consume rusty-topic -B -d --key-value [0] This is rusty record 0 [1] This is rusty record 1 [2] This is rusty record 2 [3] This is rusty record 3 [4] This is rusty record 4 Hooray, our producer worked! Let\u0026rsquo;s rewrite main to test out the consumer API in Rust:\n%copy%\nuse async_std::stream::StreamExt; #[async_std::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let consumer = fluvio::consumer(\u0026#34;rusty-topic\u0026#34;, 0).await?; let mut stream = consumer.stream(fluvio::Offset::beginning()).await?; while let Some(Ok(record)) = stream.next().await { // Let\u0026#39;s convert the key and value into Strings let key: Option\u0026lt;String\u0026gt; = record.key().map(|key| String::from_utf8_lossy(key).to_string()); let value: String = String::from_utf8_lossy(record.value()).to_string(); println!(\u0026#34;Consumed record! Key={:?}, value={}\u0026#34;, key, value); } Ok(()) } Let\u0026rsquo;s run our consumer and see what we get.\n%copy first-line%\n$ cargo run Consumed record! Key=Some(\u0026#34;0\u0026#34;), value=This is rusty record 0 Consumed record! Key=Some(\u0026#34;1\u0026#34;), value=This is rusty record 1 Consumed record! Key=Some(\u0026#34;2\u0026#34;), value=This is rusty record 2 Consumed record! Key=Some(\u0026#34;3\u0026#34;), value=This is rusty record 3 Consumed record! Key=Some(\u0026#34;4\u0026#34;), value=This is rusty record 4 ^C Sweet! We\u0026rsquo;ve now seen how we can quickly and easily write code to produce and consume key/value records from our Rust applications. Next, let\u0026rsquo;s take a look at how we can do the same with Fluvio\u0026rsquo;s Node.js API.\nKey/Value records using the Node.js API For our Node.js app, we\u0026rsquo;ll set up a simple typescript project and pull in the Fluvio library from npm.\n%copy%\nmkdir nodejs-streams \u0026amp;\u0026amp; cd nodejs-streams npm init -y npm install -D typescript ts-node @types/node npm install -S @fluvio/client touch producer.ts consumer.ts Now we have our project set up, and we\u0026rsquo;re ready to write the code for our producer.ts and consumer.ts files. Let\u0026rsquo;s start out by writing our producer.\nProducing from Node.js %copy%\n// producer.ts import Fluvio from \u0026#34;@fluvio/client\u0026#34;; const fluvio = new Fluvio(); const produce = async () =\u0026gt; { await fluvio.connect(); const producer = await fluvio.topicProducer(\u0026#34;nodejs-topic\u0026#34;); for (let i = 0; i \u0026lt; 5; i++) { let key = i.toString(); let value = `This is nodejs record ${i}`; await producer.send(key, value); console.log(`Sent record ${i}`); } }; produce(); Before we run it, let\u0026rsquo;s remember to create our new topic:\n%copy first-line%\n$ fluvio topic create nodejs-topic And let\u0026rsquo;s take our producer for a spin!\n%copy first-line%\n$ npx ts-node producer.ts Sent record 0 Sent record 1 Sent record 2 Sent record 3 Sent record 4 Awesome, let\u0026rsquo;s check that we received everything using the CLI\n%copy first-line%\n$ fluvio consume nodejs-topic -B -d --key-value [0] This is nodejs record 0 [1] This is nodejs record 1 [2] This is nodejs record 2 [3] This is nodejs record 3 [4] This is nodejs record 4 We\u0026rsquo;re nailing it 😎. Only one more example to go, the Node.js consumer.\nConsuming from Node.js Our node project is already set up, let\u0026rsquo;s just go ahead and write our consumer.ts code.\n%copy%\n// consumer.ts import Fluvio, { Offset } from \u0026#34;@fluvio/client\u0026#34;; const fluvio = new Fluvio(); const consume = async () =\u0026gt; { await fluvio.connect(); const consumer = await fluvio.partitionConsumer(\u0026#34;nodejs-topic\u0026#34;, 0); const stream = await consumer.createStream(Offset.FromBeginning()); for await (const record of stream) { const key = record.keyString(); const value = record.valueString(); console.log(`Consumed record! Key=${key}, value=${value}`); } }; consume(); And the moment of truth 🤞\n%copy first-line%\n$ npx ts-node consumer.ts Consumed record! Key=0, value=This is nodejs record 0 Consumed record! Key=1, value=This is nodejs record 1 Consumed record! Key=2, value=This is nodejs record 2 Consumed record! Key=3, value=This is nodejs record 3 Consumed record! Key=4, value=This is nodejs record 4 ^C Summary We hope you enjoyed this quick tour of key/value records in Fluvio. Feel free to check out our Github, file any feature requests, or ask us questions in our community Discord. We\u0026rsquo;d love to hear about your streaming use-cases and help to make Fluvio the best streaming platform it can be!\nUntil next time!\nQuick links: Getting started with Fluvio Fluvio CLI reference Fluvio Architecture ","description":"Learn how to use key/value records to control how your data is distributed and stored within Fluvio.","keywords":null,"summary":"This week, we\u0026rsquo;re happy to announce the addition of a Key/Value API for Fluvio producers and consumers! The ability to define a Key for your records gives you more control over how your data is distributed and stored within Fluvio. In this blog, we\u0026rsquo;ll talk more about the guarantees that key/value records give you, as well as how to use key/value records from Fluvio\u0026rsquo;s various producers and consumers.\nWhat are Key/Value records and why use them?","title":"Stream Key/Value records in Fluvio","url":"http://localhost:1315/blog/2021/03/key-value-records/"},{"body":"This week, we\u0026rsquo;re happy to announce the addition of a Python client library for Fluvio. Using the Python client is just as easy as using our other clients. Check out the hello world in Python tutorial or documentation for usage.\nIn this post, we\u0026rsquo;ll talk about how we were able to leverage some of the great Rust tools to build a Python client without writing much Python itself.\nOverview In short, we will:\nuse flapigen to define how our Rust structs will go across the FFI. use the rust-cpython extension in our Python project and call it. Setup To get started, we\u0026rsquo;ll create a new project folder that\u0026rsquo;s set up for both Rust and Python, using cargo and venv.\n%copy%\ncargo new --lib my-python-lib cd my-python-lib python -m venv venv source venv/bin/activate The above creates a Rust crate named my-python-lib, then sets-up a Python virtual environment.\n-\u0026gt; Note: You\u0026rsquo;ll need to have the rust toolchain and python 3.6 or above installed.\nRust glue We\u0026rsquo;ll need to add this to your Cargo.toml:\n%copy%\n[lib] crate-type = [\u0026#34;cdylib\u0026#34;] [dependencies] cpython = { version = \u0026#34;0.5\u0026#34;, features = [\u0026#34;extension-module\u0026#34;] } [build-dependencies] flapigen = \u0026#34;0.6.0-pre7\u0026#34; The crate-type = [\u0026quot;cdylib\u0026quot;] tells Rust to build our crate as a C-compatible dynamic library rather than a typical crate. This crate type will allow our Python code to interact with our library as if it were compiled C code rather than Rust.\nNow we\u0026rsquo;ll create a build script in build.rs by adding the following:\n%copy%\nuse flapigen::{LanguageConfig, PythonConfig}; use std::{env, path::Path}; fn main() { let in_src = Path::new(\u0026#34;src\u0026#34;).join(\u0026#34;glue.rs.in\u0026#34;); let out_dir = env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap(); let out_src = Path::new(\u0026amp;out_dir).join(\u0026#34;glue.rs\u0026#34;); let python_cfg = PythonConfig::new(\u0026#34;my_python_lib\u0026#34;.to_owned()); let flap_gen = flapigen::Generator::new(LanguageConfig::PythonConfig(python_cfg)).rustfmt_bindings(true); flap_gen.expand(\u0026#34;python bindings\u0026#34;, \u0026amp;in_src, \u0026amp;out_src); println!(\u0026#34;cargo:rerun-if-changed={}\u0026#34;, in_src.display()); } The code sets up flapigen to run on our project. At build time, it will read the \u0026ldquo;glue code\u0026rdquo; we write in src/glue.rs.in, and generate Rust code to interact with Python and place it in ${OUT_DIR}/glue.rs.\nNow we\u0026rsquo;ll add a src/glue.rs.in file with something like the following:\n%copy%\npub struct Foo { val: i32 } impl Foo { pub fn new(val: i32) -\u0026gt; Self { Self { val } } pub fn set_field(\u0026amp;mut self, new_val: i32) { self.val = new_val; } pub fn val(\u0026amp;self) -\u0026gt; i32 { self.val } } foreign_class!(class Foo { self_type Foo; constructor Foo::new(_: i32) -\u0026gt; Foo; fn Foo::set_field(\u0026amp;mut self, _: i32); fn Foo::val(\u0026amp;self) -\u0026gt; i32; }); This simple example was published in the flapigen book and we can copy and paste it here.\nThe src/lib.rs should currently have some basic tests. We\u0026rsquo;ll change it to the following:\n%copy%\n#![allow(non_snake_case, unused)] include!(concat!(env!(\u0026#34;OUT_DIR\u0026#34;), \u0026#34;/glue.rs\u0026#34;)); This is a typical Rust pattern when using build scripts. The code takes the file in ${OUT_DIR}/glue.rs and includes the contents into src/lib.rs in the build directory. The result will be as if we hand-wrote the generated code in our lib.rs file.\nThis section uses flapigen to expand the foreign_class macro into many cpython functions as an extension module, and cargo compiles it as a cdylib. If you want to see what that looks like, install cargo-expand and run cargo expand. You\u0026rsquo;ll get a lot of generated rust code.\nPython Glue In the setup, we created a virtual environment, and now we\u0026rsquo;ll need to install some Python tools via:\n%copy first-line%\n$ source venv/bin/activate \u0026amp;\u0026amp; pip install setuptools-rust Now to create a python package, you create a file called setup.py with:\n%copy%\nfrom setuptools import setup from setuptools_rust import Binding, RustExtension setup( name=\u0026#34;my-python-lib\u0026#34;, version=\u0026#34;1.0\u0026#34;, rust_extensions=[RustExtension(\u0026#34;my_python_lib\u0026#34;, binding=Binding.RustCPython)], # rust extensions are not zip safe, just like C-extensions. zip_safe=False, ) This is the most basic setuptools-rust setup, except for using RustCPython as the binding.\nTo build the Rust and the Python packages just run python setup.py develop. Python calls cargo and moves cdylib into your local directory.\nTesting it all out Create a simple.py script with the following in it:\n%copy%\nfrom my_python_lib import Foo foo = Foo(1) print(foo.val()) foo.set_field(11) print(foo.val()) Running the script via python simple.py should result in:\n%copy first-line%\n$ python simple.py 1 11 And there you go, you\u0026rsquo;ve called Rust from Python!\nConclusion You can get the source for this post in our fluvio-demo-apps-rust repository.\nThese are just the basics for setting up a Python wrapper. In the Fluvio Python Client, the Rust crate is _fluvio_python; a private python module that wraps the rust structs with python classes, giving us nice documentation generation.\nPackaging, testing, and publishing on pypi is beyond the scope of this blog. Checkout the Makefile and the github publishing workflow for additional information.\n","description":"Learn how to wrap your Rust crate in Python.","keywords":null,"summary":"This week, we\u0026rsquo;re happy to announce the addition of a Python client library for Fluvio. Using the Python client is just as easy as using our other clients. Check out the hello world in Python tutorial or documentation for usage.\nIn this post, we\u0026rsquo;ll talk about how we were able to leverage some of the great Rust tools to build a Python client without writing much Python itself.\nOverview In short, we will:","title":"How we built our Python Client that's mostly Rust","url":"http://localhost:1315/blog/2021/03/python-client/"},{"body":"Today we are pleased to announce The Infinyon Cloud Platform, the fastest and easiest way to get started with Fluvio.\nInfinyon Cloud is now in alpha, and you can create a free account using the link below:\nSign Up for Infinyon Cloud About Fluvio Our research has shown that modern businesses require real-time collaboration, analysis, and adaptation. Yet, building real-time infrastructure is a painful, expensive, and error-prone endeavor. This is why we built Fluvio - an open-source, high-performance distributed data streaming platform for real-time apps written in Rust.\nBuilding a streaming app or data pipeline is just half the battle; the other is getting it deployed. We created Infinyon Cloud to provision and manage the Fluvio cluster for you. Getting started is as simple as creating an account and installing the Fluvio CLI - our all-in-one tool for working with Fluvio.\nFluvio allows developers to build real-time quickly applications using native language libraries such as Node.js and Rust. Fluvio clients uses modern programming language constructs such as async streams for speed of development and ease of use.\nThere are many reasons why Fluvio is awesome, but this is a post about using Infinyon Cloud. \u0026#x1f609;\nUsing The Infinyon Cloud Platform There are several [blog posts]http://localhost:1315/blog/) and tutorials that show the power of Fluvio, so let\u0026rsquo;s get set up.\nSetup Setting up the Infinyon Cloud is straightforward; sign up for an account and check your email for a confirmation. Fluvio cluster will be automatically provisioned for you after confirming the email.\nThen to make use of it, just run\n%copy first-line%\n$ fluvio cloud login The command will prompt you for the credentials you used to sign up in the web form. Upon validation, Fluvio retrieves your cloud settings and saves them in your local profile.\nCheck out your available profiles at:\n%copy first-line%\n$ fluvio profile view If you already have a local instance of Fluvio running, you can use the CLI to switch from one Fluvio cluster to another. The following command informs the Fluvio client to use Infinyon Cloud:\n%copy first-line%\n$ fluvio profile switch cloud Produce/Consume using CLI On a fresh Fluvio installation, you\u0026rsquo;ll need to create topics:\n%copy first-line%\n$ fluvio topic create hello-infinyon-cloud Getting data in and out of the cloud is easy - open two terminals, one for the producer and the other for the consumer.\nIn one terminal run:\n%copy first-line%\n$ fluvio consume hello-infinyon-cloud and in the another:\n%copy first-line%\n$ fluvio produce hello-infinyon-cloud The produce command listens on stdin messages and sends them to Infinyon Cloud on the specified topic. In our case, the topic is hello-infinyon-cloud. Now, type Hello Infinyon Cloud! (or whatever you\u0026rsquo;d like) into the producer terminal.\nYour two terminals should now look like this:\n%copy first-line%\n$ fluvio produce hello-infinyon-cloud Hello Infinyon Cloud! Ok! %copy first-line%\n$ fluvio consume hello-infinyon-cloud Hello Infinyon Cloud! To exit, press \u0026lt;CTRL\u0026gt;-C.\nProduce/Consume using Rust/Node.JS You can also use produce and consume programmatically, using our rust client or nodejs client. The profile ensures Fluvio clients will use the appropriate cluster, in this case, Infinyon Cloud.\nThe following code shows a producer written in Node.js and a consumer written in Rust. (see our tutorials for a complete step-by-step):\n%copy%\nconst fluvio = await Fluvio.connect(); const producer = await fluvio.topicProducer(\u0026#39;hello-infinyon-cloud\u0026#39;); await producer.sendRecord(\u0026#34;Hello Infinyon Cloud! 🎉\u0026#34;); and\n%copy%\nlet consumer = fluvio::consumer(\u0026#34;hello-infinyon-cloud\u0026#34;, 0).await?; let mut stream = consumer.stream(Offset::beginning()).await?; while let Some(Ok(record)) = stream.next().await { let string = String::from_utf8_lossy(\u0026amp;record.as_ref()); println!(\u0026#34;Got record: {}\u0026#34;, string); } Run the rust version in one terminal and the node version in another. The rust consumer will print:\nGot Record: Hello Infinyon Cloud! 🎉 All it takes is a few lines of code, and traffic is flowing. All your cluster maintenance headaches are gone.\nPricing and AWS Region Availability Infinyon Cloud is currently available in one AWS region in the U.S. (N. Virginia). We plan to add additional region options in the U.S. and Europe and expanding to other parts of the world in the near future.\nInfinyon Cloud is in alpha, and it is free to use. We will update pricing information in the near future. Fluvio users that sign-up for our Beta program will receive additional discounts (connect to us in Discord for details).\nFluvio Platform Highlights The following is a shortlist of benefits you gain by using Fluvio:\nDeclarative Management: Fluvio allows operators to declare the desired state, and the system will do the rest. No resource available, no worries, the objects are shown in progress until the resource constraints are resolved.\nLow Latency: Fluvio takes advantage of Rust’s async runtime and all available cores. It also interacts directly with hardware I/O to achieve predictable ultra-low latency.\nLow Memory Footprint: Fluvio is a highly optimized machine code, and it does not require an intermediary virtual machine. Fluvio can run anywhere from Raspberry Pi to multi-core systems.\nBuilt-in Retention: Fluvio uses long-lived immutable storage to persist data streams. Each data stream is replicated for persistence and reliability.\nGuaranteed Message Ordering: Fluvio guarantees partition-level message ordering. Messages are stored and forwarded to consumers in the order they are received from the producers.\nCloud Native by Design: Fluvio is designed to work natively with Kubernetes. It uses Helm charts for installation, CRDs for provisioning, and Operators to interact with the KV store.\nDeveloper Friendly: Fluvio offers native language bindings in many modern programming languages such as Rust and Node (Python, Java, and Swift will be available in future releases).\nPowerful CLI: Fluvio CLI can manage all your clusters whether installed locally, in your data center, or a public cloud.\nSummary Setting up a real-time data streaming app should be easy and painless. Hopefully, this post shows you just how simple yet powerful our platform is.\nDon\u0026rsquo;t forget to join the conversation on Discord, follow the project on github or open an issue if you find a bug or have a feature request.\n","description":"Infinyon Cloud, a low latency data streaming platform for App developers like us.","keywords":null,"summary":"Today we are pleased to announce The Infinyon Cloud Platform, the fastest and easiest way to get started with Fluvio.\nInfinyon Cloud is now in alpha, and you can create a free account using the link below:\nSign Up for Infinyon Cloud About Fluvio Our research has shown that modern businesses require real-time collaboration, analysis, and adaptation. Yet, building real-time infrastructure is a painful, expensive, and error-prone endeavor. This is why we built Fluvio - an open-source, high-performance distributed data streaming platform for real-time apps written in Rust.","title":"Announcing Infinyon Cloud Platform","url":"http://localhost:1315/blog/2021/03/announcing-fluvio-cloud-platform/"},{"body":"Fluvio Policy gives security operators and data owners centralized control over all data exchanges in the organization. The policies follow the four stages - connect, protect, enrich, and store - of the Fluvio data streaming pipeline.\nPolicies can be assigned to one, all, or groups of data streams. When used in combination with role-based access control (RBAC), the policies are a potent tool to control data movement across teams, tools, and geo-locations in the organization.\nFor example, an organization that needs HIPPA compliance may create two global policies: one for consumers who have legitimate access to patient data and another for everyone else. The first policy ensures all traffic is encrypted with a HIPPA key and exclusively sent to authorized consumers. The second policy strips off all sensitive information. These two policies are then applied globally and enforced for all data streams.\nRole-Based Access Control Fluvio uses role-based access controls (RBAC) to maintain fine-grained access to system resources. Each identified user can access objects assigned to his role and restricted from all others.\nFor example, a user with a security role may apply security policies, whereas users with a developer role may only assign connection policies.\nObject Profiles Profiles are configuration parameters applied to one or more groups of objects. For example, a client profile uniquely identifies an endpoint, a QoS profile defines the quality of service parameters applied to a connection, or a storage profile defines the minimum number of replicas and the flush frequency.\nEach profile is defined in the context of the policy where it is used.\nIngress/Egress Rules Fluvio applies policies in the sequence derived from the internal pipeline. These policies can are divided into two logical groups: ingress and egress.\nIn Fluvio, data writers are called producers and readers consumers.\nIngress policies control consumers\u0026rsquo; data as it enters the stream, and apply rules to receive, permit, filter, map, transform, and write to the store.\nEgress policies restrict the data as it leaves the stream and apply rules to filter, map, transform, authorize, and send to consumers.\nConnection Policies Connection policies define the rules associated with the access and the transport properties of a data stream. For example, TLS rules may specify that only TLS 1.3 enabled clients can access the stream. Geo-location rules may restrict clients to the same country where the stream is stored.\nThe system can apply policies on either side of a data stream - receive rules for ingress and send for egress. Receive rules allow authorized producers to connect to data streams and block all others. Send rules ensure unauthorized consumers are blocked.\nProfiles Connection policies use the following profiles:\nClient Profiles - A client profile is a configuration file that uniquely identifies a client to the Fluvio cluster. The profile contains parameters such as security certificates, authorization keys, and settings required by connection policies. QOS Profiles - A quality of service profile defines the connection parameters such bandwidth limit, prioritization, compression and more. Receive Policy Rules A receive policy describes the match criteria a producer must meet to connect to a specific data stream. The policy may also apply quality of service parameters to the connection. The criteria definition is defined in terms of:\nproducer location client profile QOS profile For example:\nProducers with IP addresses from outside of Europe must be denied access to data streams labeled \u0026ldquo;Germany.\u0026rdquo; Producers with \u0026ldquo;development\u0026rdquo; profiles must be denied access to data streams labeled \u0026ldquo;production.\u0026rdquo; Producers with \u0026ldquo;low priority\u0026rdquo; profiles are allowed to use up to 1% of the bandwidth. Send Policy Rules A send policy describes the match criteria a consumer must meet to connect to a specific data stream and the quality of service parameters to apply. The criteria definition is defined in terms of:\nconsumer location client profile QOS profile For example:\nConsumers with IP addresses from outside of Europe must be denied access to data streams labeled \u0026ldquo;Germany.\u0026rdquo; Consumers with \u0026ldquo;development\u0026rdquo; profiles must be denied access to data streams labeled \u0026ldquo;finance.\u0026rdquo; Consumers with \u0026ldquo;high priority\u0026rdquo; profiles must receive data before all other consumers. Security Policies Security policies define rules that permit incoming and authorize outgoing data. For example, an encryption rule may specify that all fields containing personal identifiable information (PII) must be encrypted with a PII key. Permission rules may state that all data that have credit card information must be rejected. Authorization rules may specify that all PII fields must be stripped when sending data to untrusted clients.\nFluvio can apply security policies to either side of the data stream - permit for ingress and authorize for egress.\nProfiles Security policies use the following profiles:\nClient Profiles - A client profile uniquely identifies a client and its security parameters. Trust Profiles - Trust profiles are associated with the metadata definitions. A metadata definition is trusted if it does not contain sensitive or private information. The data is declared trusted if it matches a trusted metadata definition. Permit Policy Rules A permit policy defines permission rules that ensure data is adequately protected or rejected before written to the store.Permit policies are defined in terms of:\nclient profile trust profile For example:\nData containing sensitive or PII information must be rejected. Data containing sensitive or PII information must be encrypted with a PII key. Authorize Policy Rules An authorize policy defines authorization rules that ensure no sensitive data is sent to unauthorized consumers. Authorize policies are defined in terms of:\nclient profile trust profile For example:\nSensitive or PII data must not be sent to consumers with unauthorized client profile. Sensitive or PII data must be stripped when sent to consumers with unauthorized client profile. Control Policies Control policies define rules to manage data enrichment such as filter, transform, and map (F.T.M.). For example, a filter rule may specify that users with invalid email addresses must be rejected. Transform rules may request to compute the tax field for a customer order based on geo-location - where the algorithm may be a static function or a dynamically loaded module. Map rules may specify that all credit card fields must be masked with a random value.\nFluvio can apply control policies to the ingress or egress side of the data stream.\nProfiles Controller policies use the following profiles:\nClient Profiles - A client profile uniquely identifies a client. Ingress F.M.T. Policy Rules Ingress F.M.T. policy defines rules to reject or manipulate data before it is written to store.\nFor example:\nFor data matching \u0026ldquo;add order\u0026rdquo; event, compute the tax field. For data matching \u0026ldquo;add customer\u0026rdquo; event, check email address and reject if invalid. Egress F.M.T. Policy Rules Egress F.M.T. policy defines rules to reject or manipulate data before it is set to the consumers.\nFor example:\nMap \u0026ldquo;names\u0026rdquo; field to random string when sending to unauthorized consumers. Apply custom transformation when reading from financial data stream. Store Policies Store policies define data persistency rules such as storage medium, retention period, replication factor, and others. For example, a retention rule may ask to keep all financial records for a minimum of two years. Storage medium rules may ask to move all data older than three months to cold storage. Replication rules may request a minimum replication factor of three distributed across availability zones.\nFluvio can apply policies on writing to or reading from the store.\nProfiles Store policies use the following profiles:\nConfiguration Profiles - A configuration profile defines parameters such as storage type, retention period, replication factor, and more. Write Policy Rules Write policy defines rules enforced before writing the data to store.\nFor example:\nData streams storing financial records must be preserved for 2 years. The system enforces policy over local retention settings. Data streams storing financial records cannot be compacted to prevent data loss. The system blocks all compaction attempts. Read Policy Rules Read policy defines rules enforced before sending the data to consumers.\nFor example:\nUnauthorized consumers cannot read data persisted in cold store. Unauthorized consumers can read up to the last 24 hours of data. Conclusion Fluvio policies are potent constructs that give operators control over all data exchanges in the organization. Global policies allow security teams to protect sensitive information while continuing to offer safe access to other data. An organization using Fluvio Policy can operate in real-time with a high level of confidence that sensitive data is always protected from unauthorized use.\n","description":"Fluvio Policy gives security operators and data owners centralized control over all data exchanges in the organization.","keywords":null,"summary":"Fluvio Policy gives security operators and data owners centralized control over all data exchanges in the organization. The policies follow the four stages - connect, protect, enrich, and store - of the Fluvio data streaming pipeline.\nPolicies can be assigned to one, all, or groups of data streams. When used in combination with role-based access control (RBAC), the policies are a potent tool to control data movement across teams, tools, and geo-locations in the organization.","title":"Fluvio Policy","url":"http://localhost:1315/blog/2021/03/fluvio-policy/"},{"body":"Many successful modern applications need to interact with their users in real-time, and this capability is quickly becoming the expected standard. However, building a real-time application from scratch is a daunting task, pulling focus away from the business problems InfinyOn Team is actually trying to solve. Fluvio is a real-time data streaming platform designed to make real-time application development easy.\nIn this blog post, we\u0026rsquo;re going to build a Robot Assistant, an add-on button on the website, that interacts with users in real-time.\nWe\u0026rsquo;ll build the frontend and backend, then use Fluvio as our data streaming layer. Fluvio data streaming gives us the ability to react in real-time, deploy to a massive audience, and preserve all data exchanges.\nThe project is also available for download in github.\nPrerequisites This project is using websocket-glue for the client/server communication. For additional information on websocket checkout our blog:\nWebsocket Glue for Data Streaming Apps Familiarity with the following software packages is useful but not required: Javascript, TypeScript, Node.js, and WebSocket.\nOverview This blog takes a step-by-step approach to building a robot assistant, called Bot Assistant, from the ground up. The following outline shows the steps involved:\nStep 1: Create the project Add project directory Add node.js server Add typescript configuration Add bot-assistant.ts server file Step 2: Implement backend server Add messages type definition file Add state machine Add workflow controller Add proxy service Add bot-server.ts file Step 3: Implement frontend client Add index.html file Add stylesheet file Add assistant images Add assistant.js script Load reconnecting-socket.js file Test Bot Assistant (v1) Step 4: Add data streaming and persistency Add fluvio to session-controller Add fluvio to workflow-controller Add fluvio to bot-server Add fluvio setup script Test Bot Assistant Step 1: Create the project Bot assistant has a client and a server. The client runs in the web browser and controls the frontend user interaction, while the backend runs on a server and manages the websocket proxy and the state machine. The client and the server communicate with ech other through websocket.\nLet\u0026rsquo;s get started:\nAdd project directory Add node.js server Add typescript configuration Add bot-assistant.ts server file Add project directory Create a project directory called bot-assistant with two folders public and src:\n%copy first-line%\n$ mkdir -p bot-assistant/{public,src} \u0026amp;\u0026amp; cd bot-assistant The public directory stores the client code, and the src directory contains the server code (the \u0026ldquo;app server\u0026rdquo;). Both the client and the app server are served from the same web server that we\u0026rsquo;ll set up next.\nAdd node.js server Create a Node.js project and implement the server. This project is using Node.js v13:\n%copy first-line%\n$ npm init -y which yields the following package.json file:\n%copy%\n{ \u0026#34;name\u0026#34;: \u0026#34;bot-assistant\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;fluvio \u0026lt;admin@fluvio.io\u0026gt; (fluvio.io)\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Apache 2.0\u0026#34; } Install express, typescript and a few other development services:\n%copy first-line%\n$ npm install typescript express ws @fluvio/client Add watcher and typescript definitions:\n%copy first-line%\n$ npm install -D tsc-watch @types/ws @types/node @types/express We installed the following packages:\nexpress: to serve the client and server files. ws: for client/server communication. @fluvio/client: node API library to communicate with fluvio. tsc-watch: to keep track of typescript file changes. Update package.json file as follows:\n%copy%\n{ \u0026#34;name\u0026#34;: \u0026#34;bot-assistant\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;bot-assistant.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start:server\u0026#34;: \u0026#34;tsc-watch --onSuccess \\\u0026#34;node ./dist/bot-assistant.js $PARAMS\\\u0026#34;\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;fluvio \u0026lt;admin@fluvio.io\u0026gt; (fluvio.io)\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Apache 2.0\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@fluvio/client\u0026#34;: \u0026#34;^0.6.0-beta.3\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^4.17.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.1.3\u0026#34;, \u0026#34;ws\u0026#34;: \u0026#34;^7.4.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.9\u0026#34;, \u0026#34;@types/node\u0026#34;: \u0026#34;^14.14.19\u0026#34;, \u0026#34;@types/ws\u0026#34;: \u0026#34;^7.4.0\u0026#34;, \u0026#34;tsc-watch\u0026#34;: \u0026#34;^4.2.9\u0026#34; } } Change main to reference bot-assistant.js and start:dev script to start the typescript watcher.\nAdd typescript configuration The project is implemented in typescript which requires a typescript configuration file.\nAdd the tsconfig.json typescript configuration file:\n%copy first-line%\n$ touch tsconfig.json Paste the following content in the tsconfig.json file:\n%copy%\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es6\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;lib\u0026#34;: [ \u0026#34;dom\u0026#34;, \u0026#34;ES2017\u0026#34;, \u0026#34;ES2015\u0026#34; ], \u0026#34;outDir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, }, \u0026#34;include\u0026#34;: [ \u0026#34;src/*\u0026#34;, ], } For additional information on the typescript configuration parameters, check out the documentation.\nAdd bot-assistant.ts server file The package.json file instructs by Node.js to run bot-assistant.js when it initializes. In typescript, this file is compiled from bot-assistant.ts. This is the place where we provision the web server, add routes for the frontend, and initialize backend services.\nCreate the bot-assistant.ts file in the src directory:\n%copy first-line%\ntouch src/bot-assistant.ts Paste the following content in the src/bot-assistant.ts file:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#39;path\u0026#39;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;) app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); app.use(\u0026#34;/scripts\u0026#34;, express.static(path.join(publicPath, \u0026#39;scripts\u0026#39;))); app.use(\u0026#34;/css\u0026#34;, express.static(path.join(publicPath, \u0026#39;css\u0026#39;))); app.use(\u0026#34;/img\u0026#34;, express.static(path.join(publicPath, \u0026#39;img\u0026#39;))); const Server = http.createServer(app); Server.listen(PORT, () =\u0026gt; { console.log( `started bot assistant server at http://localhost:${PORT}...` ); }); }; process.on(\u0026#34;uncaughtException\u0026#34;, (e) =\u0026gt; { console.log(e); process.exit(1); }); process.on(\u0026#34;unhandledRejection\u0026#34;, (e) =\u0026gt; { console.log(e); process.exit(1); }); startServer(); This code adds routes for the frontend client and starts a server on port 9998. The routes are as follows:\n/ (root) =\u0026gt; `public/index.html /scripts =\u0026gt; public/scripts /css =\u0026gt; public/css /img =\u0026gt; public/img Next, we\u0026rsquo;ll implement the backend server followed by the frontend client.\nStep 2: Implement backend server The backend server has two core services, Proxy Service and Workflow Service.\nThe Proxy service handles the connection between the client and the workflows. It accepts websocket connections, forwards client messages to the workflow service, and returns the replies to the originator.\nThe Workflow Service manages state transitions. It initializes the state machine from a json file, accepts client state messages, computes the next state and returns a reply.\nThe backend server implementation has several steps:\nAdd messages type definition file Add state machine Define state machine types Define state transitions Create a state machine JSON file Add state-machine.ts file Add workflow controller Add workflow-controller.ts file Add proxy service Add outgoing proxy Add incoming proxy Add session controller Add bot-server.ts file Update bot-assistant.ts file If you prefer to skip ahead, you can download the source code from github and resume at start backend server.\nAdd messages type definition file The messages file defines the types of the messages for the client/server communication. The messages are shared by both, proxy and workflow services, and it will be a top level file.\nLet\u0026rsquo;s add messages.ts file inside src directory:\n%copy first-line%\n$ touch src/messages.ts Paste the following message type definitions:\n%copy%\nexport type TimeStamp = string; export type SID = string; export interface Message { sid: SID; payload?: Payload; timestamp: TimeStamp; } export type Payload = | Request | Response; export interface Request { kind: \u0026#34;Request\u0026#34;, message: RequestMessage, } export interface Response { kind: \u0026#34;Response\u0026#34;, message: ResponseMessage, } export type RequestMessage = | BotText | ChoiceRequest | StartChatSession | EndChatSession; export type ResponseMessage = | ChoiceResponse | UserText export interface BotText { kind: \u0026#34;BotText\u0026#34;, content: string } export interface ChoiceRequest { kind: \u0026#34;ChoiceRequest\u0026#34;, question: string, groupId: string, choices: Array\u0026lt;Choice\u0026gt;, } export interface Choice { itemId: string, content: string } export interface StartChatSession { kind: \u0026#34;StartChatSession\u0026#34;, sessionId: string, chatPrompt?: string, chatText?: string, } export interface EndChatSession { kind: \u0026#34;EndChatSession\u0026#34;, sessionId: string, } export interface ChoiceResponse { kind: \u0026#34;ChoiceResponse\u0026#34;, groupId: string, itemId: string, content?: string, } export interface UserText { kind: \u0026#34;UserText\u0026#34;, sessionId: string, content?: string, } export function buildInitMessage(sid: SID) { return \u0026lt;Message\u0026gt;{ sid: sid, timestamp: getDateTime(), }; }; export function buildRequest(sid: SID, message: RequestMessage) { return \u0026lt;Message\u0026gt;{ sid: sid, payload: \u0026lt;Request\u0026gt;{ kind: \u0026#34;Request\u0026#34;, message: message }, timestamp: getDateTime(), }; }; export function buildResponse(sid: SID, message: ResponseMessage) { return \u0026lt;Message\u0026gt;{ sid: sid, payload: \u0026lt;Response\u0026gt;{ kind: \u0026#34;Response\u0026#34;, message: message }, timestamp: getDateTime(), }; }; export function isRequest(payload?: Payload) { return (payload) ? (payload.kind == \u0026#34;Request\u0026#34;) : false; } function getDateTime() { return new Date(Date.now() - new Date().getTimezoneOffset() * 60000) .toISOString() .slice(0, -1); } The message definitions are as follows:\nMessage: is the top level type definition. Payload: defines payload types: request or response. Request: defines request messages (BotText, ChoiceRequest, StartChatSession, EndChatSession). Response: defines response messages (ChoiceResponse, UserText). BotText: is a text message sent by the Bot (text parsed as HTML). ChoiceRequest: is an array of choices sent by the Bot. StartChatSession: is a request by the Bot to enable chat editor. EndChatSession: is a request sent by the Bot to disable chat editor. ChoiceResponse: is the response to a ChoiceRequest. UserText: is text sent by the User. The definitions are followed by a series of helper APIs:\nbuildInitMessage: creates a message without a payload that indicates a new connection. buildRequest: creates a Request message. buildResponse: creates a Response message. isRequest: checks if the message is of Request kind. getDateTime: generates a timestamp. The type definitions are used extensively by the state machine defined in the following section.\nAdd state machine This project uses a state machine to implement the behavior of the robot assistant. We may think of a state machine as a guided tour where all traffic follows a well-defined path. The state machine defines the choices and the order in which they are to be sent to the client. Upon receipt, the client generates a response and returns an answer. The state machine uses the answer to identify the location to resume and generates the next choice. This request/response exchange continues until the end state is reached.\nDefine state machine types The state machine is a chain of states expressed in a JSON format. Each state can have one of two types: sendRequest, or matchResponse. The sendRequest state instructs the workflow controller to generate a message and wait for the response. When the response arrives, the controller looks up the matchResponse state to identify where it should resume. Each request/response pair has a unique identifier. The identifier is a unique id that defines the context of a client/server message exchange. The final state is defined by a state without a next field.\nThe workflow controller generates one of the following sendRequest messages:\nBotText - sends the client an information field in text or HTML format. ChoiceRequest - sends a list of choices to the user. GroupId is the unique identifier paired with a ChoiceResponse. StartChatSession - asks the client to enable chat editor. SessionId is the unique identifier paired with a UserText EndChatSession - ask the client to disable chat session. Uses the SessionId paired with a StartChatSession The Client replies with one of the following mathResponse messages:\nChoiceResponse - send one of the choices in the ChoiceRequest. UserText - sends text generated by the user. Define state transitions The state transition have two flows:\ninternal flows - driven by one or more internal states. external flows - driven by an external state. An external state tells the engine to generate a request and wait for the response before resuming. Internal states have a next field whereas external states have a sessionId or groupId but no next field. Internal states are chained internally, whereas external are chained externally through a client response.\nState transitions are triggered by a new connection or a client response. If it begins at an internal state, the engine collects the state information and moves to the next state until it encounters an external state. At that time, it generates a client request and waits for the response before it can resume.\nThe client displays the request choices and asks the user to make a selection. Upon selection, the client generates a response and the cycle repeats.\nNow that we have defined the state machine and the state transition, let\u0026rsquo;s start the implementation.\nCreate a state machine JSON file We\u0026rsquo;ll create a state machine asks the user for their favorite programming language and collect their response.\nLet\u0026rsquo;s create state-machine directory and add bot-assistant.json file:\n%copy first-line%\n$ mkdir state-machines \u0026amp;\u0026amp; touch state-machines/bot-assistant.json Copy following state machine definition in the JSON file:\n%copy%\n{ \u0026#34;greetings\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;BotText\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi, I\u0026#39;m Bot! Nice to meet you.\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;langChoices\u0026#34; }, \u0026#34;langChoices\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceRequest\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;lang\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What programming language do you use in your hobby projects?\u0026#34;, \u0026#34;choices\u0026#34;: [ { \u0026#34;itemId\u0026#34;: \u0026#34;rust\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Rust\u0026#34; }, { \u0026#34;itemId\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Go\u0026#34; }, { \u0026#34;itemId\u0026#34;: \u0026#34;other\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Other\u0026#34; } ] } }, \u0026#34;langChoiceRust\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceResponse\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;lang\u0026#34;, \u0026#34;itemId\u0026#34;: \u0026#34;rust\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;anyOtherChoices\u0026#34; }, \u0026#34;langChoiceGo\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceResponse\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;lang\u0026#34;, \u0026#34;itemId\u0026#34;: \u0026#34;go\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;anyOtherChoices\u0026#34; }, \u0026#34;langChoiceOther\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceResponse\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;lang\u0026#34;, \u0026#34;itemId\u0026#34;: \u0026#34;other\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;startLangPrefSession\u0026#34; }, \u0026#34;startLangPrefSession\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;StartChatSession\u0026#34;, \u0026#34;sessionId\u0026#34;: \u0026#34;langPreference\u0026#34;, \u0026#34;chatPrompt\u0026#34;: \u0026#34;Type your preferred language here...\u0026#34;, \u0026#34;chatText\u0026#34;: \u0026#34;I enabled chat editor\u0026#34; } }, \u0026#34;getLangPrefResponse\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;UserText\u0026#34;, \u0026#34;sessionId\u0026#34;: \u0026#34;langPreference\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;endLangPrefSession\u0026#34; }, \u0026#34;endLangPrefSession\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;EndChatSession\u0026#34;, \u0026#34;sessionId\u0026#34;: \u0026#34;langPreference\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;anyOtherChoices\u0026#34; }, \u0026#34;anyOtherChoices\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceRequest\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;others\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;Any other?\u0026#34;, \u0026#34;choices\u0026#34;: [ { \u0026#34;itemId\u0026#34;: \u0026#34;yes\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Yes\u0026#34; }, { \u0026#34;itemId\u0026#34;: \u0026#34;no\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;No\u0026#34; } ] } }, \u0026#34;anyOtherYes\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceResponse\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;others\u0026#34;, \u0026#34;itemId\u0026#34;: \u0026#34;yes\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;langChoices\u0026#34; }, \u0026#34;anyOtherNo\u0026#34;: { \u0026#34;matchResponse\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;ChoiceResponse\u0026#34;, \u0026#34;groupId\u0026#34;: \u0026#34;others\u0026#34;, \u0026#34;itemId\u0026#34;: \u0026#34;no\u0026#34; }, \u0026#34;next\u0026#34;: \u0026#34;done\u0026#34; }, \u0026#34;done\u0026#34;: { \u0026#34;sendRequest\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;BotText\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Great, thanks!\u0026#34; } } } The state machine asks users for their favorite programming language and it presents them 3 options: Rust, Go, and Other.\nIf a user chooses Rust or Go, the state machine return:\nanyOtherChoices - another choice with yes or no answers. For Other, it runs through the following states:\nstartLangPrefSession opens an interactive session, getLangPrefResponse captures the user response, endLangPrefSession ends the interaction session. This basic state machine show two different interaction models: a choice request/response or a user interaction. When the client receives a choice request, it presents the user with a series of choices. The user clicks on one of the choices and the client generates a response. For an interactive session, the client is asked to open an interactive session for the user to type his answer. After the server receives the response, it sends the client another request to close the interactive session. It is the responsibility of the server to manage access to the user editor.\nNext, we need to load the JSON file into memory.\nAdd state-machine.ts file The state machine is part of the workflow service that we\u0026rsquo;ll define in the next section.\nCreate a workflow-service directory and add the state-machine.ts file:\n%copy first-line%\n$ mkdir src/workflow-service \u0026amp;\u0026amp; touch src/workflow-service/state-machine.ts Paste the following code in the state-machine.ts file:\n%copy%\nimport Fs from \u0026#34;fs\u0026#34;; import { RequestMessage, ResponseMessage } from \u0026#34;../messages\u0026#34;; type Name = string; /* State Machine definition */ export type StateMachine = Map\u0026lt;Name, State\u0026gt;; export interface State { sendRequest?: RequestMessage, matchResponse?: ResponseMessage; next?: string, } /* Load state machine from JSON file */ export function loadStateMachine(filePath: string) { const jsonFile = Fs.readFileSync(filePath); const jsonObject = JSON.parse(jsonFile.toString()); const state_machine: StateMachine = new Map(); for (var value in jsonObject) { state_machine.set(value, jsonObject[value]) } return state_machine; } The code reads the JSON file, and provisions an internal state machine variable.\nAdd workflow controller The workflow controller is the mediator between the websocket proxy and the state machine. The controller receives messages from the client, computes the next state, generates a reply, and sends a response.\nAdd workflow-controller.ts file Add the workflow-controller.ts file to the workflow-service directory:\n%copy first-line%\n$ touch src/workflow-service/workflow-controller.ts Paste the following code:\n%copy%\nimport { SID, Message, ResponseMessage, ChoiceResponse, UserText, buildRequest, isRequest } from \u0026#34;../messages\u0026#34;; import { StateMachine, State } from \u0026#34;./state-machine\u0026#34;; import { SessionController } from \u0026#34;../proxy-service/session-controller\u0026#34;; export class WorkflowController { private stateMachine: StateMachine; private initState: string; private sessionController: SessionController; constructor( stateMachine: StateMachine, ) { this.stateMachine = stateMachine; this.initState = stateMachine.keys().next().value; this.sessionController = Object(); } public init(sessionController: SessionController) { this.sessionController = sessionController; } private processNewConnection(sid: SID) { const nextStates = this.processNext(this.initState); this.sendMessages(sid, nextStates); } private processNextState(sid: SID, response: ResponseMessage) { const state: string = this.getState(response); const nextStates = this.processNext(state); this.sendMessages(sid, nextStates); } private getState(response: ResponseMessage) { switch (response.kind) { case \u0026#34;ChoiceResponse\u0026#34;: { return this.getChoiceResponseState(response); } case \u0026#34;UserText\u0026#34;: { return this.getUserTextState(response); } } } private processNext(startState: string) { var nextStates: State[] = []; var state = this.stateMachine.get(startState); while (state) { nextStates.push(state); const next = state.next || \u0026#34;\u0026#34;; state = this.stateMachine.get(next); if (next.length \u0026gt; 0 \u0026amp;\u0026amp; !state) { console.error(`Error: Cannot find next state: ${next}`); } } return nextStates; } private getChoiceResponseState(choiceResponse: ChoiceResponse) { for (let [key, state] of this.stateMachine.entries()) { if (state.matchResponse \u0026amp;\u0026amp; state.matchResponse.kind == choiceResponse.kind \u0026amp;\u0026amp; state.matchResponse.groupId == choiceResponse.groupId \u0026amp;\u0026amp; state.matchResponse.itemId == choiceResponse.itemId) { return key; } } console.error(`Error: cannot find choice ${JSON.stringify(choiceResponse)}`); return this.initState; } private getUserTextState(userText: UserText) { for (let [key, state] of this.stateMachine.entries()) { if (state.matchResponse \u0026amp;\u0026amp; state.matchResponse.kind == \u0026#34;UserText\u0026#34; \u0026amp;\u0026amp; state.matchResponse.sessionId == userText.sessionId) { return key; } } console.error(`Error: cannot find user session ${JSON.stringify(userText)}`); return this.initState; } private sendMessages(sid: SID, nextStates: State[]) { for (let idx = 0; idx \u0026lt; nextStates.length; idx++) { const state = nextStates[idx]; if (state.sendRequest) { const message = buildRequest(sid, state.sendRequest); this.sessionController.processBotMessage(JSON.stringify(message)); } } } public processProxyMessage(clientMessage: string) { const message: Message = JSON.parse(clientMessage); if (!isRequest(message.payload)) { const sid = message.sid; if (message.payload) { this.processNextState( sid, \u0026lt;ResponseMessage\u0026gt;message.payload.message ); } else { this.processNewConnection(sid); } } } } The workflow controller performs the following functions:\nconstructor: caches a reference to the stateMachine and computes the initial state. init: caches a reference to the sessionController. This is done out of the constructor due to the circular reference. We\u0026rsquo;ll come back to this when in the Fluvio data streaming section. Note: The code does not compile until we add the session controller in the following section. processProxyMessage: is invoked by session controller to process a new client message. If the message has payload, it asks for next request, otherwise is needs the initial request: processNewConnection reads the state machine from the first state and produces a request. processNextState parses the client response, looks-up the resume state, and produces the next request. The other APIs help the controller match a response and traverse the state machine to generate subsequent requests.\nAdd proxy service The proxy service has three components, incoming proxy ProxyIn, outgoing proxy ProxyOut and the session controller. The incoming proxy handles the websocket protocol, outgoing proxy sends messages based on a session id, and the session controller the interaction between the proxy and other services.\nFor additional details, checkout Websocket Glue for Data Streaming Apps.\nAdd outgoing proxy Create a directory for the proxy-service and add proxy-out.ts file:\n%copy first-line%\nmkdir src/proxy-service \u0026amp;\u0026amp; touch src/proxy-service/proxy-out.ts Paste the following code:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; type SID = string; export class WsProxyOut { private sessions: Map\u0026lt;SID, WS\u0026gt;; constructor() { this.sessions = new Map(); } public addSession(sid: SID, ws: WS) { this.sessions.set(sid, ws); } public closeSession(sid: SID) { const ws = this.sessions.get(sid); if (ws) { ws.close(); } this.sessions.delete(sid); } public sendMessage(sid: SID, message: string) { const ws = this.sessions.get(sid); if (ws) { ws.send(message); } } } As descried in the websocket-glue blog, ProxyOut keeps a mapping between session session id and the websocket session.\nAdd incoming proxy Add proxy-in.ts file to manage the websocket protocol:\n%copy first-line%\n$ touch src/proxy-service/proxy-in.ts Paste the following code:\n%copy%\nimport crypto from \u0026#39;crypto\u0026#39;; import WS from \u0026#34;ws\u0026#34;; import http from \u0026#34;http\u0026#34;; import { SessionController } from \u0026#34;./session-controller\u0026#34;; const COOKIE_NAME = \u0026#34;Fluvio-Bot-Assistant\u0026#34; export class WsProxyIn { private static wss: WS.Server; private static sessionController: SessionController; constructor(sessionController: SessionController) { WsProxyIn.wss = new WS.Server({ clientTracking: false, noServer: true }); WsProxyIn.sessionController = sessionController; } public init(server: http.Server) { this.onUpgrade(server); this.onConnection(); } private onUpgrade(server: http.Server) { server.on(\u0026#34;upgrade\u0026#34;, (request, socket, head) =\u0026gt; { const session = WsProxyIn.parseCookie(COOKIE_NAME, request.headers.cookie); if (session) { request.headers.session = session; } WsProxyIn.wss.handleUpgrade(request, socket, head, function (ws: WS) { WsProxyIn.wss.emit(\u0026#34;connection\u0026#34;, ws, request); }); }); } private onConnection() { WsProxyIn.wss.on(\u0026#34;headers\u0026#34;, (headers: Array\u0026lt;string\u0026gt;, req) =\u0026gt; { const session = WsProxyIn.parseCookie(COOKIE_NAME, req.headers.cookie); if (!session) { let session = crypto.randomBytes(20).toString(\u0026#34;hex\u0026#34;); req.headers.session = session; headers.push(\u0026#34;Set-Cookie: \u0026#34; + COOKIE_NAME + \u0026#34;=\u0026#34; + session); } }); WsProxyIn.wss.on(\u0026#34;connection\u0026#34;, async (ws, req) =\u0026gt; { const session_hdr = req.headers.session; const sid = ((Array.isArray(session_hdr)) ? session_hdr[0] : session_hdr) || \u0026#34;\u0026#34;; await WsProxyIn.sessionController.sessionOpened(sid, ws); ws.on(\u0026#34;close\u0026#34;, async () =\u0026gt; { await WsProxyIn.sessionController.sessionClosed(sid); }); ws.on(\u0026#34;message\u0026#34;, async (clientMsg: string) =\u0026gt; { await WsProxyIn.sessionController.messageFromClient(sid, clientMsg); }); }); } private static parseCookie(cookieName: string, cookie_hdr?: string) { if (cookie_hdr) { const cookiePair = cookie_hdr.split(/; */).map((c: string) =\u0026gt; { const [key, v] = c.split(\u0026#39;=\u0026#39;, 2); return [key, decodeURIComponent(v)]; }).find(res =\u0026gt; (res[0] == cookieName) ); if (Array.isArray(cookiePair) \u0026amp;\u0026amp; cookiePair.length \u0026gt; 1) { return cookiePair[1]; } } return undefined; } } As descried in the websocket-glue blog, the code accepts websocket connections, provisions cookies (Fluvio-Bot-Assistant), and passes the messages to the session controller.\nAdd session controller Add session-controller.ts file:\n%copy first-line%\n$ touch src/proxy-service/session-controller.ts Paste the following code:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-out\u0026#34;; import { Message, SID, buildInitMessage, buildResponse, isRequest } from \u0026#34;../messages\u0026#34;; import { WorkflowController } from \u0026#34;../workflow-service/workflow-controller\u0026#34;; type Messages = Array\u0026lt;Message\u0026gt;; export class SessionController { private sessionMessages: Map\u0026lt;SID, Messages\u0026gt;; private proxyOut: WsProxyOut; private workflowController: WorkflowController; constructor( proxyOut: WsProxyOut, ) { this.sessionMessages = new Map(); this.proxyOut = proxyOut; this.workflowController = Object(); } public init(workflowController: WorkflowController) { this.workflowController = workflowController; this.show(); } public sessionOpened(sid: SID, ws: WS) { console.log(`start session - ${sid}`); this.proxyOut.addSession(sid, ws); const messages = this.sessionMessages.get(sid); if (messages) { this.sendMessagesToClient(messages); } else { const message = buildInitMessage(sid); this.workflowController.processProxyMessage(JSON.stringify(message)); } } public sessionClosed(sid: SID) { console.log(`end session - ${sid}`); this.proxyOut.closeSession(sid); } public messageFromClient(sid: SID, clientMsg: string) { console.log(`${sid} \u0026lt;== ${clientMsg}`); const clientResponse = buildResponse(sid, JSON.parse(clientMsg)); this.addMessageToSession(clientResponse); this.workflowController.processProxyMessage(JSON.stringify(clientResponse)); } public sendMessagesToClient(messages: Messages) { messages.forEach(message =\u0026gt; { this.sendMessageToClient(message); }); } public sendMessageToClient(message: Message) { if (message.payload) { const clientMessage = message.payload.message; this.proxyOut.sendMessage(message.sid, JSON.stringify(clientMessage)); } } private addMessageToSession(message: Message) { const sid = message.sid; var messages = this.sessionMessages.get(sid); if (!messages) { messages = new Array(); } messages.push(message); this.sessionMessages.set(sid, messages); } public processBotMessage(botMessage: string) { const message: Message = JSON.parse(botMessage); this.addMessageToSession(message); if (isRequest(message.payload)) { this.sendMessageToClient(message); } } private show() { let table = new Map(); for (let [sid, value] of this.sessionMessages) { table.set(sid, value.length); } console.table(table, [\u0026#34;SID\u0026#34;, \u0026#34;Messages\u0026#34;]); } } The session controller keeps a local copy of the messages exchanges anchored by session id. When a known session re-initiates a connection, the controller plays back the messages from memory. All other requests are passed along to the workflow controller.\nWe are now ready to add the bot-server file and initialize all server components.\nAdd bot-server.ts file Add bot-server.ts file in the src directory:\n%copy first-line%\ntouch src/bot-server.ts Paste the following code:\n%copy%\nimport { Server } from \u0026#34;http\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-service/proxy-in\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-service/proxy-out\u0026#34;; import { StateMachine, loadStateMachine } from \u0026#34;./workflow-service/state-machine\u0026#34;; import { WorkflowController } from \u0026#34;./workflow-service/workflow-controller\u0026#34;; import { SessionController } from \u0026#34;./proxy-service/session-controller\u0026#34;; export const initBotAssistant = (server: Server) =\u0026gt; { const wsProxyOut = new WsProxyOut(); const sessionController = new SessionController(wsProxyOut); const wsProxyIn = new WsProxyIn(sessionController); let filePath = getFileName(); const stateMachine: StateMachine = loadStateMachine(filePath); const workflowController = new WorkflowController(stateMachine); sessionController.init(workflowController); workflowController.init(sessionController); wsProxyIn.init(server); }; const getFileName = () =\u0026gt; { if (process.argv.length != 3) { console.log(\u0026#34;Usage: node bot-assistant.js \u0026lt;state-machine.json\u0026gt;\u0026#34;); process.exit(1); } return process.argv[2]; } The bot server file initializes all server components: incoming proxy, outgoing proxy, session controller, state machine and workflow controller.\nTo address circular reference challenges, it initializes sessionController and workflowController separately from the constructors. We\u0026rsquo;ll come back to this in the Fluvio data streaming section.\nUpdate bot-assistant.ts file The last step of the implementation integrates initBotAssistant into the bot-assistant.ts file.\nUpdate the bot-assistant.ts file as follows:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#39;path\u0026#39;; import { initBotAssistant } from \u0026#34;./bot-server\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;) app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); app.use(\u0026#34;/scripts\u0026#34;, express.static(path.join(publicPath, \u0026#39;scripts\u0026#39;))); app.use(\u0026#34;/css\u0026#34;, express.static(path.join(publicPath, \u0026#39;css\u0026#39;))); app.use(\u0026#34;/img\u0026#34;, express.static(path.join(publicPath, \u0026#39;img\u0026#39;))); const Server = http.createServer(app); await initBotAssistant(Server); Server.listen(PORT, () =\u0026gt; { console.log( `started bot assistant server at http://localhost:${PORT}...` ); }); }; process.on(\u0026#34;uncaughtException\u0026#34;, (e) =\u0026gt; { console.log(e); process.exit(1); }); process.on(\u0026#34;unhandledRejection\u0026#34;, (e) =\u0026gt; { console.log(e); process.exit(1); }); startServer(); The code initializes the bot-server which needs access to HTTP server. Hence initBotAssistant is called after the Server is provisioned, and the server is passed through the function parameter.\nStart backend server Let\u0026rsquo;s start the server using the bot-assistant.json state machine file. Npm reads the command line parameters through environment variables:\n%copy first-line%\n$ PARAMS=state-machines/bot-assistant.json npm run start:server 4:35:59 PM - Starting compilation in watch mode... 4:36:01 PM - Found 0 errors. Watching for file changes. ┌───────────────────┬─────┬────────┐ │ (iteration index) │ Key │ Values │ ├───────────────────┼─────┼────────┤ └───────────────────┴─────┴────────┘ started bot assistant server at http://localhost:9998... Step 3: Implement frontend client The frontend client has two HTML components:\nBot button, Bot Assistant dialog box The Bot button is displayed on the lower right-hand side of the screen that opens the Bot Assistant dialog box. The dialog box is closed, the Bot button is shown again. In essence, the two components toggle each other on and off.\nThe client builds the HTML components dynamically through javascript and it communicates with the web server through websocket.\nThe client is implemented in several steps:\nAdd index.html file Add stylesheet file Add assistant images Add assistant.js script Load reconnecting-socket.js file Test Bot Assistant (v1) Add index.html file The front end client content is placed in the public directory. Let\u0026rsquo;s add index.html file:\n%copy first-line%\n$ touch public/index.html Paste the following code in index.html file:\n%copy%\n\u0026lt;!DOCTYPE HTML\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;css/assistant.css\u0026#34;/\u0026gt; \u0026lt;script type = \u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts/assistant.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;assistant\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- debugging area - begin --\u0026gt; \u0026lt;textarea id=\u0026#34;debugOutput\u0026#34; rows=\u0026#34;20\u0026#34; cols=\u0026#34;60\u0026#34; readonly\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;!-- debugging area - end --\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; In the header we are referencing two files:\ncss/assistant.css - styles file scripts/assistant.js - script file that builds the DOM elements. In the body, there is a div with class named assistant. The script file looks-up this div to attach DOM elements. For troubleshooting, there is a textarea that prints debugging information.\nAdd stylesheet file The stylesheet controls the look and feel of the Bot Assistant button dialog box.\nAdd a stylesheet called assistant.css to public/css directory:\n%copy first-line%\n$ mkdir public/css \u0026amp;\u0026amp; touch public/css/assistant.css Paste the following code in assistant.css file:\n%copy%\n.assistant { font-family: \u0026#39;Lucida Sans\u0026#39;, Geneva, Verdana, sans-serif; position:fixed; bottom:20px; right:25px; } /* Assistant - Button */ .assistant button { width: 45px; height: 45px; background:#008CBA; border-radius:5px; cursor:pointer; border: none; outline: none; } .assistant button img { padding-top:5px; width: 25px; height: 25px; } .assistant button:focus { border: none; outline: none; } /* Assistant - Chat Box */ .assistant .chat{ display: none; width:360px; background:white; border-radius:5px 5px 0px 0px; border: 1px solid gray; } .assistant .header{ background: #008CBA; color:white; padding:8px; font-weight:bold; border-radius:5px 5px 0px 0px; line-height: 32px; } .assistant .header span{ padding-left:0; font-size: 11pt; } .assistant .header img { width:18px; height:35px; margin-right: 10px; float:right; } .assistant .header img.bot { width:35px; height:35px; border-radius:50%; background:#bbb; float:left; } .assistant .header .overlay { background-color: #f6f6f6; padding: 1px; position: absolute; top: 32px; left: 33px; border-radius: 12px; z-index: 1; } .assistant .header .status{ width: 12px; height: 12px; border-radius: 12px; } .assistant .header .status.off{ background-color: #FF3B28; } .assistant .header .status.on{ background-color: greenyellow; } .assistant .header .close{ float:right; cursor:pointer; width: 28px; margin-right: 0; } .assistant .inner-body{ min-height: 250px; max-height: calc(100vh - 300px); overflow: auto; overflow-x: hidden; } .assistant .msg-body { font-size:12px; padding: 10px 10px 5px 5px; } .assistant .msg-left{ margin-bottom:7px; word-break: break-all; } .assistant .msg-left .avatar { width: 50px; margin-top: -40px; } .assistant .msg-left .operator { margin-top: -40px; padding: 1px; font-size:1.6em; width:35px; height:35px; line-height:1.8em; text-align:center; border-radius:50%; background:plum; color:white; } .assistant .msg-left img { width:35px; height:35px; border-radius:50%; background:#bbb; border: 1px solid #eee; } .assistant .msg-left .msg { background:#f2f2f2; padding:10px; min-height:15px; margin: 3px; border: 1px solid #ddd; border-radius:7px; margin-left: 44px; margin-right: 30px; } .assistant .msg-left .button { margin: -2px 30px 7px 50px; } .assistant .msg-right { position: relative; right: 0px; margin: 3px; margin-bottom:10px; } .assistant .msg-right .msg { background:#d4e7fa; padding:10px; min-height:15px; margin-left: 80px; border-radius:7px; word-break: break-all; } .assistant .msg-right .button { float: right; } /* button */ .assistant .btn { display: inline-block; margin: 2px; width: 100%; } .assistant .button { width: max-content; border-radius:15px; padding: 10px 15px; transition-duration: 0.2s; background-color: white; color: #006687; border: 1px solid #008CBA; } .assistant .button.selected { background-color: #008CBA; color: white; } .assistant .button:hover { cursor: pointer; background-color: #008CBA; color: white; } /* footer */ .assistant .footer { background:white; bottom: 0; padding-bottom: 10px; width: 100%; } .assistant .footer .textareaElement { padding: 15px 10px 0 10px; border-top: 1px solid #ccc; min-height: 20px; overflow-x: hidden; overflow-y: auto; font-size: 11pt; font-family: Arial, Helvetica, sans-serif; color: #333; } .assistant .footer .textareaElement:focus { outline: none; } .assistant .footer [placeholder]:empty::before { content: attr(placeholder); color: #aaa; } .assistant .footer [placeholder]:empty:focus::before { content: \u0026#34;\u0026#34;; } In summary the stylesheet has three sections, Button and Chat Box.\nThe Chat Box has three subsections: a header with the bot icon, title and a close icon, the body area, and the footer. The footer has an editor for user input that is set to read-only.\nAdd assistant images The assistant button and chat dialog box uses several images to enhance the visualization.\nLet\u0026rsquo;s create an img directory and use curl to download the images from github:\n%copy%\nmkdir -p public/img/assistant curl -L https://raw.githubusercontent.com/infinyon/fluvio-demo-apps-node/master/bot-assistant/public/img/assistant/note.svg --output public/img/assistant/note.svg curl -L https://raw.githubusercontent.com/infinyon/fluvio-demo-apps-node/master/bot-assistant/public/img/assistant/bot.svg --output public/img/assistant/bot.svg curl -L https://raw.githubusercontent.com/infinyon/fluvio-demo-apps-node/master/bot-assistant/public/img/assistant/redo.svg --output public/img/assistant/redo.svg curl -L https://raw.githubusercontent.com/infinyon/fluvio-demo-apps-node/master/bot-assistant/public/img/assistant/close.svg --output public/img/assistant/close.svg The script download 4 svg images: note, bot, redo and close.\nAdd assistant.js script The most important component of the frontend client is the assistant.js script. The script creates DOM elements, handles the user interaction, and communicates with the server.\nAdd the assistant.js file to the public/scripts directory:\n%copy first-line%\n$ mkdir public/scripts \u0026amp;\u0026amp; touch public/scripts/assistant.js Paste the following code in assistant.js file:\n%copy%\nwindow.onload = () =\u0026gt; { var webSocket = null; var sessionId = \u0026#34;\u0026#34;; // Load reconnecting socket to DOM loadScript(\u0026#34;scripts/reconnecting-socket.js\u0026#34;); // Create and attach Bot Assistant HTML elements function loadAssistant() { // Add assistant button var note = createElement(\u0026#34;img\u0026#34;, { \u0026#34;src\u0026#34;: `img/assistant/note.svg` }), aButton = createElement(\u0026#34;button\u0026#34;, {}, note); // Append assistant dialog var status = createElement(\u0026#34;div\u0026#34;, { \u0026#34;id\u0026#34;: \u0026#34;bot-status\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;status off\u0026#34; }), overlay = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;overlay\u0026#34; }, status), bot = createElement(\u0026#34;img\u0026#34;, { \u0026#34;src\u0026#34;: `img/assistant/bot.svg`, \u0026#34;class\u0026#34;: \u0026#34;bot\u0026#34; }), title = createElement(\u0026#34;span\u0026#34;, {}, \u0026#34;Bot Assistant\u0026#34;), aDialogClose = createElement(\u0026#34;img\u0026#34;, { \u0026#34;src\u0026#34;: `img/assistant/close.svg`, \u0026#34;class\u0026#34;: \u0026#34;close\u0026#34; }), aDialogReset = createElement(\u0026#34;img\u0026#34;, { \u0026#34;src\u0026#34;: `img/assistant/redo.svg` }), header = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;header\u0026#34; }, [bot, overlay, title, aDialogClose, aDialogReset]), msgBody = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg-body\u0026#34; }), innerBody = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;inner-body\u0026#34; }, msgBody), body = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;body-wrapper\u0026#34; }, innerBody), userMsg = createElement(\u0026#34;div\u0026#34;, { \u0026#34;id\u0026#34;: \u0026#34;user-msg\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;textareaElement\u0026#34;, \u0026#34;placeholder\u0026#34;: \u0026#34;Choose an option\u0026#34;, \u0026#34;contenteditable\u0026#34;: \u0026#34;false\u0026#34; }), footer = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;footer\u0026#34; }, userMsg), aDialog = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;chat\u0026#34; }, [header, body, footer]); // Attach event listeners aButton.addEventListener(\u0026#39;click\u0026#39;, onOpenDialog, false); aDialogClose.addEventListener(\u0026#39;click\u0026#39;, onCloseDialog, false); aDialogReset.addEventListener(\u0026#39;click\u0026#39;, onResetSession, false); // Add to document document.querySelector(\u0026#34;.assistant\u0026#34;).appendChild(aButton); document.querySelector(\u0026#34;.assistant\u0026#34;).appendChild(aDialog); } // On open assistant dialog callback function onOpenDialog() { document.querySelector(\u0026#34;.assistant button\u0026#34;).style.display = \u0026#34;none\u0026#34;; document.querySelector(\u0026#34;.assistant .chat\u0026#34;).style.display = \u0026#34;block\u0026#34;; openWSConnection(); } // On close assistant dialog callback function onCloseDialog() { document.querySelector(\u0026#34;.assistant .chat\u0026#34;).style.display = \u0026#34;none\u0026#34;; document.querySelector(\u0026#34;.assistant button\u0026#34;).style.display = \u0026#34;block\u0026#34;; } // Clear the cookie and restart connection to create a new session. function onResetSession() { document.cookie = \u0026#34;Fluvio-Bot-Assistant=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/\u0026#34;; closeWsConnection(); clearMessages(); openWSConnection(); } // Open WebSocket connection function openWSConnection() { try { if (webSocket != null) { return; // already connected } logOutput(\u0026#34;Connecting to: ws://localhost:9998/\u0026#34;); webSocket = new ReconnectingWebSocket(\u0026#34;ws://localhost:9998/\u0026#34;); webSocket.onopen = function (openEvent) { clearMessages(); document.getElementById(\u0026#34;bot-status\u0026#34;).setAttribute(\u0026#34;class\u0026#34;, \u0026#34;status on\u0026#34;); logOutput(\u0026#34;Connected!\u0026#34;); }; webSocket.onclose = function (closeEvent) { document.getElementById(\u0026#34;bot-status\u0026#34;).setAttribute(\u0026#34;class\u0026#34;, \u0026#34;status off\u0026#34;); logOutput(\u0026#34;Disconnected!\u0026#34;); }; webSocket.onerror = function (errorEvent) { logOutput(`Error: ${JSON.stringify(errorEvent)}`); }; webSocket.onmessage = function (messageEvent) { var serverMsg = messageEvent.data; logOutput(`\u0026lt;== ${serverMsg}`); onMessageFromServer(serverMsg); }; } catch (exception) { logOutput(`error: ${JSON.stringify(exception)}`); } } // Close WS Connection function closeWsConnection() { if (webSocket.open) { webSocket.close(); webSocket = null; } } // On messages received from Websocket function onMessageFromServer(value) { const message = JSON.parse(value); switch (message.kind) { case \u0026#34;BotText\u0026#34;: showBotText(message.content); break; case \u0026#34;UserText\u0026#34;: showUserText(message.content); break; case \u0026#34;ChoiceRequest\u0026#34;: showBotText(message.question); showChoiceButtons(message.groupId, message.choices); break; case \u0026#34;ChoiceResponse\u0026#34;: choicesToButton(message.groupId, message.content); break; case \u0026#34;StartChatSession\u0026#34;: sessionId = message.sessionId; enableChatEditor(message.chatPrompt, message.chatText); break; case \u0026#34;EndChatSession\u0026#34;: disableChatEditor(); break; }; } // Send a message on WebSocket function sendWsMessage(message) { if (webSocket.readyState != WebSocket.OPEN) { logOutput(\u0026#34;WebSocket is not connected: \u0026#34; + webSocket.readyState); return; } const msgObj = JSON.stringify(message) logOutput(`==\u0026gt; ${msgObj}`); webSocket.send(msgObj); } // Show text from bot assistant function showBotText(content) { if (content.length \u0026gt; 0) { removeDuplicateAvatar(\u0026#34;bot\u0026#34;); var img = createElement(\u0026#34;img\u0026#34;, { \u0026#34;src\u0026#34;: `img/assistant/bot.svg` }), avatar = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;avatar\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;bot\u0026#34; }, img), msg = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg\u0026#34; }, content), msgLeft = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg-left\u0026#34; }, [msg, avatar]); document.querySelector(\u0026#34;.msg-body\u0026#34;).appendChild(msgLeft); scrollToBottom(\u0026#34;.inner-body\u0026#34;); } } // Show text from user interactive session function showUserText(content) { if (content.length \u0026gt; 0) { var msg = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg\u0026#34; }, content), msgLeft = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg-right\u0026#34; }, msg); document.querySelector(\u0026#34;.msg-body\u0026#34;).appendChild(msgLeft); scrollToBottom(\u0026#34;.inner-body\u0026#34;); } } // Show choices function showChoiceButtons(groupId, choices) { if (choices.length \u0026gt; 0) { var buttons = []; choices.forEach(choice =\u0026gt; { var button = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;button\u0026#34; }, choice.content); button.addEventListener(\u0026#39;click\u0026#39;, function () { pickChoice(groupId, choice.itemId, choice.content); }, false); buttons.push(createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;btn\u0026#34; }, button)); }); var msgLeft = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg-left\u0026#34;, \u0026#34;id\u0026#34;: groupId }, buttons); document.querySelector(\u0026#34;.msg-body\u0026#34;).appendChild(msgLeft); scrollToBottom(\u0026#34;.inner-body\u0026#34;); } } // Callback invoked on user selection function pickChoice(groupId, itemId, content) { choicesToButton(groupId, content); sendWsMessage({ kind: \u0026#34;ChoiceResponse\u0026#34;, groupId: groupId, itemId: itemId, content: content, }); } // Swap choices with a button representing the selection function choicesToButton(groupId, content) { document.getElementById(groupId).remove(); var button = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;button selected\u0026#34; }, content), btn = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;btn\u0026#34; }, button), msgRight = createElement(\u0026#34;div\u0026#34;, { \u0026#34;class\u0026#34;: \u0026#34;msg-right\u0026#34; }, btn); document.querySelector(\u0026#34;.msg-body\u0026#34;).appendChild(msgRight); scrollToBottom(\u0026#34;.inner-body\u0026#34;); } // On multiple bot messages, ensure avatar is only displayed on last entry function removeDuplicateAvatar(id) { var messages = document.querySelector(\u0026#39;.msg-body\u0026#39;).children; if (messages.length \u0026gt; 0) { var lastMessage = messages[messages.length - 1]; if (lastMessage.getAttribute(\u0026#34;class\u0026#34;) === \u0026#39;msg-left\u0026#39;) { if (lastMessage.lastChild.id == id) { lastMessage.removeChild(lastMessage.lastChild); } } } } // Enable interactive chat function enableChatEditor(chatPrompt, chatText) { if (chatText) { showBotText(chatText); } var chatBox = document.getElementById(\u0026#34;user-msg\u0026#34;); chatBox.setAttribute(\u0026#34;contenteditable\u0026#34;, true); chatBox.setAttribute(\u0026#34;placeholder\u0026#34;, chatPrompt || \u0026#34;Type question here ...\u0026#34;); chatBox.addEventListener(\u0026#34;keydown\u0026#34;, onEditorKeys, false); } // Disable interactive chat function disableChatEditor() { var chatBox = document.getElementById(\u0026#34;user-msg\u0026#34;); chatBox.addEventListener(\u0026#34;keydown\u0026#34;, {}, false); chatBox.setAttribute(\u0026#34;contenteditable\u0026#34;, false); chatBox.setAttribute(\u0026#34;placeholder\u0026#34;, \u0026#34;Choose an option\u0026#34;); } // Scroll to last messages function scrollToBottom(tag) { var div = document.querySelector(tag); div.scrollTop = div.scrollHeight - div.clientHeight; } // Clear messages in both editors function clearMessages() { var parent = document.querySelector(\u0026#39;.msg-body\u0026#39;); while (parent.firstChild) { parent.removeChild(parent.firstChild); } var debugOutput = document.getElementById(\u0026#34;debugOutput\u0026#34;); if (debugOutput) { debugOutput.value = \u0026#34;\u0026#34;; } } // Capture editor keys function onEditorKeys(e) { var chatBox = document.getElementById(\u0026#34;user-msg\u0026#34;); if (e.code == \u0026#39;Enter\u0026#39; \u0026amp;\u0026amp; chatBox.textContent.length \u0026gt; 0) { e.preventDefault(); const content = chatBox.textContent; sendWsMessage({ kind: \u0026#34;UserText\u0026#34;, sessionId: sessionId, content: content, }); showUserText(content); chatBox.innerHTML = \u0026#39;\u0026#39;; } } // Load external javascript file to DOM function loadScript(fileName) { var js_script = document.createElement(\u0026#39;script\u0026#39;); js_script.type = \u0026#34;text/javascript\u0026#34;; js_script.src = fileName; js_script.async = false; document.getElementsByTagName(\u0026#39;head\u0026#39;)[0].appendChild(js_script); } // Log output in the \u0026#34;debugOutput\u0026#34; textarea (if available) and the console function logOutput(value) { var debugOutput = document.getElementById(\u0026#34;debugOutput\u0026#34;); if (debugOutput) { debugOutput.value += value + \u0026#34;\\n\\n\u0026#34;; debugOutput.scrollTop = debugOutput.scrollHeight; } console.log(value); } // Create element utility function function createElement(element, attribute, inner) { if (typeof (element) === \u0026#34;undefined\u0026#34;) { return false; } if (typeof (inner) === \u0026#34;undefined\u0026#34;) { inner = \u0026#34;\u0026#34;; } var el = document.createElement(element); if (typeof (attribute) === \u0026#39;object\u0026#39;) { for (var key in attribute) { el.setAttribute(key, attribute[key]); } } if (!Array.isArray(inner)) { inner = [inner]; } for (var k = 0; k \u0026lt; inner.length; k++) { if (inner[k].tagName) { el.appendChild(inner[k]); } else { el.innerHTML = inner[k]; } } return el; } // Call main function loadAssistant(); }; The script is invoked during window.onload event. The functions are as follows:\nloadAssistant - creates DOM elements for the button and editor, and attaches event listeners. onOpenDialog - shows dialog and hides button. onCloseDialog - shows button and hides dialog. onResetSession - clears the session cookie and establishes a new connection (a new cookie gets assigned). openWsConnection - connects to websocket server and attaches event listeners. closeWsConnection - closes the connection onMessageFromServer - parses messages received from the server and publishes the result in the chat editor. sendWsMessage - sends a message to the server. loadScript - loads another script into the DOM. createElement - a utility function that makes it easy to create DOM elements. The other APIs are manipulating various DOM elements, enable/disable chat editor and clear the messages.\nThe script also loads reconnecting-websocket.js file which is discussed in the next section.\nLoad reconnecting-socket.js file The client is responsible for establishing and maintaining the connection to the server. While vanilla websocket offers the primitives to connect and disconnect to and from the server, it leaves it up to the user to implement reconnects.\nJoe Walnes has written a great utility called reconnecting-socket.js that that that implements the reconnection logic under the hood.\nLet\u0026rsquo;s copy the file in the public/scripts directory:\n%copy first-line%\n$ curl -L https://raw.githubusercontent.com/infinyon/fluvio-demo-apps-node/master/bot-assistant/public/scripts/reconnecting-socket.js --output public/scripts/reconnecting-socket.js Let\u0026rsquo;s review the public directory hierarchy:\npublic ├── css │ └── assistant.css ├── img │ └── assistant │ ├── bot.svg │ ├── close.svg │ ├── note.svg │ └── redo.svg ├── index.html └── scripts ├── assistant.js └── reconnecting-socket.js The frontend client is available for download in github.\nTest Bot Assistant (v1) Ensure the server is running, otherwise run the following command:\n%copy first-line%\n$ PARAMS=state-machines/bot-assistant.json npm run start:server In the web browser, open http://localhost:9999/, then click on \u0026ldquo;Bot Assistant` button. Click on the choices and see the bot assistant traverse through our state machine:\nCongratulations, Bot Assistant is up and running.\nStep 4: Add data streaming and persistency As seen in the previous section, Bot Assistant works well, but it has some limitations. If the webserver restarts, all messages are lost and all user sessions are reset.\nWe\u0026rsquo;ll use Fluvio to remediate this issue. Fluvio is a high throughput, low latency data streaming platform that scales horizontally to handle persistency for a large number of concurrent messages.\nWe can deploy Fluvio between the connection proxy and the workflow controller, which also enables us to divide our monolith into two independent services (aka. microservices): Proxy Service and Workflow Service:\nWhen services are bridged by Fluvio we gain additional benefits:\nscale the proxy and workflow independently of each other.\nhandoff a conversation to a human operator. We can do that by adding an operator service independently that interacts directly with the client through the data stream.\nadd-on services such as: analytics, machine learning, or connectors to other products.\nWe can also remove the circular reference hack we implemented between session-controller and workflow-controller.\n-\u0026gt; Prerequisites: This section assumes you have access to a Fluvio cluster. If you don\u0026rsquo;t have access to a cluster, check out the getting started guide to create Infinyon Cloud account.\nTo integrate Fluvio data streaming we\u0026rsquo;ll make the following changes:\nAdd fluvio to session-controller Add fluvio to workflow-controller Add fluvio to bot-server Add fluvio setup script Add fluvio to session-controller In the session-controller.ts file we replace references to workflow-controller with fluvio producers. In addition to that, the session controller can now use fluvio to look-up all transaction for a specific session.\nUpdate src/proxy-service/session-controller.ts with the following code changes:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-out\u0026#34;; import { Message, SID, buildInitMessage, buildResponse, isRequest } from \u0026#34;../messages\u0026#34;; import { TopicProducer, PartitionConsumer, Offset } from \u0026#34;@fluvio/client\u0026#34;; type Messages = Array\u0026lt;Message\u0026gt;; export class SessionController { private sessionMessages: Map\u0026lt;SID, Messages\u0026gt;; private proxyOut: WsProxyOut; private fluvioProducer: TopicProducer; private fluvioConsumer: PartitionConsumer; constructor( proxyOut: WsProxyOut, fluvioProducer: TopicProducer, fluvioConsumer: PartitionConsumer ) { this.sessionMessages = new Map(); this.proxyOut = proxyOut; this.fluvioProducer = fluvioProducer; this.fluvioConsumer = fluvioConsumer; } public async init() { (await this.fluvioConsumer.fetch(Offset.FromBeginning())).toRecords().forEach(msg =\u0026gt; { this.addMessageToSession(JSON.parse(msg)); }); this.show(); this.fluvioConsumer.stream(Offset.FromEnd(), (msg: string) =\u0026gt; { this.processBotMessage(msg); }); } public async sessionOpened(sid: SID, ws: WS) { console.log(`start session - ${sid}`); this.proxyOut.addSession(sid, ws); const messages = this.sessionMessages.get(sid); if (messages) { this.sendMessagesToClient(messages); } else { const message = buildInitMessage(sid); await this.fluvioProducer.sendRecord(JSON.stringify(message), 0); } } public sessionClosed(sid: SID) { console.log(`end session - ${sid}`); this.proxyOut.closeSession(sid); } public async messageFromClient(sid: SID, clientMsg: string) { console.log(`${sid} \u0026lt;== ${clientMsg}`); const clientResponse = buildResponse(sid, JSON.parse(clientMsg)); await this.fluvioProducer.sendRecord(JSON.stringify(clientResponse), 0); } public sendMessagesToClient(messages: Messages) { messages.forEach(message =\u0026gt; { this.sendMessageToClient(message); }); } public sendMessageToClient(message: Message) { if (message.payload) { const clientMessage = message.payload.message; this.proxyOut.sendMessage(message.sid, JSON.stringify(clientMessage)); } } private addMessageToSession(message: Message) { const sid = message.sid; var messages = this.sessionMessages.get(sid); if (!messages) { messages = new Array(); } messages.push(message); this.sessionMessages.set(sid, messages); } public processBotMessage(botMessage: string) { const message: Message = JSON.parse(botMessage); this.addMessageToSession(message); if (isRequest(message.payload)) { this.sendMessageToClient(message); } } private show() { let table = new Map(); for (let [sid, value] of this.sessionMessages) { table.set(sid, value.length); } console.table(table, [\u0026#34;SID\u0026#34;, \u0026#34;Messages\u0026#34;]); } } The code changes are as follows:\nconstructor - saves fluvio topicProducer and topicConsumer in a local variable. init: made async, to fetch fluvio messages and cache them in sessionMessages array, to register processBotMessage callback to fluvioConsumer. sessionOpened - made async to write a new message to the fluvio data stream. messageFromClient - made async to write client messages to fluvio data stream (instead of calling workflow-controller). That\u0026rsquo;s it, session-controller can now be deployed as a stand-alone service without any dependencies on workflow service.\nAdd fluvio to workflow-controller Similarly, in the workflow-controller.ts file we replace references to session-controller with fluvio producers.\nUpdate src/workflow-service/workflow-controller.ts with the following code changes:\n%copy%\nimport { SID, Message, ResponseMessage, ChoiceResponse, UserText, buildRequest, isRequest } from \u0026#34;../messages\u0026#34;; import { StateMachine, State } from \u0026#34;./state-machine\u0026#34;; import { TopicProducer, PartitionConsumer, Offset } from \u0026#34;@fluvio/client\u0026#34;; export class WorkflowController { private stateMachine: StateMachine; private initState: string; private fluvioProducer: TopicProducer; private fluvioConsumer: PartitionConsumer; constructor( stateMachine: StateMachine, fluvioProducer: TopicProducer, fluvioConsumer: PartitionConsumer, ) { this.stateMachine = stateMachine; this.initState = stateMachine.keys().next().value; this.fluvioProducer = fluvioProducer; this.fluvioConsumer = fluvioConsumer; } public init() { this.fluvioConsumer.stream(Offset.FromEnd(), async (sessionMsg: string) =\u0026gt; { await this.processProxyMessage(sessionMsg); }); } private async processNewConnection(sid: SID) { const nextStates = this.processNext(this.initState); await this.sendMessages(sid, nextStates); } private async processNextState(sid: SID, response: ResponseMessage) { const state: string = this.getState(response); const nextStates = this.processNext(state); await this.sendMessages(sid, nextStates); } private getState(response: ResponseMessage) { switch (response.kind) { case \u0026#34;ChoiceResponse\u0026#34;: { return this.getChoiceResponseState(response); } case \u0026#34;UserText\u0026#34;: { return this.getUserTextState(response); } } } private processNext(startState: string) { var nextStates: State[] = []; var state = this.stateMachine.get(startState); while (state) { nextStates.push(state); const next = state.next || \u0026#34;\u0026#34;; state = this.stateMachine.get(next); if (next.length \u0026gt; 0 \u0026amp;\u0026amp; !state) { console.error(`Error: Cannot find next state: ${next}`); } } return nextStates; } private getChoiceResponseState(choiceResponse: ChoiceResponse) { for (let [key, state] of this.stateMachine.entries()) { if (state.matchResponse \u0026amp;\u0026amp; state.matchResponse.kind == choiceResponse.kind \u0026amp;\u0026amp; state.matchResponse.groupId == choiceResponse.groupId \u0026amp;\u0026amp; state.matchResponse.itemId == choiceResponse.itemId) { return key; } } console.error(`Error: cannot find choice ${JSON.stringify(choiceResponse)}`); return this.initState; } private getUserTextState(userText: UserText) { for (let [key, state] of this.stateMachine.entries()) { if (state.matchResponse \u0026amp;\u0026amp; state.matchResponse.kind == \u0026#34;UserText\u0026#34; \u0026amp;\u0026amp; state.matchResponse.sessionId == userText.sessionId) { return key; } } console.error(`Error: cannot find user session ${JSON.stringify(userText)}`); return this.initState; } private async sendMessages(sid: SID, nextStates: State[]) { for (let idx = 0; idx \u0026lt; nextStates.length; idx++) { const state = nextStates[idx]; if (state.sendRequest) { const message = buildRequest(sid, state.sendRequest); await this.fluvioProducer.sendRecord(JSON.stringify(message), 0); } } } public async processProxyMessage(clientMessage: string) { const message: Message = JSON.parse(clientMessage); if (!isRequest(message.payload)) { const sid = message.sid; if (message.payload) { this.processNextState( sid, \u0026lt;ResponseMessage\u0026gt;message.payload.message ); } else { await this.processNewConnection(sid); } } } } The code changes are as follows:\nconstructor - saves fluvio topicProducer and topicConsumer in a local variable. init - registers processProxyMessage to callback fluvioConsumer. processNewConnection - made async to use sendMessages. processNextState - made async to use sendMessages. sendMessages - made async to write client message to the fluvio data stream (instead of calling session-controller). processProxyMessage - made async to processNewConnection. The workflow controller is now a stand-alone service decoupled from session-controller. The Fluvio middle tier allows these services to be moved to a different machine and be scaled-up independently. However, this improvement is beyond the scope of this blog.\nAdd fluvio to bot-server The bot-server is responsible for the initialization of the producer and consumer. After initialization, the producer and the consumer are passed to the session-controller and workflow-controller for processing.\nUpdate src/bot-server.ts with the following code changes:\n%copy%\nimport { Server } from \u0026#34;http\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-service/proxy-in\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-service/proxy-out\u0026#34;; import { StateMachine, loadStateMachine } from \u0026#34;./workflow-service/state-machine\u0026#34;; import { WorkflowController } from \u0026#34;./workflow-service/workflow-controller\u0026#34;; import { SessionController } from \u0026#34;./proxy-service/session-controller\u0026#34;; import Fluvio from \u0026#39;@fluvio/client\u0026#39;; const BOT_ASSIST_MESSAGES = \u0026#34;bot-assist-messages\u0026#34;; export const initBotAssistant = async (server: Server) =\u0026gt; { const fluvio = await Fluvio.connect(); await checkTopic(fluvio); const fluvioProducer = await fluvio.topicProducer(BOT_ASSIST_MESSAGES); const fluvioConsumer = await fluvio.partitionConsumer(BOT_ASSIST_MESSAGES, 0); const wsProxyOut = new WsProxyOut(); const sessionController = new SessionController(wsProxyOut, fluvioProducer, fluvioConsumer); const wsProxyIn = new WsProxyIn(sessionController); let filePath = getFileName(); const stateMachine: StateMachine = loadStateMachine(filePath); const workflowController = new WorkflowController(stateMachine, fluvioProducer, fluvioConsumer); await sessionController.init(); workflowController.init(); wsProxyIn.init(server); }; const getFileName = () =\u0026gt; { if (process.argv.length != 3) { console.log(\u0026#34;Usage: node bot-assistant.js \u0026lt;state-machine.json\u0026gt;\u0026#34;); process.exit(1); } return process.argv[2]; } const checkTopic = async (fluvio: Fluvio) =\u0026gt; { const admin = await fluvio.admin(); if (!await admin.findTopic(BOT_ASSIST_MESSAGES)) { console.error(\u0026#34;Error: Fluvio topic not found! Run `npm run setup`\u0026#34;); process.exit(1); } } The code changes are as follows:\nBOT_ASSIST_MESSAGES - defines bot assistant topic name. fluvio - connects to fluvio, checks topic existence, and provisions fluvioProducer and fluvioConsumer. SessionController - passes fluvioProducer and fluvioConsumer to session controller. WorkflowController - passes fluvioProducer and fluvioConsumer to workflow controller. Congratulations! You made all code changes for Bot Assistant. Next, we\u0026rsquo;ll add couple of scripts to add/remove topic and we are ready for testing.\nAdd fluvio setup script Fluvio needs a setup script to perform administrative operations such as add/remove topics. Let\u0026rsquo;s add a couple of files to perform these operations and link the files with npm.\n-\u0026gt; This section assumes that the Fluvio CLI is installed on your machine.\nCreate a tools directory and add setup.sh and cleanup.sh files:\n%copy%\nmkdir tools touch tools/setup.sh \u0026amp;\u0026amp; chmod +x tools/setup.sh touch tools/cleanup.sh \u0026amp;\u0026amp; chmod +x tools/cleanup.sh Paste the following in the setup.sh file:\n%copy%\n#!/bin/bash fluvio topic create bot-assist-messages Paste the following in the cleanup.sh file:\n%copy%\n#!/bin/bash fluvio topic delete bot-assist-messages Finally, update package.json file to link the script files:\n%copy%\n{ \u0026#34;name\u0026#34;: \u0026#34;bot-assistant\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;bot-assistant.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start:server\u0026#34;: \u0026#34;tsc-watch --onSuccess \\\u0026#34;node ./dist/bot-assistant.js $PARAMS\\\u0026#34;\u0026#34;, \u0026#34;setup\u0026#34;: \u0026#34;sh ./tools/setup.sh\u0026#34;, \u0026#34;cleanup\u0026#34;: \u0026#34;sh ./tools/cleanup.sh\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;fluvio \u0026lt;admin@fluvio.io\u0026gt; (fluvio.io)\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Apache 2.0\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@fluvio/client\u0026#34;: \u0026#34;^0.6.0-beta.3\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^4.17.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.1.3\u0026#34;, \u0026#34;ws\u0026#34;: \u0026#34;^7.4.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.9\u0026#34;, \u0026#34;@types/node\u0026#34;: \u0026#34;^14.14.19\u0026#34;, \u0026#34;@types/ws\u0026#34;: \u0026#34;^7.4.0\u0026#34;, \u0026#34;tsc-watch\u0026#34;: \u0026#34;^4.2.9\u0026#34; } } Let\u0026rsquo;s run setup, to create the new topic:\n%copy first-line%\n$ npm run setup \u0026gt; bot-assistant@1.0.0 setup /projects/bot-assistant \u0026gt; sh ./tools/setup.sh topic \u0026#34;bot-assist-messages\u0026#34; created Congratulations! You have completed changes in the for Bot Assistant project.\nTest Bot Assistant Repeat the tests in Test Bot Assistant (v1) and refresh the screen. Note that the messages are refreshed as they have been persistent by Fluvio.\nThe persistence also survives server reboots. Go ahead and reboot the server and refresh the browser screen. Notice how the messages are preserved.\nFurthermore, you can now use Fluvio or other programs with a Fluvio consumer interface to read session messages.\nLet\u0026rsquo;s read the last 5 messages with Fluvio CLI:\n%copy first-line%\nfluvio consume bot-assist-messages --offset=\u0026#34;-4\u0026#34; {\u0026#34;sid\u0026#34;:\u0026#34;fb7e2971d989070361c30d825bf6a853a406916e\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Response\u0026#34;,\u0026#34;message\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ChoiceResponse\u0026#34;,\u0026#34;groupId\u0026#34;:\u0026#34;others\u0026#34;,\u0026#34;itemId\u0026#34;:\u0026#34;no\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;No\u0026#34;}},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-05T17:58:46.511\u0026#34;} {\u0026#34;sid\u0026#34;:\u0026#34;fb7e2971d989070361c30d825bf6a853a406916e\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Request\u0026#34;,\u0026#34;message\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;BotText\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;Great, thanks!\u0026#34;}},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-05T17:58:46.514\u0026#34;} {\u0026#34;sid\u0026#34;:\u0026#34;fb7e2971d989070361c30d825bf6a853a406916e\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Response\u0026#34;,\u0026#34;message\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ResetSession\u0026#34;}},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-05T18:04:20.811\u0026#34;} {\u0026#34;sid\u0026#34;:\u0026#34;0fb1574b3b6d7c98c7089aa4a2c58a80894bbc6e\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-05T18:04:20.815\u0026#34;} {\u0026#34;sid\u0026#34;:\u0026#34;0fb1574b3b6d7c98c7089aa4a2c58a80894bbc6e\u0026#34;,\u0026#34;payload\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Request\u0026#34;,\u0026#34;message\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;BotText\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;Hi, I\u0026#39;m Bot! Nice to meet you.\u0026#34;}},\u0026#34;timestamp\u0026#34;:\u0026#34;2021-01-05T18:04:20.817\u0026#34;} When you use fluvio to transfer real-time messages between services, you gain much more than a transport layer. Benefits range from decoupling service to recovering from errors, from monitoring to troubleshooting and much more.\nConclusion In this blog, we explored how to build a robot assistant that can interact with users in real-time. By using Fluvio, our application has a foundation that allows it to scale horizontally to meet the demands of a massive user audience. Since our backend services are stateless and decoupled, we can scale them independently to handle the particular load characteristics that we observe in production, preventing technical bottlenecks.\nThis project also just scratches the surface of what a real-time streaming application can do. By leveraging Fluvio\u0026rsquo;s persistent data streams, we can build improvements to our application by simply writing new microservices that interact with topic data. This gives us the power to develop new real-time features, as well as to analyze historical data for purposes such as Machine Learning personalization use-cases.\nWe hope you enjoyed the blog! If you have any questions or comments, or if you just want to come say hi, you can find us on our community Discord channel → ","description":"Use data streaming to build a custom robot assistant that can connect to any backend service in the organization.","keywords":null,"summary":"Many successful modern applications need to interact with their users in real-time, and this capability is quickly becoming the expected standard. However, building a real-time application from scratch is a daunting task, pulling focus away from the business problems InfinyOn Team is actually trying to solve. Fluvio is a real-time data streaming platform designed to make real-time application development easy.\nIn this blog post, we\u0026rsquo;re going to build a Robot Assistant, an add-on button on the website, that interacts with users in real-time.","title":"Build Your Own Custom Robot Assistant","url":"http://localhost:1315/blog/2021/02/bot-assistant/"},{"body":"","description":"Leverage fluvio data streaming to build a persistent chat app without a database.","keywords":null,"summary":"","title":"Build a Persistent Chat App without a Database","url":"http://localhost:1315/blog/2021/01/persistent-chat-app/"},{"body":"Modern applications need to interact with their users in real-time which require real time data streaming support. While we had Fluvio to handle data streaming, we needed a websocket connection proxy to bridge the data streaming with web applications.\nThis websocket connection layer should have the following properties:\nestablish long-lived websocket connections work natively in javascript for easy integration with front end clients handle cookies for session identification handle session management for server initiated connections scale to a large number of concurrent sessions Since we couldn\u0026rsquo;t find such code we ended up building it ourselves. We used this solution in both of our data streaming apps: Chat App and Bot Assistant.\nIf you want to jump right into it, you can download it from github. The rest of this article takes you through a step-by-step on how we\u0026rsquo;ve built this solution one component at a time.\nOverview Websocket is a well established client/server communication mechanism for real-time data exchange. Most modern web browsers have built-in websocket libraries ready for use by front-end clients. There are also well established websocket libraries for most programming language which gives us a variety of choices for the backend server.\nThis project is using the following technologies:\nJavascript - for the front end client Node.js and TypeScript for backend server. WebSocket - for real-time communication This blog is written in a set-by-step tutorial format:\nStep 1: Add backend server Step 2: Add frontend client Step 3: Add websocket communication Step 4: Add session support This blog is part of a series of tutorials that shows the power of combining the Websocket glue with Fluvio to build full featured data streaming apps.\nArchitecture The goals is to create a session aware client/server websocket mechanism that we can leveraged by any real-time data streaming apps.\nLet\u0026rsquo;s get started.\nStep 1: Add backend server The backend websocket server uses Node.js and Typescript. First we setup the environment, then we implement the websocket connection handler.\nAdd Node.js server Let\u0026rsquo;s create a directory websocket-glue:\n%copy first-line%\n$ mkdir websocket-glue \u0026amp;\u0026amp; cd websocket-glue Initialize node (this example uses Node v13.5.0):\n%copy first-line%\n$ npm init --yes It generates the following package.json file:\n%copy%\n{ \u0026#34;name\u0026#34;: \u0026#34;websocket-glue\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;fluvio \u0026lt;admin@fluvio.io\u0026gt; (fluvio.io)\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Apache 2.0\u0026#34; } Install express, typescript and ts-node services:\n%copy first-line%\n$ npm install typescript ts-node express Install tsc-watch and typescript definitions:\n%copy first-line%\n$ npm install -D tsc-watch @types/node @types/express typescript: package to add static typing and strict syntactical check to JavaScript. ts-node: package for using TypeScript with Node.js. For example: ts-node app.ts. tsc-watch: a development tool to restart the server on code changes. express: web application framework for routing, cookies, and more. @types/node: typescript library for Node.js. @types/express: typescript library express application server. Add typescript configuration Next, create a typescript configuration file:\n%copy first-line%\n$ touch tsconfig.json Paste the following content in tsconfig.json file:\n%copy%\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es6\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;lib\u0026#34;: [ \u0026#34;dom\u0026#34;, \u0026#34;ES2017\u0026#34;, \u0026#34;ES2015\u0026#34; ], \u0026#34;outDir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;./src\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, } } For additional information on the typescript configuration parameters, checkout the documentation here.\nAdd server.ts file It\u0026rsquo;s time to add the src directory and provision the server.ts file:\n%copy first-line%\n$ mkdir src \u0026amp;\u0026amp; touch src/server.ts Paste the following content in src\\server.ts file:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); In summary, the code:\ncreates an express application framework. attaches express to the http webserver. starts the webserver on port 9998. Update Node.js configuration Updated package.json configuration file to run server.ts file:\n%copy%\n{ \u0026#34;name\u0026#34;: \u0026#34;websocket-glue\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;server.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start:dev\u0026#34;: \u0026#34;tsc-watch --onSuccess \\\u0026#34;node ./dist/server.js\\\u0026#34;\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;npx ts-node ./src/server.ts\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;fluvio \u0026lt;admin@fluvio.io\u0026gt; (fluvio.io)\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Apache 2.0\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;express\u0026#34;: \u0026#34;^4.17.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^9.1.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.1.3\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.9\u0026#34;, \u0026#34;@types/node\u0026#34;: \u0026#34;^14.14.16\u0026#34;, \u0026#34;tsc-watch\u0026#34;: \u0026#34;^4.2.9\u0026#34; } } update main to server.js add 2 scripts: start:dev for development and start for production. Test backend server Let\u0026rsquo;s sanity check the directory hierarchy:\n%copy first-line%\n$ tree -I \u0026#39;node_modules|dist\u0026#39; . ├── package-lock.json ├── package.json ├── src │ └── server.ts └── tsconfig.json Run the server:\n%copy first-line%\n$ npm run start:dev 7:28:31 AM - Starting compilation in watch mode... 7:28:34 AM - Found 0 errors. Watching for file changes. started websocket server at ws://localhost:9998... Congratulations, the server is up and running.\nStep 2: Add frontend client The frontend client is a simple html and javascript file that establishes a websocket connection with the server. In this section we\u0026rsquo;ll hook-up the html file to the web server.\nAdd index.html file The webserver we have implemented for the backend will also serve the frontend. We\u0026rsquo;ll add the frontend code in a public directory.\nLet\u0026rsquo;s create the public directory and add index.html file:\n%copy first-line%\n$ mkdir public \u0026amp;\u0026amp; touch public/index.html Paste the following code in index.html file:\n%copy%\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;style\u0026gt;body {background-color:#f4f6f5;} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt; \u0026lt;button id=\u0026#34;connect\u0026#34;\u0026gt;Connect\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026#34;disconnect\u0026#34;\u0026gt;Disconnect\u0026lt;/button\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;input id=\u0026#34;message\u0026#34; type=\u0026#34;text\u0026#34; size=\u0026#34;20\u0026#34; maxlength=\u0026#34;240\u0026#34; placeholder=\u0026#34;ping\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;sendMessage\u0026#34;\u0026gt;SendMessage\u0026lt;/button\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;textarea id=\u0026#34;output\u0026#34; rows=\u0026#34;20\u0026#34; cols=\u0026#34;60\u0026#34; readonly\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The code has a standard HTML header with some background color and a body with the following elements:\nconnect and disconnect buttons to connect with websocket server. message input and sendMessage button to send messages to the server. output textarea to log the progress of the communication exchange. Hookup index.html to server To hook-up index.html file we need to update src\\server.ts:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); cache publicPath, the path to the public directory. add a route to retrieve index.html for the root URL. -\u0026gt; NOTE: tsc-watch automatically refreshed the server.\nTest frontend client Open a web browser and navigate to http://localhost:9998/.\nCongrats! Client/server scaffolding is done. Next we\u0026rsquo;ll focus on the websocket communication.\nStep 3: Add websocket communication In this section, we\u0026rsquo;ll implement the client and the server side of the WebSocket (WS) protocol. Let\u0026rsquo;s start with the server side.\nImplement websocket server Server side websocket implementation has has multiple steps:\nInstall ws package Add incoming proxy Attach proxy to server Install ws package In node there is a Websocket package, called ws, available for download through npm.\nInstall ws package and the typescript definition file:\n%copy first-line%\nnpm install ws \u0026amp;\u0026amp; npm install -D @types/ws Add incoming proxy Our websocket implementation is a proxy server that intermediates the communication between clients and the server business logic. As the solution allows the clients or the server to initiate requests, we\u0026rsquo;ll create two separate files proxy-in and proxy-out. This separation gives us better division of responsibility between incoming and outgoing requests. We\u0026rsquo;ll come back to this in Step 4.\nIncoming proxy is responsible for the websocket negotiation and message handling for incoming requests. Let\u0026rsquo;s add the file:\n%copy first-line%\n$ touch src/proxy-in.ts Paste the following content in the src/proxy-in.ts file:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; import http from \u0026#34;http\u0026#34;; export class WsProxyIn { private static wss: WS.Server; constructor() { WsProxyIn.wss = new WS.Server({ clientTracking: false, noServer: true }); } public init(server: http.Server) { this.onUpgrade(server); this.onConnection(); } private onUpgrade(server: http.Server) { server.on(\u0026#34;upgrade\u0026#34;, function (request, socket, head) { WsProxyIn.wss.handleUpgrade(request, socket, head, function (ws: WS) { WsProxyIn.wss.emit(\u0026#34;connection\u0026#34;, ws, request); }); }); } private onConnection() { WsProxyIn.wss.on(\u0026#34;connection\u0026#34;, function (ws, req) { console.log(\u0026#34;session opened\u0026#34;); ws.on(\u0026#34;close\u0026#34;, function () { console.log(\u0026#34;session closed\u0026#34;); }); ws.on(\u0026#34;message\u0026#34;, (clientMsg: string) =\u0026gt; { console.log(`\u0026lt;== ${clientMsg}`); var response = \u0026#34;ok\u0026#34;; if (clientMsg == \u0026#34;ping\u0026#34;) { response = \u0026#34;pong\u0026#34;; } ws.send(response); console.log(\u0026#34;==\u0026gt; \u0026#34;, response); }); }); } } In summary, the code:\naccepts new websocket connections in the upgrade event. emits a connection event. captures the connection event in a new thread. checks the data in the message event and replies with pong for ping and ok for everything else. For addition information on how websocket package works, checkout the documentation in github.\nAttach proxy to server We\u0026rsquo;ll hook-up the incoming proxy in the src/server.ts file:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-in\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;); const wsProxyIn = new WsProxyIn(); wsProxyIn.init(Server); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); The code imports and initializes the incoming proxy with the websocket server instance.\nImplement websocket client The client utilizes the Websocket API available in most modern web browsers.\nClient side websocket implementation has several steps:\nAdd websocket script file Load script to html file Add scripts route to server Add websocket script file The script implements websocket connection and messages exchanges. It also attaches the DOM buttons we defined in the index.html file.\nIn a terminal, create the ws-script.js file:\n%copy first-line%\n$ mkdir public/scripts \u0026amp;\u0026amp; touch public/scripts/ws-script.js Paste the following code in public/scripts/ws-script.js:\n%copy%\nconst SERVER_HOST = \u0026#34;localhost:9998\u0026#34;; window.onload = () =\u0026gt; { var webSocket = null; function init() { var connectBtn = document.getElementById(\u0026#34;connect\u0026#34;); var disconnectBtn = document.getElementById(\u0026#34;disconnect\u0026#34;); var sendMessageBtn = document.getElementById(\u0026#34;sendMessage\u0026#34;); var messageEditor = document.getElementById(\u0026#34;message\u0026#34;); connectBtn.onclick = onConnectClick; disconnectBtn.onclick = onDisconnectClick; sendMessageBtn.onclick = onSendMessageClick; messageEditor.addEventListener(\u0026#34;keydown\u0026#34;, onEditorKeys, false); } function openWSConnection() { try { if (webSocket != null) { return; } const server = \u0026#34;ws://\u0026#34; + SERVER_HOST; logOutput(\u0026#34;Connecting to: \u0026#34; + server); webSocket = new WebSocket(server); webSocket.onopen = function (openEvent) { logOutput(\u0026#34;Connected!\u0026#34;); }; webSocket.onclose = function (closeEvent) { webSocket = null; logOutput(\u0026#34;Disconnected!\u0026#34;); }; webSocket.onerror = function (errorEvent) { logOutput(`Error: ${JSON.stringify(errorEvent)}`); }; webSocket.onmessage = function (messageEvent) { var serverMsg = messageEvent.data; logOutput(`\u0026lt;== ${serverMsg}`); }; } catch (exception) { logOutput(`error: ${JSON.stringify(exception)}`); } } function onConnectClick() { openWSConnection(); } function onDisconnectClick() { if (webSocket) { webSocket.close(); } else { logOutput(`Not Connected!`); } } function onSendMessageClick() { if (webSocket) { var message = document.getElementById(\u0026#34;message\u0026#34;).value || \u0026#34;ping\u0026#34;; logOutput(\u0026#34;==\u0026gt; \u0026#34; + message); webSocket.send(message); } else { logOutput(`Not Connected!`); } } function onEditorKeys(e) { if (e.code == \u0026#39;Enter\u0026#39;) { e.preventDefault(); onSendMessageClick(); } } function logOutput(value) { var debugOutput = document.getElementById(\u0026#34;output\u0026#34;); if (debugOutput) { debugOutput.value += value + \u0026#34;\\n\\n\u0026#34;; debugOutput.scrollTop = debugOutput.scrollHeight; } console.log(value); } init(); }; The code performs the following operations:\ninit: attaches DOM objects to their handlers. openWSConnection: uses WebSocket API to initiate a websocket connection. It also registers callback handlers for websocket operations. onConnectClick: click handler to open a websocket connection. onDisconnectClick: click handler to close the websocket connection. onSendMessageClick: click handler to send message on websocket connection. onEditorKeys: Captures the enter key invoke send message. logOutput: Prints messages in the textarea. Load script to html file Next, we\u0026rsquo;ll update index.html to load the ws-script.js:\n%copy%\n\u0026lt;!DOCTYPE HTML\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;style\u0026gt;body {background-color:#f4f6f5;} \u0026lt;/style\u0026gt; \u0026lt;script type = \u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts/ws-script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt; \u0026lt;button id=\u0026#34;connect\u0026#34;\u0026gt;Connect\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026#34;disconnect\u0026#34;\u0026gt;Disconnect\u0026lt;/button\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;input id=\u0026#34;message\u0026#34; type=\u0026#34;text\u0026#34; size=\u0026#34;20\u0026#34; maxlength=\u0026#34;240\u0026#34; placeholder=\u0026#34;ping\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;sendMessage\u0026#34;\u0026gt;SendMessage\u0026lt;/button\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;textarea id=\u0026#34;output\u0026#34; rows=\u0026#34;20\u0026#34; cols=\u0026#34;60\u0026#34; readonly\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The code attaches a script tag that loads ws-script.js file to the HTML headers.\nAdd scripts route to serve The script is not yet visible to your web server. Let\u0026rsquo;s add a route to the web server src/server.ts file:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-in\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;); const wsProxyIn = new WsProxyIn(); wsProxyIn.init(Server); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); app.use(\u0026#34;/scripts\u0026#34;, express.static(path.join(publicPath, \u0026#39;scripts\u0026#39;))); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); The code defines a route that expose the /scripts directory to the web server.\nTest websocket communication Open the web browser and refresh http://localhost:9998/ to load the latest javascript code. You should see the same page as before, but the buttons should now be operational. Let\u0026rsquo;s establish a a connection and send a message:\nclick Connect should connect to the websocket server. click Send Message should send \u0026lsquo;ping\u0026rsquo; to server. server should respond with \u0026lsquo;pong\u0026rsquo; type \u0026lsquo;hello word\u0026rsquo; and click Send Message again. server should respond with \u0026lsquo;ok\u0026rsquo;. click Disconnect to release websocket connection. Congratulations, you have a basic websocket glue is ready for use.\nStep 4: Add session support A websocket servers must be able to support multiple conversations in parallel, where each conversation is uniquely identified by a session id. The preferred method to managed these conversations is through HTTP cookies.\nOnce a websocket session is established both, the client and the server can leverage it to initiate requests. Client initiated requests are straight forward as the session information is passed through header cookies. Server initiated requests are bit more involved as they require a session management layer.\nIn this section covers both aspects:\nAdd session cookies for client requests Add session management for server requests Add session cookies for client requests Web servers push cookies to clients through HTTP headers. After receipt, the clients attaches the cookies to each subsequent message it sends the server. Messages exchanged using the same cookie are also known as sessions.\nLet\u0026rsquo;s updates src/proxy-in.ts websocket server to generate and push session cookies:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; import http from \u0026#34;http\u0026#34;; import crypto from \u0026#39;crypto\u0026#39;; const COOKIE_NAME = \u0026#34;CookieName\u0026#34; export class WsProxyIn { private static wss: WS.Server; constructor() { WsProxyIn.wss = new WS.Server({ clientTracking: false, noServer: true }); } public init(server: http.Server) { this.onUpgrade(server); this.onConnection(); } private onUpgrade(server: http.Server) { server.on(\u0026#34;upgrade\u0026#34;, function (request, socket, head) { const session = WsProxyIn.parseCookie(COOKIE_NAME, request.headers.cookie); if (session) { request.headers.session = session; } WsProxyIn.wss.handleUpgrade(request, socket, head, function (ws: WS) { WsProxyIn.wss.emit(\u0026#34;connection\u0026#34;, ws, request); }); }); } private onConnection() { WsProxyIn.wss.on(\u0026#34;headers\u0026#34;, (headers: Array\u0026lt;string\u0026gt;, req) =\u0026gt; { const session = WsProxyIn.parseCookie(COOKIE_NAME, req.headers.cookie); if (!session) { let session = crypto.randomBytes(20).toString(\u0026#34;hex\u0026#34;); req.headers.session = session; headers.push(\u0026#34;Set-Cookie: \u0026#34; + COOKIE_NAME + \u0026#34;=\u0026#34; + session); } }); WsProxyIn.wss.on(\u0026#34;connection\u0026#34;, function (ws, req) { const session_hdr = req.headers.session; const sid = ((Array.isArray(session_hdr)) ? session_hdr[0] : session_hdr) || \u0026#34;\u0026#34;; console.log(`session opened: ${sid}`); ws.on(\u0026#34;close\u0026#34;, function () { console.log(`session closed: ${sid}`); }); ws.on(\u0026#34;message\u0026#34;, (clientMsg: string) =\u0026gt; { console.log(`\u0026lt;== ${clientMsg} from ${sid}`); var response = \u0026#34;ok\u0026#34;; if (clientMsg == \u0026#34;ping\u0026#34;) { response = \u0026#34;pong\u0026#34;; } ws.send(response); console.log(\u0026#34;==\u0026gt; \u0026#34;, response); }); }); } private static parseCookie(cookieName: string, cookie_hdr?: string) { if (cookie_hdr) { const cookiePair = cookie_hdr.split(/; */).map((c: string) =\u0026gt; { const [key, v] = c.split(\u0026#39;=\u0026#39;, 2); return [key, decodeURIComponent(v)]; }).find(res =\u0026gt; (res[0] == cookieName) ); if (Array.isArray(cookiePair) \u0026amp;\u0026amp; cookiePair.length \u0026gt; 1) { return cookiePair[1]; } } return undefined; } } The code generates and assigns a session id to a cookie with arbitrary name. Then, it utilizes WebSocket headers to send it to the client.\nThe webSocket connection setup in two steps:\nheaders request connection request. In the headers request, the code checks the HTTP headers for the session cookie. If not found, it generates a new session id and appends the result to the HTTP header. WebSocket will take care of the rest. In connection callback, the code reads the session id from request.headers.session.\nFinally, parseSessionFromCookie reads the session id from the cookie header and returns the the caller.\nTest session cookies Open the web browser and refresh http://localhost:9998/. Then, open browser cookies and look for an entry called CookieName. The cookie stores the session id.\nIf you open a 2nd browser in Incognito Mode a new cookie is assigned.\nAt the terminal the web server logs new connection with the cookie information.\nstarted websocket server at ws://localhost:9998... session opened: 8771b27ee1c8af52e8473c2d8f8e3d932a654155 \u0026lt;== hello from 8771b27ee1c8af52e8473c2d8f8e3d932a654155 ==\u0026gt; ok session opened: 3cd4c1fd800f8adb0b1ca5a5a8cce80ce0335788 \u0026lt;== ping from 3cd4c1fd800f8adb0b1ca5a5a8cce80ce0335788 ==\u0026gt; pong session closed: 3cd4c1fd800f8adb0b1ca5a5a8cce80ce0335788 session closed: 8771b27ee1c8af52e8473c2d8f8e3d932a654155 Congratulations, you have implemented session cookies using websocket.\nAdd session management for server requests Session management is an intermediate file that caches the session id with the websocket connection object. These cached connections allows server modules to send messages to any of the clients based on their session identifier.\nAdd session management file Let\u0026rsquo;s add the session management to a file called proxy-out:\n%copy first-line%\n$ touch src/proxy-out.ts Paste the following content in the src/proxy-out.ts file:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; type SID = string; export class WsProxyOut { private sessions: Map\u0026lt;SID, WS\u0026gt;; constructor() { this.sessions = new Map(); } public addSession(sid: SID, ws: WS) { this.sessions.set(sid, ws); } public getSession(sid: SID) { this.sessions.get(sid); } public closeSession(sid: SID) { const ws = this.sessions.get(sid); if (ws) { ws.close(); } this.sessions.delete(sid); } public sendMessage(sid: SID, message: string) { const ws = this.sessions.get(sid); if (ws) { ws.send(message); } } public broadcastMessage(message: string) { for (let ws of this.sessions.values()) { ws.send(message); } } public show() { console.log(\u0026#34;Sessions\u0026#34;); console.table(this.sessions); } } WsProxyOut class saves session ids (SID) together with the Websocket connection in a Map object. Any module with a reference to this object can send messages to one or all clients.\nIntegrated session management Sessions are generated inside incoming proxy file, hence the integration is done there. However, the sessions must be shareable with any number of server modules and it needs to be a top level object.\nLet\u0026rsquo;s update src/proxy-in.ts:\n%copy%\nimport WS from \u0026#34;ws\u0026#34;; import http from \u0026#34;http\u0026#34;; import crypto from \u0026#39;crypto\u0026#39;; import { WsProxyOut } from \u0026#34;./proxy-out\u0026#34;; const COOKIE_NAME = \u0026#34;CookieName\u0026#34; export class WsProxyIn { private static wss: WS.Server; private static proxyOut: WsProxyOut constructor(proxyOut: WsProxyOut) { WsProxyIn.wss = new WS.Server({ clientTracking: false, noServer: true }); WsProxyIn.proxyOut = proxyOut; } public init(server: http.Server) { this.onUpgrade(server); this.onConnection(); } private onUpgrade(server: http.Server) { server.on(\u0026#34;upgrade\u0026#34;, function (request, socket, head) { const session = WsProxyIn.parseCookie(COOKIE_NAME, request.headers.cookie); if (session) { request.headers.session = session; } WsProxyIn.wss.handleUpgrade(request, socket, head, function (ws: WS) { WsProxyIn.wss.emit(\u0026#34;connection\u0026#34;, ws, request); }); }); } private onConnection() { WsProxyIn.wss.on(\u0026#34;headers\u0026#34;, (headers: Array\u0026lt;string\u0026gt;, req) =\u0026gt; { const session = WsProxyIn.parseCookie(COOKIE_NAME, req.headers.cookie); if (!session) { let session = crypto.randomBytes(20).toString(\u0026#34;hex\u0026#34;); req.headers.session = session; headers.push(\u0026#34;Set-Cookie: \u0026#34; + COOKIE_NAME + \u0026#34;=\u0026#34; + session); } }); WsProxyIn.wss.on(\u0026#34;connection\u0026#34;, function (ws, req) { const session_hdr = req.headers.session; const sid = ((Array.isArray(session_hdr)) ? session_hdr[0] : session_hdr) || \u0026#34;\u0026#34;; WsProxyIn.proxyOut.addSession(sid, ws); console.log(`session opened: ${sid}`); ws.on(\u0026#34;close\u0026#34;, function () { WsProxyIn.proxyOut.closeSession(sid); console.log(`session closed: ${sid}`); }); ws.on(\u0026#34;message\u0026#34;, (clientMsg: string) =\u0026gt; { console.log(`\u0026lt;== ${clientMsg} from ${sid}`); var response = \u0026#34;ok\u0026#34;; if (clientMsg == \u0026#34;ping\u0026#34;) { response = \u0026#34;pong\u0026#34;; } WsProxyIn.proxyOut.sendMessage(sid, response); console.log(\u0026#34;==\u0026gt; \u0026#34;, response); }); }); } private static parseCookie(cookieName: string, cookie_hdr?: string) { if (cookie_hdr) { const cookiePair = cookie_hdr.split(/; */).map((c: string) =\u0026gt; { const [key, v] = c.split(\u0026#39;=\u0026#39;, 2); return [key, decodeURIComponent(v)]; }).find(res =\u0026gt; (res[0] == cookieName) ); if (Array.isArray(cookiePair) \u0026amp;\u0026amp; cookiePair.length \u0026gt; 1) { return cookiePair[1]; } } return undefined; } } The code adds a private static variable proxyOut which stores a pointer to the session management object. When the connection status changes, proxyOut is notified to update its internal cache.\nOnMessage API was also changed to use proxyOut for sending messages using the session id.\nNext, we need to update src\\server.ts to crate the session manager object an pass it to the incoming proxy:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-in\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-out\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;); const proxyOut = new WsProxyOut(); const wsProxyIn = new WsProxyIn(proxyOut); wsProxyIn.init(Server); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); app.use(\u0026#34;/scripts\u0026#34;, express.static(path.join(publicPath, \u0026#39;scripts\u0026#39;))); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); The code create a new WsProxyOut object and passes it to WsProxyIn.\nTest server initiated requests To test server initiated requests, we\u0026rsquo;ll add a simple timeout that triggers every 3 seconds and broadcasts a notification to all clients.\nWe\u0026rsquo;ll temporarily add a trigger to src\\server.ts:\n%copy%\nimport http from \u0026#34;http\u0026#34;; import express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; import { WsProxyIn } from \u0026#34;./proxy-in\u0026#34;; import { WsProxyOut } from \u0026#34;./proxy-out\u0026#34;; const PORT = 9998; const startServer = async () =\u0026gt; { const app = express(); const Server = http.createServer(app); const publicPath = path.join(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;); const proxyOut = new WsProxyOut(); const wsProxyIn = new WsProxyIn(proxyOut); wsProxyIn.init(Server); // test broadcast setInterval(() =\u0026gt; { proxyOut.broadcastMessage(\u0026#34;notify\u0026#34;); }, 3000); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.sendFile(path.join(publicPath, \u0026#39;index.html\u0026#39;)); }); app.use(\u0026#34;/scripts\u0026#34;, express.static(path.join(publicPath, \u0026#39;scripts\u0026#39;))); Server.listen(PORT, () =\u0026gt; { console.log( `started websocket server at ws://localhost:${PORT}...` ); }); }; startServer(); Navigate to your browser pointing at http://localhost:9998/ and connect. The notify message is printed to the output every 3 seconds:\nCongratulations, your session aware websocket glue is ready for use.\nConclusion Websocket is a good choice for streaming real-time data between web server clients and backend servers. The blog shows that 220 lines of code is all you need to build your own real-time applications on websocket.\nThis websocket glue is used in the following sample applications:\nBuild your own custom Bot Assistant Build a persistent Chat App without a database You can reach us on discord. We look forward to hearing from you.\n","description":"Create session-aware websocket glue that can be utilized by data streaming applications.","keywords":null,"summary":"Modern applications need to interact with their users in real-time which require real time data streaming support. While we had Fluvio to handle data streaming, we needed a websocket connection proxy to bridge the data streaming with web applications.\nThis websocket connection layer should have the following properties:\nestablish long-lived websocket connections work natively in javascript for easy integration with front end clients handle cookies for session identification handle session management for server initiated connections scale to a large number of concurrent sessions Since we couldn\u0026rsquo;t find such code we ended up building it ourselves.","title":"Websocket Glue for Data Streaming Apps","url":"http://localhost:1315/blog/2021/01/websocket-glue-for-streaming-apps/"},{"body":"","description":"","keywords":null,"summary":"","title":"ArrayMap Json","url":"http://localhost:1315/docs/smartmodules/array-map-json/"},{"body":"Manage sensitive data like passwords and API keys securely in Fluvio Cloud using the fluvio cloud secret command. For more details, see the Cloud documentation.\nSecret names For the sake of portability, secret names must\nConsist solely of letters, digits, and the underscore (_) Must not begin with a digit. Set secret values on Cloud %copy first-line%\n$ fluvio cloud secret set my_secret my-secret-value Secret \u0026#34;my_secret\u0026#34; set successfully Using secrets with connectors Cloud Connectors support secrets. The secrets can be referenced in the connector configuration file.\nIn order to use secrets, first, they need to be defined in the metadata section of the connector configuration file. The secrets are defined as a list of names. The names are used to reference the secret in the connector configuration file.\n%copy%\n# http-source-connector-with-secrets.yml apiVersion: 0.1.0 meta: version: 0.2.1 name: my-http-source-connector type: http-source topic: my-topic secrets: - name: MY_TOKEN http: endpoint: https://my.secure.api/ interval: 10s headers: - \u0026#34;AUTHORIZATION: token ${{ secrets.MY_TOKEN }}\u0026#34; In that config, it is defined a MY_TOKEN secret and it is used in the headers configuration of the http-source connector.\n","description":"A short tutorial for using Secret","keywords":null,"summary":"Manage sensitive data like passwords and API keys securely in Fluvio Cloud using the fluvio cloud secret command. For more details, see the Cloud documentation.\nSecret names For the sake of portability, secret names must\nConsist solely of letters, digits, and the underscore (_) Must not begin with a digit. Set secret values on Cloud %copy first-line%\n$ fluvio cloud secret set my_secret my-secret-value Secret \u0026#34;my_secret\u0026#34; set successfully Using secrets with connectors Cloud Connectors support secrets.","title":"Cloud Secrets Basics","url":"http://localhost:1315/docs/tutorials/secrets-basics/"},{"body":"Overview This Cookie Policy (“Policy”) explains how Infinyon, Inc., and subsidiaries (collectively, “Infinyon”, “we”, “us” or “our”) use cookies and similar technologies to recognize you when you visit one of our websites, including any other media form, media channel, mobile website, or mobile application related or connected thereto provided or officially sponsored by Infinyon, or any other Infinyon websites that link to this Policy (collectively, the “Websites”).\nThis Policy explains what these technologies are and why we use them, as well as our users’ (“data subject”, “user”, “you” or “your”) rights to control our use of them.\nIn some cases, we may use cookies to collect personal information, or that becomes personal information if we combine it with other information. In such cases, our Privacy Policy will apply in addition to this Cookie Policy.\nWhat are cookies? Cookies are small text files that are stored on your computer or mobile device when you visit a website. Cookies are created when you visit a website that uses cookies to keep track of your movements within the site, help you resume where you left off, and any preferences. Websites store a corresponding file to the one that is set in your browser, and in this file websites track and keep information on your movements within the site, and any information you may voluntarily give while visiting the website, such as email address.\nWhy do we use cookies? We use cookies for several reasons, such as to provide you with a consistent and customized experience on our Websites. Like most organizations, we also partner with selected third parties, listed below, for tracking and marketing purposes on our Websites; these cookies enable us to analyze and track the use of our Websites, determine the popularity of certain content and better understand online activity.\nYou are encouraged to review the privacy policies of these third parties and contact them directly for responses to your questions.\nEssential cookies These are necessary to provide you with the services available through our Websites and to use some of its features, such as access to secure or restricted areas.\nWho serves these cookies: Infinyon, Inc. We collect IP addresses and browser metadata, such as IP addresses and device information.\nhttps://www.infinyon.com\nNetlify Netlify is our hosting provider.\nhttps://www.netlify.com/\nCloudFlare CloudFlare provides DNS services.\nhttps://www.cloudflare.com/cookie-policy/\nHow to opt-out: Because these cookies are necessary for our Websites to function, you cannot opt-out. You can, however, block or delete them by changing your browser settings as described below under the heading “How can I control cookies?”\nAnalytics cookies These cookies are used to enhance the performance and functionality of our Websites, but are generally non-essential to their use. Without these cookies, certain functionality may become unavailable.\nAnalytics services enhance the performance and functionality of our Websites, and keep track of page traffic and user behavior while browsing the site. We use this data internally to improve the usability and performance of the site. Disabling any of these scripts makes it more difficult for us to understand how our site is being used, and slower to improve it.\nWho serves these cookies: Google Analytics https://analytics.google.com/analytics/web/\nGoogle Analytics is a popular service for tracking web traffic. We use this data to tailor content to our users\nHow to opt-out: Please also refer to “How can I control cookies?” section below.\nSocial networking cookies By accessing social networks or using social features of our Websites, these social networking services or providers may set additional cookies, which may be used to track your browsing activity. We do not have control over these cookies. You are encouraged to review the relevant privacy and cookie policies for each respective social network provider to ensure you are comfortable with their data privacy practices. To opt-out, please refer to each respective social network provider. Additional resources may be found in the “How can I control cookies?” section of this policy.\nWhat about other tracking technologies? In addition to cookies, we use other tracking technologies like tracking pixels. A tracking pixel is an HTML code snippet which is loaded when a user visits a website or opens an email. It is useful for tracking user behavior and conversions. We use tracking pixels to monitor the traffic patterns of our users, to understand if you have clicked on an ad displayed on a third-party website, improve site performance, and to measure the success of email marketing campaigns. In many instances, these technologies are reliant on cookies to function properly, and so declining cookies will impair this functionality. Personal information collected using tracking pixels is stored and analyzed by us to optimize the delivery of our newsletters and emails, and to improve the relevance of the distributed content. Many email clients and web browsers support functionality to opt-out or prevent the use of these tracking mechanisms.\nHow can I control cookies? You have the right to decide whether to accept or reject cookies. You can set or amend your Internet browser controls to accept or refuse cookies. If you choose to reject cookies, you may still use our Websites, though your access to some functionality and areas of our Websites may be restricted. Please visit your browser’s help menu for information on how to opt-out of cookies. Tracking pixels may be refused by configuring the settings on your email client or web browser to prevent web beacons from being activated. Please refer to the documentation for your particular web browser, email client or device for additional information on whether it is supported and how to configure this setting. For additional information regarding our privacy practices and how we protect your personal information, please refer to our Privacy Policy.\nA special note on Do-Not-Track signals Modern web browsers and mobile operating systems include a Do-Not-Track (“DNT”) feature; however, no uniform technology standard for recognizing and implementing DNT signals has been finalized. Therefore, we do not currently commit to responding to browsers’ DNT signals with respect to our Websites. If a universal standard for responding to DNT signals is adopted in the future, we will update this Cookie Policy to reflect such practices.\nHow often will you update this Cookie Policy? We may update this Cookie Policy periodically to reflect changes to the cookies we use or for other operational, legal or regulatory reasons. Please review this Cookie Policy to stay informed about our use of cookies and related technologies and your choices.\nContact Us For any and all privacy-related matters, questions or comments, or to exercise a right under the GDPR, Privacy Shield, or the CCPA, you may contact us in writing or by email. Our contact information is as follow:\nInfinyOn, Inc. 2445 Augustine Drive, Suite 249 Santa Clara, CA. 95054 United States Phone: +1 (925) 290-7018 Email: support@infinyon.com\nEU or Swiss residents with inquiries or complaints regarding this Privacy Policy should first contact InfinyOn at support@infinyon.com. Please allow a reasonable amount of time to respond to your request. If you do not receive timely acknowledgement of your complaint, or if your complaint is not addressed by InfinyOn, you may contact our U.S.-based alternative dispute resolution provider (free of charge) at https://feedback-form.truste.com/watchdog/request.\nIf these processes do not result in a resolution, you may then contact your local data protection authority, the U.S. Department of Commerce, and/or the Federal Trade Commission for assistance. Under certain conditions, more fully described on the Privacy Shield website https://www.privacyshield.gov/article?id=How-to-Submit-a-Complaint, you may invoke binding arbitration when other dispute resolution procedures have been exhausted and upon written notice to InfinyOn at support@infinyon.com.\n","description":"","keywords":null,"summary":"Overview This Cookie Policy (“Policy”) explains how Infinyon, Inc., and subsidiaries (collectively, “Infinyon”, “we”, “us” or “our”) use cookies and similar technologies to recognize you when you visit one of our websites, including any other media form, media channel, mobile website, or mobile application related or connected thereto provided or officially sponsored by Infinyon, or any other Infinyon websites that link to this Policy (collectively, the “Websites”).\nThis Policy explains what these technologies are and why we use them, as well as our users’ (“data subject”, “user”, “you” or “your”) rights to control our use of them.","title":"Cookie Notice","url":"http://localhost:1315/legal/cookies/"},{"body":"","description":"","keywords":null,"summary":"","title":"Deduplication","url":"http://localhost:1315/docs/smartmodules/dedupe/"},{"body":" Data Direction Inbound Outbound ❌ ✅ DuckDb connector is experimental.\nOutbound Connector The DuckDB Sink connector reads records from Fluvio topic, applies configured transformations, and sends new records to the SQL database (via INSERT statements).\nSQL Model to DuckDB types mapping The DuckDB Sink connector expects the data in Fluvio SQL Model in JSON format. In order to work with different data formats or data structures.\nThe following table shows the mapping between SQL Model and DuckDB types:\nModel DuckDB Bool bool Char str SmallInt i16 Int i32 BigInt i64 Float f32 DoublePrecision f64 Text str Bytes [u8] Numeric TODO Timestamp Timestamp Date TODO Time TODO Uuid UUID Json JSON Configuration This connector can be configured using the following properties:\nURL A URL is path to duckdb database path. It can be any expression duckdb supports. For example, to use a local database, it can be my_duckdb_file.\nTo connect to Motherduck server, use prefix: md. For example, md://motherduck_path. Please see MotherDuck documentation for more details.\nExample of opening to local duckdb apiVersion: 0.1.0 meta: version: 0.1.0 name: duckdb-connector type: duckdb-sink topic: fluvio-topic-source create-topic: true duckdb: url: \u0026#39;local.db\u0026#39; # local duckdb Transformations transformations can be applied. The transformation is a SmartModule pulled from the SmartModule Hub. Transformations are chained according to the order in the config. If a SmartModule requires configuration, it is passed via with section of transforms entry.\nSecrets The connector can use secrets in order to hide sensitive information. The example below uses secrets to pass the token to MotherDuck server.\napiVersion: 0.1.0 meta: version: 0.1.0 name: motherduck-connector type: duckdb-sink topic: sql-topic secrets: - name: MD_TOKEN duckdb: url: \u0026#34;md:?token=${{ secrets.MD_TOKEN }}\u0026#34; Usage Example Let\u0026rsquo;s look at the example of the connector with one transformation named infinyon/json-sql. The transformation takes records in JSON format and creates SQL insert operation to topic_message table. The value from device.device_id JSON field will be put to device_id column and the entire json body to record column.\nThe JSON record:\n{ \u0026#34;device\u0026#34;: { \u0026#34;device_id\u0026#34;: 1 } } The SQL database (Postgres):\nCREATE TABLE topic_message (device_id int, record json); Connector configuration file:\n# connector-config.yaml apiVersion: 0.1.0 meta: version: 0.1.0 name: json-sql-connector type: duckdb-sink topic: sql-topic create-topic: true secrets: - name: MD_TOKEN duckdb: url: \u0026#34;md:?token=${{ secrets.MD_TOKEN }}\u0026#34; transforms: - uses: infinyon/json-sql with: mapping: table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true You can use Fluvio fluvio cloud connector tool to deploy the connector:\nand then:\nfluvio cloud connector create --config connector-config.yaml To delete the connector run:\nfluvio cloud connector delete \u0026lt;name of connector\u0026gt; After you run the connector you will see records in your database table.\nSee more in our MQTT to SQL and HTTP to SQL guides.\n","description":"Reference for configuring DuckDB data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ❌ ✅ DuckDb connector is experimental.\nOutbound Connector The DuckDB Sink connector reads records from Fluvio topic, applies configured transformations, and sends new records to the SQL database (via INSERT statements).\nSQL Model to DuckDB types mapping The DuckDB Sink connector expects the data in Fluvio SQL Model in JSON format. In order to work with different data formats or data structures.\nThe following table shows the mapping between SQL Model and DuckDB types:","title":"DuckDB Connector","url":"http://localhost:1315/docs/connectors/duckdb/"},{"body":"","description":"","keywords":null,"summary":"","title":"Github Stars + Forks Changes","url":"http://localhost:1315/docs/smartmodules/stars-forks-changes/"},{"body":" Data Direction Inbound Outbound ❌ ✅ Outbound Connector The Graphite Sink connector reads records from Fluvio topic and sends them to the configured Graphite\u0026rsquo;s Metric using the PlainText approach.\nConfiguration This connectors establishes a TCP Stream against the specified host on Graphite, records are sent as UTF-8 encoded strings following Graphite\u0026rsquo;s PlainText format.\nThe following example connector configuration can be used to send records to the Graphite\u0026rsquo;s Metric weather.temperature.ca.sandiego, the Graphite\u0026rsquo;s TCP server address is specified on the addr field.\n# sample-config.yaml apiVersion: 0.1.0 meta: version: 0.1.2 name: my-graphite-connector-test-connector type: graphite-sink topic: test-graphite-connector-topic graphite: # https://graphite.readthedocs.io/en/latest/feeding-carbon.html#step-1-plan-a-naming-hierarchy metric-path: \u0026#34;weather.temperature.ca.sandiego\u0026#34; addr: \u0026#34;my-graphite-host:2003\u0026#34; Configuration Fields Model Data Type Description metric-path String Graphite Metric to send records to addr String Graphite TCP Address to stream out Usage This section will walk you through the process of setting up a Graphite instance and using InfinyOn Cloud to send metrics to this Graphite instance.\nThis section assumes you have Docker and Fluvio installed on your host, and it is reachable by InfinyOn Cloud.\nSetting Up Graphite We will run our Graphite instance on Docker using the docker compose command for simplicity.\nThe Graphite container will setup Carbon Configuration files in your working directory, we need to update these files to reduce Carbon\u0026rsquo;s persistance intervals, making it more frequent.\nCreate a copy of our docker-compose.yml file and execute the container:\nversion: \u0026#39;3\u0026#39; services: graphite: image: graphiteapp/graphite-statsd:1.1.10-4 ports: - \u0026#39;12345:80\u0026#39; - \u0026#39;2003-2004:2003-2004\u0026#39; - \u0026#39;2023-2024:2023-2024\u0026#39; - \u0026#39;8125:8125/udp\u0026#39; - \u0026#39;8126:8126\u0026#39; volumes: - ./.graphite/conf:/opt/graphite/conf environment: # Enable full debug page display on exceptions (Internal Server Error pages) GRAPHITE_DEBUG: True``` ```bash docker compose up --build -d This will generate a directory with the name .graphite, which contains configuration files.\nReplace the contents of .graphite/conf/storage-schemas.conf to record on an interval of 10 seconds and persist the last 12 hours of data.\n[all] pattern = .* retentions = 10s:12h Now we need to re run the Graphite container so Carbon uses the new configuration.\ndocker compose down docker compose up --build -d You can visit http://your-graphite-host:12345 in your browser to access the Dashboard.\nCredentials for the Dashboard are User: root and Password: root\nWith the Graphite instance set, we can move into Setting Up Fluvio with Graphite Sink Connector.\nSetting Up Fluvio with Graphite Sink Connector In this section we are going use the Fluvio Cloud CLI to spin up the Graphite Sink Connector to send metrics from Fluvio Records to the Graphite instance.\nIf you don\u0026rsquo;t have the Fluvio CLI installed already visit the CLI section\nCreate a YAML file with the name weather-monitor-config.yaml and specify connector settings:\napiVersion: 0.1.0 meta: version: 0.1.2 name: weather-monitor-sandiego type: graphite-sink topic: weather-ca-sandiego graphite: # https://graphite.readthedocs.io/en/latest/feeding-carbon.html#step-1-plan-a-naming-hierarchy metric-path: \u0026#34;weather.temperature.ca.sandiego\u0026#34; addr: \u0026#34;your-graphite-host:2003\u0026#34; Deploy the Connector using the Fluvio Cloud CLI\nfluvio cloud connector create --config weather-monitor-config.yaml Make sure your Graphite instance is running on my-graphite-host:2003, use the fluvio cloud connector log weather-ca-sandiego subcommand to read logs from the connector instance.\nThen produce records as usual:\necho 120 | fluvio produce weather-ca-sandiego Remember that Carbon\u0026rsquo;s retention is set to 10s:12h, this means that if will write metrics every 10s.\nUse Graphite\u0026rsquo;s REST API to check on the stored data.\ncurl -o ./data.json http://localhost:12345/render\\?target\\=weather.temperature.ca.sandiego\\\u0026amp;format\\=json\\\u0026amp;noNullPoints ","description":"Reference for configuring Graphite data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ❌ ✅ Outbound Connector The Graphite Sink connector reads records from Fluvio topic and sends them to the configured Graphite\u0026rsquo;s Metric using the PlainText approach.\nConfiguration This connectors establishes a TCP Stream against the specified host on Graphite, records are sent as UTF-8 encoded strings following Graphite\u0026rsquo;s PlainText format.\nThe following example connector configuration can be used to send records to the Graphite\u0026rsquo;s Metric weather.temperature.ca.sandiego, the Graphite\u0026rsquo;s TCP server address is specified on the addr field.","title":"Graphite Connector","url":"http://localhost:1315/docs/connectors/graphite/"},{"body":" Data Direction Inbound Outbound ✅ ✅ Inbound Connector Read HTTP Responses given input HTTP request configuration options and interval x and produces them to Fluvio topics.\nSupports HTTP/1.0, HTTP/1.1, HTTP/2.0 protocols.\nSee docs here. Tutorial for HTTP to SQL Pipeline.\nConfiguration Option default type description interval 10s String Interval between each HTTP Request. This is in the form of \u0026ldquo;1s\u0026rdquo;, \u0026ldquo;10ms\u0026rdquo;, \u0026ldquo;1m\u0026rdquo;, \u0026ldquo;1ns\u0026rdquo;, etc. method GET String GET, POST, PUT, HEAD endpoint - String HTTP URL endpoint headers - Array\u0026lt;String\u0026gt; Request header(s) \u0026ldquo;Key:Value\u0026rdquo; pairs body - String Request body e.g. in POST user-agent \u0026ldquo;fluvio/http-source 0.1.0\u0026rdquo; String Request user-agent output_type text String text = UTF-8 String Output, json = UTF-8 JSON Serialized String output_parts body String body = body only, full = all status, header and body parts Record Type Output Matrix Output output_type = text (default), output_parts = body (default) Only the body of the HTTP Response output_type = text (default), output_parts = full The full HTTP Response output_type = json, output_parts = body (default) Only the \u0026ldquo;body\u0026rdquo; in JSON struct output_type = json, output_parts = full HTTP \u0026ldquo;status\u0026rdquo;, \u0026ldquo;body\u0026rdquo; and \u0026ldquo;header\u0026rdquo; JSON Usage Example This is an example of simple connector config file:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts create-topic: true secrets: - name: AUTHORIZATION_TOKEN http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s headers: - \u0026#34;Authorization: token ${{ secrets.AUTHORIZATION_TOKEN }}\u0026#34; - \u0026#34;Cache-Control: no-cache\u0026#34; The produced record in Fluvio topic will be:\n{ \u0026#34;fact\u0026#34;: \u0026#34;The biggest wildcat today is the Siberian Tiger. It can be more than 12 feet (3.6 m) long (about the size of a small car) and weigh up to 700 pounds (317 kg).\u0026#34;, \u0026#34;length\u0026#34;: 158 } Secrets Fluvio HTTP Source Connector supports Secrets in the endpoint and in the headers parameters:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts create-topic: true secrets: - name: MY_SECRET_URL - name: MY_AUTHORIZATION_HEADER http: endpoint: secret: name: MY_SECRET_URL headers: - \u0026#34;Authorization: ${{ secrets.MY_AUTHORIZATION_HEADER }} interval: 10s Transformations Fluvio HTTP Source Connector supports Transformations. Records can be modified before sending to Fluvio topic.\nThe previous example can be extended to add extra transformations to outgoing records:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: cat-facts type: http-source topic: cat-facts create-topic: true http: endpoint: \u0026#34;https://catfact.ninja/fact\u0026#34; interval: 10s transforms: - uses: infinyon/jolt@0.1.0 with: spec: - operation: default spec: source: \u0026#34;http-connector\u0026#34; - operation: remove spec: length: \u0026#34;\u0026#34; In this case, additional transformation will be performed before records are sent to Fluvio topic: field length will be removed and field source with string value http-connector will be added.\nNow produced records will have a different shape, for example:\n{ \u0026#34;fact\u0026#34;: \u0026#34;A cat has more bones than a human; humans have 206, and the cat - 230.\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;http-connector\u0026#34; } Read more about JSON to JSON transformations.\nOutbound Connector HTTP sink connector reads records from data streaming and generates an HTTP request.\nSupports HTTP/1.0, HTTP/1.1, HTTP/2.0 protocols.\nConfiguration HTTP Sink is configured using a YAML file:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: my-http-sink type: http-sink topic: http-sink-topic secrets: - name: HTTP_TOKEN http: endpoint: \u0026#34;http://my.svc.tld/post\u0026#34; headers: - \u0026#34;Authorization: token ${{ secrets.HTTP_TOKEN }}\u0026#34; - \u0026#34;Cache-Control: no-cache\u0026#34; Option default type description method POST String POST, PUT endpoint - String HTTP URL endpoint headers - Array\u0026lt;String\u0026gt; Request header(s) \u0026ldquo;Key:Value\u0026rdquo; pairs user-agent fluvio/http-sink 0.2.2 String Request user-agent http_request_timeout 1s String HTTP Request Timeout http_connect_timeout 15s String HTTP Connect Timeout By default HTTP headers will use Content-Type: text/html unless another value is provided to the Headers configuration.\nUsage Login to your Fluvio Cloud Account via Fluvio CLI\nfluvio cloud login --use-oauth2 Then configure the HTTP Request to be sent using a YAML file following the connector configuration schema. The following configuration will send a POST HTTP request to http://httpbin.org/post.\n# config.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: httpbin type: http-sink topic: httpbin-send-post http: endpoint: http://httpbin.org/post interval: 3s Finally create your connector by running:\nfluvio cloud connector create --config ./config.yaml You can see active connectors by running the following command:\nfluvio cloud connector list Check connector logs by running\nfluvio cloud connector logs httpbin INFO connect:connect_with_config:connect: fluvio_socket::versioned: connect to socket add=fluvio-sc-public:9003 INFO dispatcher_loop{self=MultiplexDisp(10)}: fluvio_socket::multiplexing: multiplexer terminated 2023-05-02T20:59:50.192104Z INFO stream_with_config:inner_stream_batches_with_config:request_stream{offset=Offset { inner: FromEnd(0) }}:create_serial_socket:create_serial_socket_from_leader{leader_id=0}:connect_to_leader{leader=0}:connect: fluvio_socket::versioned: connect to socket add=fluvio-spu-main-0.acct-584fd564-1d4a-4308-9061-09acea387bea.svc.cluster.local:9005 INFO fluvio_connector_common::monitoring: using metric path: /fluvio_metrics/connector.sock INFO fluvio_connector_common::monitoring: monitoring started Produce Records to send as HTTP POST Requests You can produce records using fluvio produce \u0026lt;TOPIC\u0026gt;, values produced will be sent as HTTP Body payloads on HTTP Sink Connector.\nRunning the following command will attach stdin to the topic stream, any data written to stdin will be sent as a record through the httpbin-send-post topic, and as a side effect of the HTTP Sink Connector, these records will also be sent as HTTP POST requests to http://httpbin.org/post, based on our configuration.\nfluvio produce httpbin-send-post Then send data:\n\u0026gt; {\\\u0026#34;hello\\\u0026#34;: \\\u0026#34;world\\\u0026#34;} Ok! Teardown To stop your connector just use fluvio cloud connector delete \u0026lt;NAME\u0026gt;\nfluvio cloud connector delete httpbin httpbin is our connector instance name from the configuration file shown above\nTransformations Fluvio HTTP Sink Connector supports Transformations. Records can be modified before sending to endpoint.\nThe previous example can be extended to add extra transformations to outgoing records:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: my-http-sink type: http-sink topic: http-sink-topic secrets: - name: AUTHORIZATION_TOKEN http: endpoint: \u0026#34;http://my.svc.tld/post\u0026#34; headers: - \u0026#34;Authorization: token ${{ secrets.AUTHORIZATION_TOKEN }}\u0026#34; - \u0026#34;Content-Type: application/json\u0026#34; transforms: - uses: infinyon/jolt@0.1.0 with: spec: - operation: shift spec: \u0026#34;result\u0026#34;: \u0026#34;text\u0026#34; In this case, additional transformation will be performed before records are sent the http endpoint. A json field called result will be renamed to text.\nRead more about JSON to JSON transformations.\n","description":"Reference for configuring HTTP data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ✅ ✅ Inbound Connector Read HTTP Responses given input HTTP request configuration options and interval x and produces them to Fluvio topics.\nSupports HTTP/1.0, HTTP/1.1, HTTP/2.0 protocols.\nSee docs here. Tutorial for HTTP to SQL Pipeline.\nConfiguration Option default type description interval 10s String Interval between each HTTP Request. This is in the form of \u0026ldquo;1s\u0026rdquo;, \u0026ldquo;10ms\u0026rdquo;, \u0026ldquo;1m\u0026rdquo;, \u0026ldquo;1ns\u0026rdquo;, etc. method GET String GET, POST, PUT, HEAD endpoint - String HTTP URL endpoint headers - Array\u0026lt;String\u0026gt; Request header(s) \u0026ldquo;Key:Value\u0026rdquo; pairs body - String Request body e.","title":"HTTP Connector","url":"http://localhost:1315/docs/connectors/http/"},{"body":"","description":"","keywords":null,"summary":"","title":"InfinyOn | Book a Call","url":"http://localhost:1315/request/book-a-call/"},{"body":"","description":"","keywords":null,"summary":"","title":"InfinyOn | Early Access Request","url":"http://localhost:1315/request/ss-early-access/"},{"body":"","description":"","keywords":null,"summary":"","title":"InfinyOn | Subscription","url":"http://localhost:1315/ok/subscribe/"},{"body":"The Hub service, is a where users can download SmartModules and Connectors and integrate them into their data pipelines. To use this service you need an InfinyOn Cloud account.\nEach SmartModule and Connector in the Hub are uniquely identified by a group, name, and version.\nFor example, the following SmartModules is published by InfinyOn:\ninfinyon/sql-json@0.1.0 Private/Public SmartModules and Connectors SmartModules published to the Hub can be public or private. Public SmartModules are visible and downloadable by anyone, whereas private SmartModules are only visible to the owner.\n-\u0026gt; This feature is under development; please reach out on Discord to request early access.\nCertified SmartModules and Connectors Certified SmartModules are publicly available to anyone with an Infinyon Cloud account.\nCurrently only InfinyOn SmartModules are certified. Please reach out on Discord if you are interested in the SmartModule Certification program.\n","description":"A description of the features of InfinyOn Hub","keywords":null,"summary":"The Hub service, is a where users can download SmartModules and Connectors and integrate them into their data pipelines. To use this service you need an InfinyOn Cloud account.\nEach SmartModule and Connector in the Hub are uniquely identified by a group, name, and version.\nFor example, the following SmartModules is published by InfinyOn:\ninfinyon/sql-json@0.1.0 Private/Public SmartModules and Connectors SmartModules published to the Hub can be public or private. Public SmartModules are visible and downloadable by anyone, whereas private SmartModules are only visible to the owner.","title":"InfinyOn Hub Features","url":"http://localhost:1315/docs/resources/infinyon-hub-features/"},{"body":"","description":"","keywords":null,"summary":"","title":"Json Formatter","url":"http://localhost:1315/docs/smartmodules/json-formatter/"},{"body":" Data Direction Inbound Outbound ✅ ✅ Inbound Connector This is a connector for taking data from a Kafka topic and sending to a Fluvio topic.\nConfiguration Opt default type description url - String The url for the kafka connector topic - String The kafka topic partition 0 Integer The kafka partition group fluvio-kafka-source String The kafka consumer group Example:\napiVersion: 0.1.0 meta: version: 0.2.5 name: my-kafka-connector type: kafka-source topic: kafka-topic create-topic: true kafka: url: \u0026#34;my.kafka.host:9092\u0026#34; topic: fluvio-topic Transformations Fluvio Kafka Connectors support Transformations.\nOutbound Connector This is a connector for taking data from a Fluvio topic and sending to a Kafka topic.\nConfiguration Opt default type description url - String The url for the kafka connector topic - String The kafka topic partition 0 Integer The kafka partition create-topic false Boolean Create or not a topic before start options - Mapping The kafka client options security - Mapping Optional. The kafka security config Security configuration Option default type description security_protocol ssl String The kafka security protocol ssl_key - Mapping The SSL key file to use ssl_cert - Mapping The SSL cert file to use ssl_ca - Mapping The SSL ca file to use Parameters ssl_key, ssl_cert and ssl_ca can be defined via file - path to the file, or pem - content as string value.\nExample without security:\napiVersion: 0.1.0 meta: version: 0.2.7 name: my-kafka-connector type: kafka-sink topic: kafka-topic create-topic: true kafka: url: \u0026#34;my.kafka.host:9092\u0026#34; topic: fluvio-topic create-topic: true Example with security enabled:\napiVersion: 0.1.0 meta: version: 0.2.7 name: my-kafka-connector type: kafka-sink topic: kafka-topic create-topic: true secrets: - name: KAFKA_BROKER_URL - name: SSL_CERT_PEM kafka: url: ${{ secrets.KAFKA_BROKER_URL }} topic: fluvio-topic create-topic: true security: ssl_key: file: /path/to/file ssl_cert: pem: \u0026#34;${{ secrets.SSL_CERT_PEM }}\u0026#34; ssl_ca: file: /path/to/file security_protocol: ssl Testing with security Instructions of how to deploy local kafka cluster with SSL using docker. After all steps done, in the secrets folder there will be fluvio.key.pem, fluvio.pem and fake-ca-1.crt files that can be used in the connector config as ssl_key, ssl_cert and ssl_ca correspondingly.\nTransformations Fluvio Kafka Connectors support Transformations.\n","description":"Reference for configuring Kafka data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ✅ ✅ Inbound Connector This is a connector for taking data from a Kafka topic and sending to a Fluvio topic.\nConfiguration Opt default type description url - String The url for the kafka connector topic - String The kafka topic partition 0 Integer The kafka partition group fluvio-kafka-source String The kafka consumer group Example:\napiVersion: 0.1.0 meta: version: 0.2.5 name: my-kafka-connector type: kafka-source topic: kafka-topic create-topic: true kafka: url: \u0026#34;my.","title":"Kafka Connector","url":"http://localhost:1315/docs/connectors/kafka/"},{"body":" Data Direction Inbound Outbound ✅ ❌ Inbound Connector Reads record from MQTT topic and writes to Fluvio topic.\nSupports MQTT V3.1.1 and V5 protocols.\nGuide for MQTT to SQL Pipeline.\nConfiguration Option default type description timeout 60s Duration mqtt broker connect timeout in seconds and nanoseconds url - SecretString MQTT url which includes schema, domain, port and credentials such as username and password. topic - String mqtt topic to subscribe and source events from client_id UUID V4 String mqtt client ID. Using same client id in different connectors may close connection payload_output_type binary String controls how the output of payload field is produced url option with type SecretString can be set as raw string value:\nurl: \u0026#34;mqtt://test.mosquitto.org/\u0026#34; or, as a reference to a secret with the given name:\nurl: secret: name: \u0026#34;URL_SECRET_NAME\u0026#34; Record Type Output JSON Serialized string with fields mqtt_topic and payload\nPayload Output Type Value Output binary Array of bytes json UTF-8 JSON Serialized String Usage Example This is an example of connector config file:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: my-mqtt-connector type: mqtt-source topic: mqtt-topic create-topic: true mqtt: url: \u0026#34;mqtt://test.mosquitto.org/\u0026#34; topic: \u0026#34;mqtt-to-fluvio\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json Run connector in Cloud with fluvio cloud CLI\nfluvio cloud connector create --config config-example.yaml fluvio cloud connector list # to see the status fluvio cloud connector logs my-mqtt-connector # to see connector\u0026#39;s logs Install MQTT Client such as\n# for mac , this takes while.... brew install mosquitto Insert records:\nmosquitto_pub -h test.mosquitto.org -t mqtt-to-fluvio -m \u0026#39;{\u0026#34;device\u0026#34;: {\u0026#34;device_id\u0026#34;:1, \u0026#34;name\u0026#34;:\u0026#34;device1\u0026#34;}}\u0026#39; The produced record in Fluvio topic will be:\n{ \u0026#34;mqtt_topic\u0026#34;: \u0026#34;mqtt-to-fluvio\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;device\u0026#34;: { \u0026#34;device_id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;device1\u0026#34; } } } Transformations Fluvio MQTT Source Connector supports Transformations. Records can be modified before sending to Fluvio topic.\nThe previous example can be extended to add extra transformations to outgoing records:\n# config-example.yaml apiVersion: 0.1.0 meta: version: 0.2.5 name: my-mqtt-connector type: mqtt-source topic: mqtt-topic create-topic: true mqtt: url: \u0026#34;mqtt://test.mosquitto.org/\u0026#34; topic: \u0026#34;mqtt-to-fluvio\u0026#34; timeout: secs: 30 nanos: 0 payload_output_type: json transforms: - uses: infinyon/jolt@0.1.0 with: spec: - operation: shift spec: payload: device: \u0026#34;device\u0026#34; - operation: default spec: source: \u0026#34;mqtt-connector\u0026#34; The object device in the resulting record will be \u0026ldquo;unwrapped\u0026rdquo; and the addition field source with value mqtt-connector will be added.\nRead more about JSON to JSON transformations.\n","description":"Reference for configuring MQTT data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ✅ ❌ Inbound Connector Reads record from MQTT topic and writes to Fluvio topic.\nSupports MQTT V3.1.1 and V5 protocols.\nGuide for MQTT to SQL Pipeline.\nConfiguration Option default type description timeout 60s Duration mqtt broker connect timeout in seconds and nanoseconds url - SecretString MQTT url which includes schema, domain, port and credentials such as username and password. topic - String mqtt topic to subscribe and source events from client_id UUID V4 String mqtt client ID.","title":"MQTT Connector","url":"http://localhost:1315/docs/connectors/mqtt/"},{"body":"Overview This Privacy Policy (“Privacy Policy”, “Policy”) explains how InfinyOn, Inc. (\u0026ldquo;InfinyOn\u0026rdquo;, “we”, “us” or “our”) collects, uses, shares, processes and protects personal information (“Personal Information”) relating to individuals (“you”, or “your”), who may use our websites or services, or when you otherwise interact with us. “You” may be a visitor to one of our websites, a user (“User”) of one or more of our services, a partner, a collaborator, or a customer (“Customer”).\nThis Privacy Policy also describes your choices regarding use, access and correction of personal information collected about you through our Services. Please read this Privacy Policy carefully and ensure that you understand it before you start to use our Services.\nBy accessing and using the Services, you acknowledge that you have read and understood the content of this Privacy Policy. We reserve the right to update this Privacy Policy from time to time. If we make changes, we will notify you by revising the date at the top of the Privacy Policy and, in some cases, we may send you a notification. We encourage you to review the Privacy Policy whenever you access the Services or otherwise interact with us to stay informed about our information practices and the ways you can help protect your privacy.\nInfinyOn respects your privacy and is committed to protecting your Personal Information (any information that relates to an identified or identifiable individual). It is our belief that any Personal Information provided to us by you is personal and private and we\u0026rsquo;ll treat it accordingly. We do not sell or trade your Personal Information.\nCoverage This Policy applies to all visitors of our websites, and users of our products, features or services, or any other InfinyOn websites that link to this Policy (collectively, the “Websites”), unless covered by a separate privacy policy, and explains how we collect, use, disclose, and safeguard your information. The Privacy Policy does not cover situations where we offer cloud products and services, through which our customers (and/or their affiliates) connect their own applications to our hosted platform, sell or offer their own products and services, send electronic communications to other individuals, or otherwise collect, use, share or process Personal Information via our cloud products and services.\nData Collections And Uses This Policy describes how we collect and use your Personal Information, whether it is shared and/or disclosed, and how we address privacy matters, such as deletion of your Personal Information upon request, and opting-out of marketing communications. Lastly, we describe methods for contacting us if you have privacy questions, comments or feedback.\nInformation You Provide To Us We collect and process information you provide directly to us via the Services. Personal data submitted through the Services include the details you submit when you create an account, participate in any interactive features of the Services, fill out a form, pay for subscriptions, communicate with us via third party social media sites, request customer support or otherwise communicate with us. The types of information we may collect include your name, email address, company name, postal address, phone number and any other information you choose to provide. To the extent you provide credit card information through the Services, that information is collected and processed by our third-party payment processor pursuant to their Privacy Policy and practices.\nInformation We Collect From Other Sources We may obtain information from other sources and combine that with information we collect through our Services for purposes of advertising and user authentication. For example, if you create or log into your InfinyOn account using your Google Apps credentials via single sign-on, we will have access to certain information such as your name and email address as authorized in your Google Apps profile settings.\nInformation Based On Your Relationship With Us The type of information we collect from you and why, based upon your relationship with us and as your relationship evolves with InfinyOn. We categorize our users as follows:\nVisitors Individuals who visit our public websites, without logging into an account or using our products and/or services.\nInformation we collect:\nIP Address InfinyOn Cookie Information Browser Information Purpose of this information is to:\nGauge interest in our products Assess the effectiveness of marketing engagement model Monitor usage patterns to improve our products and services As a Visitor, you are not obligated to provide us with such Personal Information, and you are free to change or completely remove information shared with us; however refusing to provide requested Personal Information might prevent you from using various features of the websites.\nKnown Visitors Individuals who voluntarily shared their information with us to receive information about our products, services, or marketing material.\nInformation we collect:\nName Email Company Name (optional) Job Title (optional) Phone number (optional) Purpose of this information is to:\nShare information about our products and services Share marketing material As a Known Visitor, you may ask us to remove information shared with us; however refusing to provide requested Personal Information might prevent you from using various features of the websites.\nUsers Individuals who establish an account with us, through the use of our websites, products or services.\nInformation we collect:\nName Email Company Name (optional) Job Title (optional) Phone Number (optional) Purpose of this information is to:\nIdentity verification Assess the usage of our product Gauge interest in products, services, or marketing material Users are also considered Visitors, we collect this data in addition to what was described in the Visitors section.\nCollaborators Individuals who contribute to our public repositories.\nInformation we collect:\nName Email Username Purpose of this information is to:\nidentify who contributes to our repositories track changes to our repositories Cookies We use cookies to recognize you during return visits and deliver a consistent experience. Most modern browsers allow you to delete or block cookies all together. For additional information on our use of cookies and similar technologies, please refer to our Cookie Notice.\nHow We Use Personal Information? We use your Personal Information to facilitate the business relationships we have with you, to provide the products you use, operate our websites, meet our contractual and legal obligations, protect the security of our systems and our customers, or fulfil other legitimate interests as described in this Privacy Policy and in our notices to you.\nFor example, we may use your Personal Information to:\nOperate, maintain and improve our internal operations, products, and websites. Understand your preferences to enhance your experience while using our products and websites Provide recommendations, solicit feedback, and better market relevant content to you Analyze the interactions with our products and websites to identify trends, usage, and activity patterns Respond to your comments or questions, provide technical support and customer service. Comply with applicable laws, rules, or regulations and cooperate and defend legal claims and audits Communicate with you about promotions, upcoming events, and other news about products and services Protect the products and websites, and investigate and deter against fraudulent, unauthorized, or illegal activity InfinyOn will only share and disclose Personal Information in accordance to applicable laws and legal process. We will not sell your Personal Information for any purpose.\nWe Keep Your Information Safe InfinyOn cares about the security of your information and takes reasonable and appropriate technical and organizational measures designed to prevent loss, misuse, and unauthorized access, disclosure, alteration, and destruction of personal information. However, no security system is impenetrable, and we cannot guarantee the security of our systems or your information.\nHow Long Do We Keep Your Data? We only process and keep any Personal Information for as long as necessary. to achieve the purpose for which the information was originally collected. The exact length of time we keep Personal Information depends on our processing purposes and the statutory retention period for that type of information. After the statutory period of time passes, or if storage of Personal Information is not needed, Personal Information is deleted or anonymized.\nPolicy For Children We do not knowingly solicit information from or market to children under the age of thirteen (13). If you are under age 13, please do not give us any Personal Information. We encourage parents and legal guardians to monitor their children’s Internet usage and to help us enforce our Privacy Policy by instructing them to never share Personal Information through our websites without their permission. If you suspect or become aware of any data we have collected from children under age 13, please contact us immediately using the contact information provided below.\nNotice To All Non-U.S. Residents Our servers are located in the U.S.. If you are located outside of the U.S., please be aware that any information provided to us, including Personal Information, will be transferred from your country of origin to the U.S.. InfinyOn transfers and processes data, including the data transfers under the EU-U.S. Privacy Shield, the Swiss-U.S. Privacy Shield, and the General Data Protection Regulation (GDPR), in accordance with applicable laws and regulations.\nNotice For Residents Of The European And Swiss Economic Areas If you are located in the European and Swiss Economic Area, InfinyOn, Inc., is the data controller of your information. Our legal basis for collecting and using the personal information above will depend on the personal information concerned and the specific context in which we collect it. However, we will normally collect personal information only where we have your consent to do so, where we need the personal information to perform a contract with you, or where the processing is in our legitimate interests and not overridden by your data protection interests or fundamental rights and freedoms. In some cases, we may also have a legal obligation to collect personal information from you.\nIf we ask you to provide personal information to comply with a legal requirement or to perform a contract with you, we will indicate this at the relevant time and advise you whether the provision of your personal information is mandatory or not (as well as the possible consequences, if any, if you do not provide your personal information). Similarly, if we collect and use your personal information in reliance on our legitimate interests (or those of a third party), we will indicate to you at the relevant time what those legitimate interests are.\nNotice For California Residents The California Consumer Privacy Act (CCPA) is a new data privacy law that applies to certain businesses which collect Personal Information from California residents. The law became effective on January 1, 2020. Your rights under the CCPA are described below.\nPlease note that InfinyOn does not rent or sell any Personal Information.\nIn addition, California Civil Code Section 1798.83, also known as the “Shine The Light” law, permits California residents to request and obtain from us, once a year and free of charge, 1) information about categories of Personal Information (if any) we disclosed to third parties for direct marketing purposes, and, 2) the names and addresses of the third parties with which we shared Personal Information in the preceding calendar year.\nIf you are under 18 years of age, reside in California, and have a registered account with our websites, you have the right to request removal of unwanted data that you publicly post on our websites. To request removal of such data, please contact us using the contact information provided below, and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on our websites, but please be aware that the data may not be completely or comprehensively removed from our systems.\nIf you are a California resident and would like to make a request, please submit your request in writing to us using the contact information provided below.\nYour Rights We recognize, under the EU-U.S. Privacy Shield, the Swiss-U.S. Privacy Shield, CCPA, and GDPR, that you have certain rights in regards to your Personal Information. We feel that your privacy and ability to preserve and exercise your rights is very important. You are encouraged to review and understand these rights as they pertain to you and your Personal Information. In certain circumstances, these rights include, but are not limited to:\nRight to be Informed: This means we have to tell you why we process your Personal Information, our retention periods, and who it will be shared with.\nRight of Access: This means we have to provide you with a copy of your Personal Information we process upon your request.\nRight to Rectification: This allows you to have inaccurate Personal Information rectified, or completed if it is incomplete.\nRight to Erasure: This allows you to have your Personal Information erased.\nRight to Restrict Processing: This means you can limit the way we use their data.\nRight to Data Portability: This allows you to receive a copy of your Personal Information in a structured, commonly used and machine-readable format and gives you the right to transmit those data to another controller without hindrance.\nRight to Object: This allows you to object to the processing of your Personal Information at any time.\nRight to Non-Discrimination: The CCPA prohibits covered businesses from discriminating against consumers for exercising their CCPA rights. This means we cannot charge a different price, deny access to our products, or impose penalties for exercising your rights under the CCPA.\nRight to Withdraw Consent: This means you can withdraw your consent at any time.\nIn support of these rights, you may exercise any of the above rights, with respect to your Personal Information. You may update, correct or delete your Personal Information; if you wish to delete or suspend your account, please note that we may retain certain information as required by law or for legitimate business purposes. If you have become aware that an account has been created about you without your knowledge or consent, you may contact us to request deletion of that said account. You may contact us by emailing support@infinyon.com\nFor your protection, we may only respond with the Personal Information associated with the particular email address that you use to send us your request, and we may need to verify your identity before implementing your request. We will respond to your request within 30 days.\nContact Us For any and all privacy-related matters, questions or comments, or to exercise a right under the GDPR, Privacy Shield, or the CCPA, you may contact us in writing or by email. Our contact information is as follow:\nInfinyOn, Inc. 2445 Augustine Drive, Suite 249 Santa Clara, CA. 95054 United States Phone: +1 (925) 290-7018 Email: support@infinyon.com\nEU or Swiss residents with inquiries or complaints regarding this Privacy Policy should first contact InfinyOn at support@infinyon.com. Please allow a reasonable amount of time to respond to your request. If you do not receive timely acknowledgement of your complaint, or if your complaint is not addressed by InfinyOn, you may contact our U.S.-based alternative dispute resolution provider (free of charge) at https://feedback-form.truste.com/watchdog/request.\nIf these processes do not result in a resolution, you may then contact your local data protection authority, the U.S. Department of Commerce, and/or the Federal Trade Commission for assistance. Under certain conditions, more fully described on the Privacy Shield website https://www.privacyshield.gov/article?id=How-to-Submit-a-Complaint, you may invoke binding arbitration when other dispute resolution procedures have been exhausted and upon written notice to InfinyOn at support@infinyon.com.\n","description":"","keywords":null,"summary":"Overview This Privacy Policy (“Privacy Policy”, “Policy”) explains how InfinyOn, Inc. (\u0026ldquo;InfinyOn\u0026rdquo;, “we”, “us” or “our”) collects, uses, shares, processes and protects personal information (“Personal Information”) relating to individuals (“you”, or “your”), who may use our websites or services, or when you otherwise interact with us. “You” may be a visitor to one of our websites, a user (“User”) of one or more of our services, a partner, a collaborator, or a customer (“Customer”).","title":"Privacy Policy","url":"http://localhost:1315/legal/privacy/"},{"body":"Fluvio\u0026rsquo;s Deduplication feature allows for the removal of duplicate records based on their keys, streamlining your data processing.\nTo utilize deduplication, enable it on a desired topic. Duplicates are identified and dropped within a specified window, governed by the bounds configuration.\nThe available bounds options are age and count, elaborated in the bounds section.\nBehavior The deduplication process is deterministic and maintains its state across restarts. Upon a restart, the deduplication algorithm traverses the data stream, reconstructing the memory object accordingly.\nExample topic config Example configuration on topic:\n%copy%\n# topic.yaml version: 0.1.0 meta: name: topic-with-dedup deduplication: bounds: count: 5 # remember at least 5 last records age: 5s # remember records for at least 5 seconds filter: transform: uses: infinyon-labs/dedup-filter@0.0.2 A topic can be created using this config file like so:\n%copy first-line%\n$ fluvio topic create -c topic.yaml After creating the topic, it can be tested like so:\n%copy first-line%\n$ fluvio produce topic-with-dedup --key-separator : 1:2 1:2 2:5 %copy first-line%\n$ fluvio consume -B topic-with-dedup 2 5 Bounds Parameter default type optional description count - Integer false Base number of records the filter keeps in mind. It doesn\u0026rsquo;t ensure remembering records from count records ago, but it sets a starting point. age - Integer true The basic time length the filter holds onto a record. You can set it like this: 15days 2min 2s, or 2min 5s, or 15ms to specify the duration. Implementation The deduplication task is managed by a SmartModule, and as of now, the dedup-filter is the designated SmartModule for this task.\nThe dedup-filter takes the data and divides it into smaller chunks, holding these chunks in memory. Each chunk is tagged with an age, indicating how old it is.\nThere\u0026rsquo;s a limit to the total number of records the memory can hold, set by bounds.count. When this count is reached, dedup-filter looks at the oldest chunk, checks its age against the bounds.age setting, and if it\u0026rsquo;s old enough, it\u0026rsquo;s removed. This setup allows for quick removal of old data with a minimal amount of tracking.\nThe approach of breaking down data into chunks does use a bit more memory, but it ensures that the filter operates smoothly, without any sudden increases in the time or memory needed.\n","description":"","keywords":null,"summary":"Fluvio\u0026rsquo;s Deduplication feature allows for the removal of duplicate records based on their keys, streamlining your data processing.\nTo utilize deduplication, enable it on a desired topic. Duplicates are identified and dropped within a specified window, governed by the bounds configuration.\nThe available bounds options are age and count, elaborated in the bounds section.\nBehavior The deduplication process is deterministic and maintains its state across restarts. Upon a restart, the deduplication algorithm traverses the data stream, reconstructing the memory object accordingly.","title":"Record Deduplication","url":"http://localhost:1315/docs/cli/deduplication/"},{"body":"","description":"","keywords":null,"summary":"","title":"RSS to Json","url":"http://localhost:1315/docs/smartmodules/rss-json/"},{"body":"Overview InfinyOn implements security controls designed to protect and secure customer data. Infinyon Cloud uses multiple security controls, including access control, strong authentication, logging and monitoring, and many others.\nInfinyOn is committed to working with the user community and security experts to ensure our products are secure. We appreciate and are fully committed to investigating any vulnerability reported to us in the shortest amount of time.\nPrivacy InfinyOn is committed to being transparent about the data we handle and how we handle it. The Privacy Policy page discloses what information we collect and how we use it.\nReport a Vulnerability Please report security vulnerabilities at support@infinyon.com. InfinyOn is making every possible effort to address the issues in the shortest amount of time possible.\n","description":"","keywords":null,"summary":"Overview InfinyOn implements security controls designed to protect and secure customer data. Infinyon Cloud uses multiple security controls, including access control, strong authentication, logging and monitoring, and many others.\nInfinyOn is committed to working with the user community and security experts to ensure our products are secure. We appreciate and are fully committed to investigating any vulnerability reported to us in the shortest amount of time.\nPrivacy InfinyOn is committed to being transparent about the data we handle and how we handle it.","title":"Security","url":"http://localhost:1315/legal/security/"},{"body":" Data Direction Inbound Outbound ❌ ✅ Outbound Connector The SQL Sink connector reads records from Fluvio topic, applies configured transformations, and sends new records to the SQL database (via INSERT statements).\nSupported databases PostgreSQL SQLite Data types Model PostgreSQL SQLite Bool BOOL BOOLEAN Char CHAR INTEGER SmallInt SMALLINT, SMALLSERIAL, INT2 INTEGER Int INT, SERIAL, INT4 INTEGER BigInt BIGINT, BIGSERIAL, INT8 BIGINT, INT8 Float REAL, FLOAT4 REAL DoublePrecision DOUBLE PRECISION, FLOAT8 REAL Text VARCHAR, CHAR(N), TEXT, NAME TEXT Bytes BYTEA BLOB Numeric NUMERIC REAL Timestamp TIMESTAMP DATETIME Date DATE DATE Time TIME TIME Uuid UUID BLOB, TEXT Json JSON, JSONB TEXT Transformations The SQL Sink connector expects the data in Fluvio SQL Model in JSON format. In order to work with different data formats or data structures, transformations can be applied. The transformation is a SmartModule pulled from the SmartModule Hub. Transformations are chained according to the order in the config. If a SmartModule requires configuration, it is passed via with section of transforms entry.\nConfiguration Option default type description url - String SQL database conection url Basic example: apiVersion: 0.1.0 meta: version: 0.3.3 name: my-sql-connector type: sql-sink topic: sql-topic create-topic: true secrets: - name: DB_USERNAME - name: DB_PASSWORD - name: DB_HOST - name: DB_PORT - name: DB_NAME sql: url: \u0026#39;postgresql://${{ secrets.DB_USERNAME }}:${{ secrets.DB_PASSWORD }}@${{ secrets.DB_HOST }}:${{ secrets.DB_PORT }}/${{ secrets.DB_NAME }}\u0026#39; Secrets The connector can use secrets in order to hide sensitive information.\napiVersion: 0.1.0 meta: version: 0.3.3 name: my-sql-connector type: sql-sink topic: sql-topic secrets: - name: DATABASE_URL sql: url: ${{ secrets.DATABASE_URL }} Insert Usage Example Let\u0026rsquo;s look at the example of the connector with one transformation named infinyon/json-sql. The transformation takes records in JSON format and creates SQL insert operation to topic_message table. The value from device.device_id JSON field will be put to device_id column and the entire json body to record column.\nThe JSON record:\n{ \u0026#34;device\u0026#34;: { \u0026#34;device_id\u0026#34;: 1 } } The SQL database (Postgres):\nCREATE TABLE topic_message (device_id int, record json); Connector configuration file:\n# connector-config.yaml apiVersion: 0.1.0 meta: version: 0.3.3 name: json-sql-connector type: sql-sink topic: sql-topic create-topic: true secrets: - name: DATABASE_URL sql: url: ${{ secrets.DATABASE_URL }} transforms: - uses: infinyon/json-sql with: mapping: table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true You can use Fluvio cdk tool to deploy the connector:\nfluvio install cdk and then:\ncdk deploy start --config connector-config.yaml To delete the connector run:\ncdk deploy shutdown --config connector-config.yaml After you run the connector you will see records in your database table.\nSee more in our MQTT to SQL and HTTP to SQL guides.\nUpsert Usage Example Every step would be same except the connector config and the behavior of the connector after deployment.\nWe have a operation parameter which defaults to insert but we can pass upsert here to specify we want to do an upsert operation.\nUpsert additionaly takes an unique-columns argument. unique-columns specifies the list indices or column names to check for uniqueness of a record. If a record with same value in unique-columns exists in the database, it will be updated. If no record exists with same value, the given record will be inserted.\nConnector configuration file for upsert (assuming device_id is a unique column or an index in the database):\n# connector-config.yaml apiVersion: 0.1.0 meta: version: 0.3.3 name: json-sql-connector type: sql-sink topic: sql-topic create-topic: true secrets: - name: DATABASE_URL sql: url: ${{ secrets.DATABASE_URL }} transforms: - uses: infinyon/json-sql with: mapping: operation: \u0026#34;upsert\u0026#34; unique-columns: - \u0026#34;device_id\u0026#34; table: \u0026#34;topic_message\u0026#34; map-columns: \u0026#34;device_id\u0026#34;: json-key: \u0026#34;device.device_id\u0026#34; value: type: \u0026#34;int\u0026#34; default: \u0026#34;0\u0026#34; required: true \u0026#34;record\u0026#34;: json-key: \u0026#34;$\u0026#34; value: type: \u0026#34;jsonb\u0026#34; required: true See more about upsert in our blog. Note: the blog doesn\u0026rsquo;t use json-sql smartmodule and has hardcoded records for demonstration. sql-connector is intended to be used with json-sql.\n","description":"Reference for configuring SQL data connectors in InfinyOn Cloud","keywords":null,"summary":"Data Direction Inbound Outbound ❌ ✅ Outbound Connector The SQL Sink connector reads records from Fluvio topic, applies configured transformations, and sends new records to the SQL database (via INSERT statements).\nSupported databases PostgreSQL SQLite Data types Model PostgreSQL SQLite Bool BOOL BOOLEAN Char CHAR INTEGER SmallInt SMALLINT, SMALLSERIAL, INT2 INTEGER Int INT, SERIAL, INT4 INTEGER BigInt BIGINT, BIGSERIAL, INT8 BIGINT, INT8 Float REAL, FLOAT4 REAL DoublePrecision DOUBLE PRECISION, FLOAT8 REAL Text VARCHAR, CHAR(N), TEXT, NAME TEXT Bytes BYTEA BLOB Numeric NUMERIC REAL Timestamp TIMESTAMP DATETIME Date DATE DATE Time TIME TIME Uuid UUID BLOB, TEXT Json JSON, JSONB TEXT Transformations The SQL Sink connector expects the data in Fluvio SQL Model in JSON format.","title":"SQL Connector","url":"http://localhost:1315/docs/connectors/sql/"},{"body":"The Lookback feature in Fluvio SmartModule is the way to access records from the data stream before the SmartModule starts allowing it to build its internal state depending on what the topic currently has.\nIf configured, the Lookback phase is guaranteed to take place after init method but before the first record is processed.\nTo activate Lookback SmartModule must fit the following criteria:\nA function annotated with #[smartmodule(look_back)] macro is present in the code. This function is used to pass requested records to SmartModule: #[smartmodule(look_back)] pub fn look_back(record: \u0026amp;SmartModuleRecord) -\u0026gt; Result\u0026lt;()\u0026gt; { ... } lookback parameter is specified in transform configuration. It defines a set of lookback records that SmartModule receives. Supported ways to define the set: by size (last N records existing in the topic) by age (records that are younger than the specified age) by size and age (max N records younger than the specified age) transforms: - uses: local/filter-with-lookback@0.1.0 lookback: age: 30m # we want only records that are younger than 30 minutes last: 10 # we want maximum of 10 records (last) If Fluvio topic is empty, look_back is never called.\n-\u0026gt; If you start processing from an offset other than the end, you will receive records both as lookback record and as regular processing\n-\u0026gt; Lookback is only supported for SmartModules that are run on SPU. This implies that Source Connectors don\u0026rsquo;t support it.\nExamples This section assumes that SMDK is [installed].\nMonotonically increasing values This is an example of SmartModule that leverages Lookback functionality. It reads the last record from a topic and only allows a record that is higher than the previous value.\n%copy%\nuse std::sync::atomic::{AtomicI32, Ordering::SeqCst}; use fluvio_smartmodule::{smartmodule, SmartModuleRecord, Result}; static PREV: AtomicI32 = AtomicI32::new(0); #[smartmodule(filter)] pub fn filter(record: \u0026amp;SmartModuleRecord) -\u0026gt; Result\u0026lt;bool\u0026gt; { let string = std::str::from_utf8(record.value.as_ref())?; let current: i32 = string.parse()?; let last = PREV.load(SeqCst); if current \u0026gt; last { PREV.store(current, SeqCst); Ok(true) } else { Ok(false) } } #[smartmodule(look_back)] pub fn look_back(record: \u0026amp;SmartModuleRecord) -\u0026gt; Result\u0026lt;()\u0026gt; { let string = std::str::from_utf8(record.value.as_ref())?; let last: i32 = string.parse()?; PREV.store(last, SeqCst); Ok(()) } Using this transforms.yaml config we define that we only need one last record:\n%copy%\ntransforms: - uses: local/filter-with-lookback@0.1.0 lookback: last: 1 Test with Fluvio Cli First, let\u0026rsquo;s generate a new SmartModule using SMDK tool:\n%copy first-line%\n$ smdk generate Using hub https://hub-dev.infinyon.cloud 🤷 Project Name: filter-with-lookback ✔ 🤷 Will your SmartModule be public? · false ✔ 🤷 Which type of SmartModule would you like? · filter ✔ 🤷 Will your SmartModule use init parameters? · false [1/7] Done: .gitignore [2/7] Done: Cargo.toml [3/7] Done: README.md [4/7] Done: SmartModule.toml [5/7] Done: rust-toolchain.toml [6/7] Done: src/lib.rs [7/7] Done: src Then, put the code snippet from above into src/lib.rs file.\nNow we are ready to build and load SmartModule to the cluster:\n%copy first-line%\n$ smdk build $ smdk load Let\u0026rsquo;s produce 3 records into our topic:\n%copy first-line%\n$ fluvio produce test_topic \u0026gt; 1 Ok! \u0026gt; 2 Ok! \u0026gt; 3 Ok! In another terminal, run Fluvio Consumer with transforms.yaml file from the example above. It will use our SmartModule with configured lookback:\n%copy first-line%\n$ fluvio consume test_topic --transforms-file transforms.yaml If you insert another record in Producer, Consumer will only output it only if it\u0026rsquo;s greater than 3 (last record value when Consumer started).\nTest with SMDK tool We will use smdk test subcommand to verify that our SmartModule works.\nBy --lookback-last 1 we specify lookback parameter to \u0026ldquo;read last 1 record\u0026rdquo;.\nBy --record \u0026quot;N\u0026quot; we specify records that will exist before the SmartModule start processing.\nThe following commands will pass records \u0026ldquo;2\u0026rdquo; and \u0026ldquo;3\u0026rdquo; to look_back and record \u0026ldquo;4\u0026rdquo; to filter function:\n%copy first-line%\n$ smdk test --text 4 --lookback-last 1 --record 2 --record 3 4 The output result is 4. But if we run:\n%copy first-line%\n$ smdk test --text 1 --lookback-last 1 --record 2 --record 3 the record will be filtered out as expected.\n[installed]:\n","description":"","keywords":null,"summary":"The Lookback feature in Fluvio SmartModule is the way to access records from the data stream before the SmartModule starts allowing it to build its internal state depending on what the topic currently has.\nIf configured, the Lookback phase is guaranteed to take place after init method but before the first record is processed.\nTo activate Lookback SmartModule must fit the following criteria:\nA function annotated with #[smartmodule(look_back)] macro is present in the code.","title":"Stream Processing with Lookback SmartModules","url":"http://localhost:1315/docs/resources/lookback/"},{"body":"Your use of InfinyOn\u0026rsquo;s products and services, including the use of Infinyon Cloud Service (\u0026ldquo;Cloud Service\u0026rdquo;), Subscriptions to the deployed on-premise Fluvio Platform (together the Infinyon Cloud Service and Subscriptions are the \u0026ldquo;InfinyOn Products”), and Services (defined below) is governed by these terms of service (“Agreement”). “InfinyOn” means InfinyOn, Inc., a Delaware corporation located at 2445 Augustine Drive, Suite 249, Santa Clara, CA 95054, USA. “Your”, “You” and “Customer” are used interchangeably throughout the Agreement and have the same meaning.\nBY REGISTERING TO USE THE INFINYON CLOUD SERVICE, YOU AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT ON BEHALF OF YOUR ORGANIZATION. YOU REPRESENT AND WARRANT THAT YOU HAVE THE LEGAL AUTHORITY TO BIND YOUR ORGANIZATION TO THIS AGREEMENT, AND THAT YOU HAVE READ AND UNDERSTOOD THIS AGREEMENT. IF YOU DO NOT HAVE SUCH AUTHORITY, OR IF YOU OR YOUR ORGANIZATION DOES NOT AGREE WITH THE TERMS OF THIS AGREEMENT, YOU SHOULD NOT ACCEPT IT.\nDEFINITIONS\n1.1 \u0026ldquo;Affiliate\u0026rdquo; means any entity that, directly or indirectly through one or more intermediaries, controls, or is controlled by, or is under common control with Customer, for so long as such control exists.\n1.2 \u0026ldquo;Authorized User” means means a named individual that: (a) is an employee, representative, consultant, contractor or agent of Customer or a Customer Affiliate; (b) is authorized to use the SaaS Service pursuant to this Agreement; and (c) has been supplied a user identification and password by Customer.\n1.3 \u0026ldquo;Cloud Service\u0026rdquo; means the managed service offering(s) that InfinyOn makes available through the website as a hosted, web-based service, and that are used by Customer or Authorized Users.\n1.4 “Customer Data” means any electronic data or materials provided or submitted by Customer or Authorized Users to or through the Cloud Service.\n1.5 “Documentation” means the online help materials, including technical specifications, describing the features and functionality of the Cloud Service, which are located on InfinyOn\u0026rsquo;s publicly-available website at https://www.fluvio.io/docs as updated by InfinyOn.\n1.6 “Intellectual Property Rights” means all current and future worldwide intellectual property rights, including without limitation, all patents, copyrights, trademarks, service marks, trade names, domain name rights, know-how and other trade secret rights, and all other intellectual property rights and similar forms of protection, and all applications and registrations for any of the foregoing.\n1.7 “Subscription Term(s)” means the term of Customer’s or Authorized User\u0026rsquo;s use of the Subscription, which commences on the commencement date specified in the Subscription. If no such commencement date is specified, the Subscription Term will commence after Customer fully and accurately completes registration on the Infinyon Cloud Services website.\n1.8 \u0026ldquo;Support Services\u0026rdquo; means the applicable support and maintenance service that InfinyOn provides pursuant to the applicable Subscription.\nCLOUD LICENSE AND SUPPORT TERMS\n2.1 Generally. InfinyOn will host the Cloud Service and will make the Service available to Customer during the Subscription Term(s), subject to the terms and conditions of this Agreement. The Cloud Service is offered to Customer at no cost, unless customer has selected a paid version of the Software. Customer is responsible for Users\u0026rsquo; compliance with this Agreement.\n2.2 Customer Responsibility. Customer shall not resell, sublicense, rent or lease the Cloud Service, or otherwise make it available to anyone other than its Users. Customer shall not use the Cloud Service to violate the security or integrity of any network, computer or communications system, software application, or network or computing device. Customer shall not make network connections to any users, hosts, or networks unless Customer has permission to communicate with them, and may not use manual or electronic means to avoid any use limitations placed on the Cloud Service, such as access and storage restrictions. InfinyOn may, but has no obligation to (a) investigate any violation of this provision or misuse of the Cloud Service, or (b) remove any content, or disable access to any resource, that violates the foregoing. Customer is solely responsible for (i) the legality of Content, (ii) ensuring compliance with all privacy laws applicable to the collection and provision of Content; (iii) Customer’s configuration and use of the Cloud Service, and (iv) taking appropriate action to secure, protect and backup Content, which may include use of encryption to protect Content from unauthorized access. Customer is responsible for communicating with the Cloud Service through encrypted and authenticated connections, as may be required by InfinyOn, and for transmitting all Content using such security methods. To the extent that Customer will be subject to user data access and deletion requests, Customer is responsible for configuring the retention period on Fluvio topics that contain EU personal data to a maximum of 30 days.\n2.3 Sensitive Data. Customer agrees that it will not submit the following types of information to the Cloud Service except with InfinyOn\u0026rsquo;s prior written approval: government-issued identification numbers, consumer financial account information, credit and payment card information, personal health information, or information deemed “sensitive” under applicable law (such as racial or ethnic origin, political opinions, or religious or philosophical beliefs) or personal data (as described in the Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data) of data subjects that reside in the European Economic Area (EEA). If Customer wishes to submit any such European personal data to the SaaS Service, Customer will notify InfinyOn and the parties may enter into a separate data processing agreement (including the European Commission’s Standard Contract Clauses for the transfer of personal data to processors established in third countries which do not ensure an adequate level of data protection) with InfinyOn prior to submission of such personal data to the Cloud Service. Customer represents and warrants that it has obtained all necessary consents and permissions from data subjects for the submission and processing of personal data in the Cloud Service.\n2.4 Excessive Use of Resources. Customer’s access and usage of the Cloud Service may not exceed the typical and customary utilization of such service, as contemplated by InfinyOn in its sole discretion (including but not limited to the creation of an excessive number of resources), and may not interfere with other users’ utilization of the SaaS Service.\n2.5 Service Updates. InfinyOn may update the content, features, functionality, and user interface of the Cloud Service from time to time in its sole discretion, and may discontinue or suspend all or any portion of the Cloud Service at any time in its sole discretion, including during a Subscription Term; provided, that InfinyOn will give Customer at least fifteen (15) days’ advance notice before discontinuing the Cloud Service or materially decreasing the functionality of the Cloud Service during the Subscription Term.\n2.6 Privacy and Security. InfinyOn will use commercially reasonable administrative, physical and technical safeguards designed to prevent unauthorized access, use or disclosure of Content for the Customer\u0026rsquo;s data. InfinyOn will not access any Content except as necessary to provide the Cloud Service or Support Services, to enforce the provisions of this Agreement, or for a Permitted Disclosure.\n2.7 Registration. Customer must register and setup an authorized account to use the Cloud Service. Customer must keep the registration information accurate and complete. Customer is responsible for the security of its User IDs and passwords and for the use of its accounts and will immediately notify InfinyOn of any unauthorized use at support@infinyon.com.\n2.8 Support Services. InfinyOn will provide Support Services for the Cloud Service, as provided in InfinyOn\u0026rsquo;s applicable support policy and in accordance with the level of Support Services purchased. If Customer does not purchase a different level of Support Services, InfinyOn will provide a “Free” level of Support Services. Support Services do not include support for any open source versions of Fluvio, and Customer agrees to request support only for the Cloud Service licensed under this Agreement.\n2.9 Restrictions. Except as otherwise expressly set forth in this Agreement, Customer will not and will not permit any third party to: (a) sublicense, sell, transfer, assign, distribute or otherwise grant or enable access to the Cloud Service in a manner that allows anyone to access or use the Cloud Service without an Authorized User subscription, or to commercially exploit the Cloud Service; (b) copy, modify or create derivative works based on the Cloud Service; (c) reverse engineer or decompile the Cloud Service (except to the extent permitted by applicable law and only if InfinyOn fails to provide permitted interface information within a reasonable period of time after Customer’s written request); (d) copy any features, functions or graphics of the Cloud Service; (e) allow Authorized User subscriptions to be shared or used by more than one individual Authorized User (except that Authorized User subscriptions may be reassigned to new Authorized Users replacing individuals who no longer use the Cloud Service for any purpose, whether by termination of employment or other change in job status or function); or (f) access to or use of the Cloud Service: (i) to send or store infringing, obscene, threatening, or otherwise unlawful material, including material violative of third-party privacy rights; (ii) in violation of applicable laws; (iii) to send or store material containing software viruses, worms, Trojan horses or other harmful computer code, files, scripts, or agents; (iv) in a manner that interferes with or disrupts the integrity or performance of the Cloud Service (or the data contained therein); (v) to gain unauthorized access to the Cloud Service (including unauthorized features and functionality) or its related systems or networks; (vi) Circumvent defined limits on an account in an unauthorized manner; (vii) Abuse referrals, promotions or credits to get more features than paid for; or (viii) Access, search, or create accounts for the Cloud Service by any means other than InfinyOn\u0026rsquo;s publicly supported interfaces (for example, “scraping” or creating accounts in bulk).\nINTELLECTUAL PROPERTY OWNERSHIP\n3.1 Ownership. The Cloud Service and Documentation, all copies and portions thereof, and all Intellectual Property Rights therein, including, but not limited to derivative works therefrom, are and will remain the sole and exclusive property of InfinyOn notwithstanding any other provision in this Agreement. Customer is not authorized to use (and will not permit any third party to use) the Cloud Service, Documentation or any portion thereof except as expressly authorized by this Agreement.\n3.2 License to Customer Data. Customer grants InfinyOn a worldwide, non-exclusive license to host, copy, process, transmit Customer Data in accordance with this Agreement. InfinyOn may use the data to produce statistics and analytics for its own business purposes, including to maintain and improve its products and services and to monitor and analyze its activities in connection with the performance of such services. Subject to this limited license, as between Customer and InfinyOn, Customer owns all right, title and interest, including all related Intellectual Property Rights, in and to the Customer Data.\n3.3 Use of Aggregate Information. InfinyOn may collect and aggregate data derived from the operation of the Cloud Service (“Aggregated Data”), and InfinyOn may use such Aggregated Data for purposes of operating InfinyOn\u0026rsquo;s business, monitoring performance of the Cloud Service, and/or improving the Cloud Service; provided, that InfinyOn\u0026rsquo;s use of Aggregated Data does not reveal any Customer Data, Customer Confidential Information, or personally identifiable information of Authorized Users.\n3.4 Feedback. Customer has no obligation to provide InfinyOn any suggestions, enhancement requests, recommendations, or other feedback regarding InfinyOn\u0026rsquo;s products and services (“Feedback”). However, InfinyOn may use and include any Feedback that Customer provides in InfinyOn\u0026rsquo;s products and services without restriction or payment.\nTERM; TERMINATION\n4.1 Term. This Agreement commences on the Effective Date. Unless earlier terminated as provided below, the Agreement will continue through the Subscription Term. Unless one party notifies the other the Subscription Term will automatically renew for an additional Subscription Term of the same length.\n4.2 Termination. Unless otherwise stated, Customer may discontinue its use of the Cloud Service at any time for any reason by following the process in the InfinyOn website interface to “Delete” Customer’s purchased Cloud Service. Either Party may terminate this Agreement for any reason or no reason by providing the other party at least fifteen (15) days prior written notice. In the event the Customer materially breaches this Agreement and such breach is not cured within thirty (30) days will result in termination (as described \u0026ldquo;Suspension\u0026rdquo; section below). The termination the Subscription will not automatically result in the termination of this Agreement. Either party may also terminate this Agreement immediately if the other party (a) terminates or suspends its business; (b) becomes subject to any bankruptcy or insolvency proceeding under Federal or state statute; (c) becomes insolvent or subject to direct control by a trustee, receiver or similar authority; or (d) has wound up or liquidated, voluntarily or otherwise.\n4.3 Suspension. InfinyOn may discontinue or suspend Customer’s access to the Cloud Service immediately if Customer has (or InfinyOn reasonably suspects that Customer has) breached Section 2.9 or infringed InfinyOn\u0026rsquo;s Intellectual Property Rights.\n4.4 Effect of Termination. Upon expiration or termination of this Agreement for any reason: (a) InfinyOn’s obligation to provide Support Services and the Cloud Service will terminate, and (b) all of Customer’s and its Authorized Users’ rights to use the Cloud Service will terminate. Customer agrees that following termination of this Agreement, InfinyOn may immediately deactivate Customer’s account(s) for the cloud Service, and InfinyOn has the right to delete those accounts, including all Customer Data, from InfinyOn\u0026rsquo;s site unless legally prohibited. Customer acknowledges and agrees that is responsible to retrieve Customer Data from the Cloud Service prior to expiration of this Agreement. Customer acknowledges that following termination it will have no further access to any Content.\nCONFIDENTIALITY\n5.1 Confidentiality Obligations. Each party shall retain in confidence the non-public information and technical information disclosed or made available by the other party pursuant to this Agreement which is either designated in writing as proprietary and/or confidential, if disclosed in writing, or if disclosed orally, is designated in writing (which may be via email) as confidential within thirty (30) days of the oral disclosure or should reasonably be understood to be confidential by the recipient (“Confidential Information”). Notwithstanding any failure to so designate it, the InfinyOn Software is InfinyOn\u0026rsquo;s Confidential Information, and Content is Customer’s Confidential Information. Each party shall (a) maintain the confidentiality of the other party’s Confidential Information using at least a reasonable degree of care; (b) refrain from using the other party’s Confidential Information except for the purpose of performing its obligations under this Agreement; and (c) not disclose Confidential Information to any third party except to employees, subcontractors and agents as is reasonably required in connection with this Agreement and who are subject to confidentiality obligations at least as protective as those set forth in this section. Each party shall immediately notify the other party of any unauthorized disclosure or use of any Confidential Information and assist the other party in remedying such unauthorized use or disclosure by taking such steps as are reasonably requested by such other party. The foregoing obligations will not apply to Confidential Information of the other party which (a) is or becomes publicly known without breach of this Agreement; (b) is discovered or created by the receiving party without use of, or reference to, the Confidential Information of the disclosing party, as shown in records of the receiving party; or (c) is otherwise known to the receiving party without confidentiality restrictions and through no wrongful conduct of the receiving party. Receiving party may disclose Confidential Information to the extent required by law or court order if the receiving party provides prompt notice and reasonable assistance to the disclosing party to enable the disclosing party to seek a protective order or otherwise prevent or restrict such disclosure (\u0026ldquo;Compelled Disclosures\u0026rdquo;).\n5.2 Compelled Disclosures. The Recipient will not be in violation of Section 5.1 regarding a disclosure that was in response to a valid order by a court or other governmental body, provided that the Recipient provides the Discloser with prior written notice of such disclosure in order to permit the Discloser to seek confidential treatment of such information.\n5.3 Injunctive Relief. Any breach or threatened breach of this section may cause irreparable harm to the disclosing party for which there is no adequate remedy at law. Therefore, the disclosing party will be entitled to seek injunctive relief without the necessity of proving actual damages or posting a bond, in addition to any other remedies available at law.\nREPRESENTATIONS AND WARRANTIES\n6.1 By Each Party. Each Party represents and warrants that it has the power and authority to enter into this Agreement and that its respective provision and use of the SaaS Service is in compliance with laws applicable to such Party.\n6.2 Support Services. InfinyOn represents and warrants that it shall perform Support Services in a professional manner, employing a standard of care, skill and diligence consistent with industry standards. In the event of a material breach of the foregoing warranty, Customer’s exclusive remedy and InfinoOn’s entire liability will be for Customer to request InfinyOn’s assistance through the Support Services, which InfinyOn will provide in accordance with its obligations under Section 2.8 (“Support Services”).\n6.3 Malicious Code. InfinyOn warrants that, to the best of its knowledge, the Cloud Service is free from, and InfinyOn will not knowingly introduce, software viruses, worms, Trojan horses or other code, files, scripts, or agents intended to do harm.\n6.4 Warranty Disclaimer. EXCEPT FOR THE EXCLUSIVE WARRANTIES SET FORTH IN THIS SECTION 6, TO THE MAXIMUM EXTENT PERMITTED UNDER APPLICABLE LAW, THE CLOUD SERVICE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, AND INFINYON MAKES NO WARRANTIES, EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH REGARDING OR RELATING TO THE CLOUD SERVICE, DOCUMENTATION, OR SUPPORT SERVICES.INFINYON SPECIFICALLY AND EXPLICITLY DISCLAIMS ALL OTHER WARRANTIES, EXPRESS AND IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, INFINYON DOES NOT WARRANT THAT THE CLOUD SERVICE OR INFINYON SOFTWARE WILL OPERATE UNINTERRUPTED OR ERROR FREE, OR THAT ALL ERRORS WILL BE CORRECTED. FOR THE AVOIDANCE OF DOUBT, NOTHING IN THIS DISCLAIMER DIMINISHES ANY OBLIGATIONS EXPRESSLY STATED IN THE SERVICE LEVEL AGREEMENT REFERENCED IN THIS AGREEMENT OR AN ORDER HEREUNDER.\nINDEMNIFICATION\n7.1 By InfinyOn. Subject to liability limitations set forth in Section 8, Infinyon will: (a) defend Customer against any third party claim that the Cloud Service infringes any trademark or copyright of such third party, enforceable in the jurisdiction of Customer’s use of the Cloud Service, or misappropriates a trade secret (but only to the extent that such misappropriation is not a result of Customer’s actions) (“Infringement Claim”); and (b) indemnify Customer against and pay any settlement of such Infringement Claim consented to by InfinyOn or any damages finally awarded against Customer to such third party by a court of competent jurisdiction. InfinyOn will have no obligation and assumes no liability under this Section 7 or otherwise with respect to any claim to the extent based on: (a) operation or use of the Cloud Service with any Customer Data or any Customer or third party products, services, hardware, data, content, or business processes not provided by InfinyOn where there would be no Infringement Claim but for such combination; (b) use of the Cloud Service other than in accordance with the terms and conditions of this Agreement and the Documentation; or (c) Customer’s or any Authorized User’s use of the Cloud Service other than as permitted under this Agreement. THIS SECTION SETS FORTH INFINYON’S ENTIRE LIABILITY AND OBLIGATION AND CUSTOMER’S SOLE AND EXCLUSIVE REMEDY FOR ANY CLAIM OF INFRINGEMENT CLAIMS OR ACTIONS.\n7.2 Remedies. Should the Cloud Service become, or in InfinyOn\u0026rsquo;s opinion be likely to become, the subject of an Infringement Claim, InfinyOn may, at its option (a) procure for Customer the right to use the Cloud Service in accordance with this Agreement; (b) replace or modify, the Cloud Service to make it non-infringing; or (c) terminate Customer’s right to use the Cloud Service and discontinue the related Support Services.\n7.3 By Customer. Customer will defend, indemnify and hold harmless InfinyOn and its Affiliates, and its and their directors, officers, employees, agents and licensors from and against any third-party claims, lawsuit, and costs (including reasonable attorneys’ fees and costs incurred by the indemnified parties) arising out of or relating to (a) Customer’s use of the SaaS Service or (b) Customer Data; provided, that Customer will have no obligation under this Section to the extent the applicable claim arises from InfinyOn’s breach of this Agreement.\n7.4 Indemnity Process. Each Party’s indemnification obligations are conditioned on the indemnified party: (a) promptly giving written notice of the claim to the indemnifying Party; (b) giving the indemnifying Party sole control of the defense and settlement of the claim; and (c) providing to the indemnifying Party all available information and assistance in connection with the claim, at the indemnifying Party’s request and expense. The indemnified Party may participate in the defense of the claim, at the indemnified Party’s sole expense (not subject to reimbursement). Neither Party may admit liability for or consent to any judgment or concede or settle or compromise any claim unless such admission or concession or settlement or compromise includes a full and unconditional release of the other Party from all liabilities in respect of such claim.\nLIMITATION OF LIABILITY\n8.1 Exclusions and Limitations. IN NO EVENT WILL EITHER PARTY OR ITS AFFILIATES OR LICENSORS BE LIABLE UNDER THIS AGREEMENT FOR ANY CONSEQUENTIAL, INCIDENTAL, SPECIAL, INDIRECT, PUNITIVE OR EXEMPLARY DAMAGES, INCLUDING WITHOUT LIMITATION LOST PROFITS, LOSS OF USE, BUSINESS INTERRUPTIONS, LOSS OF DATA, REVENUE, GOODWILL, PRODUCTION, ANTICIPATED SAVINGS, OR COSTS OF PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, WHETHER ALLEGED AS A BREACH OF CONTRACT OR TORTIOUS CONDUCT, INCLUDING NEGLIGENCE, EVEN IF A PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. EXCEPT WITH RESPECT TO LIABILITY ARISING FROM ITS OBLIGATIONS UNDER SECTION 7 (“INDEMNIFICATION”), IN NO EVENT WILL INFINYON’S TOTAL AGGREGATE LIABILITY ARISING UNDER THIS AGREEMENT EXCEED THE AMOUNT PAID OR PAYABLE BY CUSTOMER PURSUANT TO THIS AGREEMENT DURING THE TWELVE (12) MONTHS IMMEDIATELY PRIOR TO THE EVENT GIVING RISE TO SUCH LIABILITY. NOTHING IN THIS SECTION WILL BE DEEMED TO LIMIT EITHER PARTY’S LIABILITY FOR WILLFUL MISCONDUCT, GROSS NEGLIGENCE, FRAUD, OR INFRINGEMENT BY ONE PARTY OF THE OTHER’S INTELLECTUAL PROPERTY RIGHTS.\n8.2 Limitations Fair and Reasonable. EACH PARTY ACKNOWLEDGES THAT THE LIMITATIONS OF LIABILITY SET FORTH IN THIS SECTION 8 REFLECT THE ALLOCATION OF RISK BETWEEN THE PARTIES UNDER THIS AGREEMENT, AND THAT IN THE ABSENCE OF SUCH LIMITATIONS OF LIABILITY, THE ECONOMIC TERMS OF THIS AGREEMENT WOULD BE SIGNIFICANTLY DIFFERENT.\nGENERAL\n9.1 Assignment. Neither party may assign or otherwise transfer this Agreement or any rights or obligations hereunder, in whole or in part, whether by operation of law or otherwise, to any third party without the other party’s prior written consent, except to an Affiliate or to any successor to its business or assets to which this Agreement relates, whether by merger, sale of assets, sale of stock, reorganization or otherwise. Any purported transfer, assignment or delegation without such prior written consent will be void. Subject to this section, this Agreement shall be binding upon and inure to the benefit of the parties, and their respective successors and permitted assigns.\n9.2 Anti-Corruption. Each Party acknowledges that it is aware of, understands and has complied and will comply with, all applicable U.S. and foreign anti-corruption laws, including without limitation, the U.S. Foreign Corrupt Practices Act (“FCPA”) and the U.K. Bribery Act.\n9.3 Delays. In the event that either party is unable to perform any of its obligations under this Agreement due to any Act of God, fire, casualty, flood, earthquake, war, strike, lockout, epidemic, destruction of production facilities, riot, insurrection, material unavailability, or any other cause beyond the reasonable control of the party invoking this section, and if such party used its commercially reasonable efforts to mitigate its effects, such party shall give prompt written notice to the other party, and the time for the performance shall be extended for the period of delay or inability to perform due to such occurrences.\n9.4 Non-waiver. Any failure of either Party to insist upon or enforce performance by the other Party of any of the provisions of this Agreement or to exercise any rights or remedies under this Agreement will not be interpreted or construed as a waiver or relinquishment of such Party\u0026rsquo;s right to assert or rely upon such provision, right or remedy in that or any other instance.\n9.5 Governing Law. This Agreement is governed by the laws of the State of California without regard to its conflicts of laws principles. All disputes arising out of this Agreement will be subject to the exclusive jurisdiction of and venue in the federal and state courts within Santa Clara County, California. The parties consent to the personal and exclusive jurisdiction and venue of these courts. Neither the United Nations Convention of Contracts for the International Sale of Goods nor the Uniform Computer Information Transactions Act will apply to this Agreement.\n9.6 Severability. If any provision of this Agreement is held invalid or unenforceable under applicable law by a court of competent jurisdiction, it will be replaced with the valid provision that most closely reflects the intent of the Parties and the remaining provisions of the Agreement will remain in full force and effect.\n9.7 Relationship of the Parties. Nothing in this Agreement is to be construed as creating an agency, partnership, or joint venture relationship between the Parties hereto. Neither Party has any right or authority to assume or create any obligations or to make any representations or warranties on behalf of any other Party, whether express or implied, or to bind the other Party in any respect whatsoever. Each Party may identify the other as a customer or supplier, as applicable.\n9.8 Export Compliance. InfinyOn Materials are subject to export control laws and regulations. Customer may not access or use the InfinyOn Materials or any underlying information or technology except in full compliance with all applicable United States export control laws. Neither the InfinyOn Technology nor any underlying information or technology may be accessed or used (a) by any individual or entity in any country to which the United States has embargoed goods; or (b) by anyone on the U.S. Treasury Department’s list of specially designated nationals or the U.S. Commerce Department’s list of prohibited countries or debarred or denied persons or entities.\n9.9 Government Restricted Rights. If InfinyOn Software is being licensed by the U.S. Government, the InfinyOn Software is deemed to be “commercial computer software” and “commercial computer documentation” developed exclusively at private expense, and (a) if acquired by or on behalf of a civilian agency, shall be subject solely to the terms of this computer software license as specified in 48 C.F.R. 12.212 of the Federal Acquisition Regulations and its successors; and (b) if acquired by or on behalf of units of the Department of Defense (“DOD”) shall be subject to the terms of this commercial computer software license as specified in 48 C.F.R. 227.7202-2, DOD FAR Supplement and its successors.\n9.10 Execution. This Agreement comprises the entire agreement between Customer and InfinyOn, and supersedes all prior or contemporaneous proposals, quotes, negotiations, discussions, or agreements, whether written or oral, between the Parties regarding its subject matter. In the event of a conflict between the terms of this Agreement and any other document referenced in this Agreement, this Agreement will control. Any preprinted terms on any Customer ordering documents or terms referenced or linked therein will have no effect on the terms of this Agreement and are hereby rejected, including where such Customer ordering document is signed by InfinyOn. This Agreement may be executed in counterparts, which taken together form one binding legal instrument. The Parties hereby consent to the use of electronic signatures in connection with the execution of this Agreement, and further agree that electronic signatures to this Agreement will be legally binding with the same force and effect as manually executed signatures.\n","description":"","keywords":null,"summary":"Your use of InfinyOn\u0026rsquo;s products and services, including the use of Infinyon Cloud Service (\u0026ldquo;Cloud Service\u0026rdquo;), Subscriptions to the deployed on-premise Fluvio Platform (together the Infinyon Cloud Service and Subscriptions are the \u0026ldquo;InfinyOn Products”), and Services (defined below) is governed by these terms of service (“Agreement”). “InfinyOn” means InfinyOn, Inc., a Delaware corporation located at 2445 Augustine Drive, Suite 249, Santa Clara, CA 95054, USA. “Your”, “You” and “Customer” are used interchangeably throughout the Agreement and have the same meaning.","title":"User Agreement","url":"http://localhost:1315/legal/user-agreement/"}
]